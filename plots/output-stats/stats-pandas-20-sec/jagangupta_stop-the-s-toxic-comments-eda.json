{
  "max-mem-in-mb": 594,
  "max-mem-in-mb2": 598,
  "cells": [
    {
      "raw": "#Check the dataset sizes(in MB)\n# FIRST-AUTHOR: remove commands\n# !du -l ../input/*",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 142688
    },
    {
      "raw": "#import required packages\n#basics\n# import pandas as pd \nexec(os.environ['IREWR_IMPORTS'])\nimport numpy as np\n\n#misc\n# FIRST-AUTHOR: remove ML code, plotting\n# import gc\n# import time\n# import warnings\n\n# #stats\n# from scipy.misc import imread\n# from scipy import sparse\n# import scipy.stats as ss\n\n# #viz\n# import matplotlib.pyplot as plt\n# import matplotlib.gridspec as gridspec \n# import seaborn as sns\n# from wordcloud import WordCloud ,STOPWORDS\n# from PIL import Image\n# import matplotlib_venn as venn\n\n# #nlp\nimport string\nimport re    #for regex\n# FIRST-AUTHOR: remove ML code, plotting\n# import nltk\n# from nltk.corpus import stopwords\n# import spacy\n# from nltk import pos_tag\n# from nltk.stem.wordnet import WordNetLemmatizer \n# from nltk.tokenize import word_tokenize\n# # Tweet tokenizer does not split at apostophes which is what we want\n# from nltk.tokenize import TweetTokenizer   \n\n\n# #FeatureEngineering\n# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n# from sklearn.decomposition import TruncatedSVD\n# from sklearn.base import BaseEstimator, ClassifierMixin\n# from sklearn.utils.validation import check_X_y, check_is_fitted\n# from sklearn.linear_model import LogisticRegression\n# from sklearn import metrics\n# from sklearn.metrics import log_loss\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.model_selection import train_test_split\n\n\n\n\n\n# #settings\n# start_time=time.time()\n# color = sns.color_palette()\n# sns.set_style(\"dark\")\n# eng_stopwords = set(stopwords.words(\"english\"))\n# warnings.filterwarnings(\"ignore\")\n\n# lem = WordNetLemmatizer()\n# tokenizer=TweetTokenizer()\n\n# %matplotlib inline",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 785940
    },
    {
      "raw": "#importing the dataset\ntrain=pd.read_csv(\"./input/train.scaled.csv\")\ntest=pd.read_csv(\"./input/test.scaled.csv\")",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 1537090329
    },
    {
      "raw": "#take a peak\ntrain.tail(10)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 451074
    },
    {
      "raw": "nrow_train=train.shape[0]\nnrow_test=test.shape[0]\nsum=nrow_train+nrow_test\nprint(\"       : train : test\")\nprint(\"rows   :\",nrow_train,\":\",nrow_test)\nprint(\"perc   :\",round(nrow_train*100/sum),\"   :\",round(nrow_test*100/sum))",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 605481
    },
    {
      "raw": "x=train.iloc[:,2:].sum()\n#marking comments without any tags as \"clean\"\nrowsums=train.iloc[:,2:].sum(axis=1)\ntrain['clean']=(rowsums==0)\n#count number of clean entries\ntrain['clean'].sum()\nprint(\"Total comments = \",len(train))\nprint(\"Total clean comments = \",train['clean'].sum())\nprint(\"Total tags =\",x.sum())",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 8494907
    },
    {
      "raw": "print(\"Check for missing values in Train dataset\")\nnull_check=train.isnull().sum()\nprint(null_check)\nprint(\"Check for missing values in Test dataset\")\nnull_check=test.isnull().sum()\nprint(null_check)\nprint(\"filling NA with \\\"unknown\\\"\")\ntrain[\"comment_text\"].fillna(\"unknown\", inplace=True)\ntest[\"comment_text\"].fillna(\"unknown\", inplace=True)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 113177803
    },
    {
      "raw": "x=train.iloc[:,2:].sum()\n#plot\n# FIRST-AUTHOR: remove plotting\n# plt.figure(figsize=(8,4))\n# ax= sns.barplot(x.index, x.values, alpha=0.8)\n# plt.title(\"# per class\")\n# plt.ylabel('# of Occurrences', fontsize=12)\n# plt.xlabel('Type ', fontsize=12)\n# #adding the text labels\n# rects = ax.patches\n# labels = x.values\n# for rect, label in zip(rects, labels):\n#     height = rect.get_height()\n#     ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\n# plt.show()\n_ = x.index\n_ = x.values\n_ = x.values",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 3151714
    },
    {
      "raw": "x=rowsums.value_counts()\n\n#plot\n# FIRST-AUTHOR: remove plotting\n# plt.figure(figsize=(8,4))\n# ax = sns.barplot(x.index, x.values, alpha=0.8,color=color[2])\n# plt.title(\"Multiple tags per comment\")\n# plt.ylabel('# of Occurrences', fontsize=12)\n# plt.xlabel('# of tags ', fontsize=12)\n\n# #adding the text labels\n# rects = ax.patches\n# labels = x.values\n# for rect, label in zip(rects, labels):\n#     height = rect.get_height()\n#     ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\n# plt.show()\n_ = x.index\n_ = x.values\n_ = x.values",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 1422767
    },
    {
      "raw": "temp_df=train.iloc[:,2:-1]\n# filter temp by removing clean comments\n# temp_df=temp_df[~train.clean]\n\ncorr=temp_df.corr()\n# FIRST-AUTHOR: remove plotting\n# plt.figure(figsize=(10,8))\n# sns.heatmap(corr,\n#             xticklabels=corr.columns.values,\n#             yticklabels=corr.columns.values, annot=True)\n_ = corr.columns.values\n_ = corr.columns.values",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 22207638
    },
    {
      "raw": "# https://pandas.pydata.org/pandas-docs/stable/style.html\ndef highlight_min(data, color='yellow'):\n    '''\n    highlight the maximum in a Series or DataFrame\n    '''\n    attr = 'background-color: {}'.format(color)\n    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n        is_min = data == data.min()\n        return [attr if v else '' for v in is_min]\n    else:  # from .apply(axis=None)\n        is_max = data == data.min().min()\n        return pd.DataFrame(np.where(is_min, attr, ''),\n                            index=data.index, columns=data.columns)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 953373
    },
    {
      "raw": "#Crosstab\n# Since technically a crosstab between all 6 classes is impossible to vizualize, lets take a \n# look at toxic with other tags\nmain_col=\"toxic\"\ncorr_mats=[]\nfor other_col in temp_df.columns[1:]:\n    confusion_matrix = pd.crosstab(temp_df[main_col], temp_df[other_col])\n    corr_mats.append(confusion_matrix)\nout = pd.concat(corr_mats,axis=1,keys=temp_df.columns[1:])\n\n#cell highlighting\n# FIRST-AUTHOR: remove output styling\n# out = out.style.apply(highlight_min,axis=0)\nout = out.apply(highlight_min,axis=0)\nout",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 58236477
    },
    {
      "raw": "#https://stackoverflow.com/questions/20892799/using-pandas-calculate-cram%C3%A9rs-coefficient-matrix/39266194\ndef cramers_corrected_stat(confusion_matrix):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n# FIRST-AUTHOR: remove ML code\n#     chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n# FIRST-AUTHOR: remove ML code\n#     phi2 = chi2/n\n    r,k = confusion_matrix.shape\n# FIRST-AUTHOR: remove ML code\n#     phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n    rcorr = r - ((r-1)**2)/(n-1)\n    kcorr = k - ((k-1)**2)/(n-1)\n# FIRST-AUTHOR: remove ML code\n#     return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 717334
    },
    {
      "raw": "#Checking for Toxic and Severe toxic for now\n# import pandas as pd\ncol1=\"toxic\"\ncol2=\"severe_toxic\"\nconfusion_matrix = pd.crosstab(temp_df[col1], temp_df[col2])\nprint(\"Confusion matrix between toxic and severe toxic:\")\nprint(confusion_matrix)\n# FIRST-AUTHOR: remove ML code\n# new_corr=cramers_corrected_stat(confusion_matrix)\n# print(\"The correlation between Toxic and Severe toxic using Cramer's stat=\",new_corr)\ncramers_corrected_stat(confusion_matrix)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 12958162
    },
    {
      "raw": "print(\"toxic:\")\nprint(train[train.severe_toxic==1].iloc[3,1])\n#print(train[train.severe_toxic==1].iloc[5,1])",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 1127981
    },
    {
      "raw": "print(\"severe_toxic:\")\nprint(train[train.severe_toxic==1].iloc[4,1])\n#print(train[train.severe_toxic==1].iloc[4,1])",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 972443
    },
    {
      "raw": "print(\"Threat:\")\nprint(train[train.threat==1].iloc[1,1])\n#print(train[train.threat==1].iloc[2,1])",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 832971
    },
    {
      "raw": "print(\"Obscene:\")\nprint(train[train.obscene==1].iloc[1,1])\n#print(train[train.obscene==1].iloc[2,1])",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 1866928
    },
    {
      "raw": "print(\"identity_hate:\")\nprint(train[train.identity_hate==1].iloc[4,1])\n#print(train[train.identity_hate==1].iloc[4,1])",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 953055
    },
    {
      "raw": "# FIRST-AUTHOR: remove JPython commands, ML code\n# !ls ../input/imagesforkernal/\n# stopword=set(STOPWORDS)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 113966
    },
    {
      "raw": "#clean comments\n# FIRST-AUTHOR: remove plotting\n# clean_mask=np.array(Image.open(\"./input/safe-zone.png\"))\n# clean_mask=clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset=train[train.clean==True]\ntext=subset.comment_text.values\n# FIRST-AUTHOR: remove plotting\n# wc= WordCloud(background_color=\"black\",max_words=2000,mask=clean_mask,stopwords=stopword)\n# wc.generate(\" \".join(text))\n# plt.figure(figsize=(20,10))\n# plt.axis(\"off\")\n# plt.title(\"Words frequented in Clean Comments\", fontsize=20)\n# plt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\n# plt.show()",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 7400975
    },
    {
      "raw": "# FIRST-AUTHOR: remove plotting\n# toxic_mask=np.array(Image.open(\"./input/toxic-sign.png\"))\n# toxic_mask=toxic_mask[:,:,1]\n#wordcloud for clean comments\nsubset=train[train.toxic==1]\ntext=subset.comment_text.values\n# FIRST-AUTHOR: remove plotting\n# wc= WordCloud(background_color=\"black\",max_words=4000,mask=toxic_mask,stopwords=stopword)\n# wc.generate(\" \".join(text))\n# plt.figure(figsize=(20,20))\n# plt.subplot(221)\n# plt.axis(\"off\")\n# plt.title(\"Words frequented in Toxic Comments\", fontsize=20)\n# plt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244), alpha=0.98)\n\n# #Severely toxic comments\n# plt.subplot(222)\n# severe_toxic_mask=np.array(Image.open(\"./input/bomb.png\"))\n# severe_toxic_mask=severe_toxic_mask[:,:,1]\nsubset=train[train.severe_toxic==1]\ntext=subset.comment_text.values\n# FIRST-AUTHOR: remove plotting\n# wc= WordCloud(background_color=\"black\",max_words=2000,mask=severe_toxic_mask,stopwords=stopword)\n# wc.generate(\" \".join(text))\n# plt.axis(\"off\")\n# plt.title(\"Words frequented in Severe Toxic Comments\", fontsize=20)\n# plt.imshow(wc.recolor(colormap= 'Reds' , random_state=244), alpha=0.98)\n\n#Threat comments\n# plt.subplot(223)\n# threat_mask=np.array(Image.open(\"./input/anger.png\"))\n# threat_mask=threat_mask[:,:,1]\nsubset=train[train.threat==1]\ntext=subset.comment_text.values\n# FIRST-AUTHOR: remove plotting\n# wc= WordCloud(background_color=\"black\",max_words=2000,mask=threat_mask,stopwords=stopword)\n# wc.generate(\" \".join(text))\n# plt.axis(\"off\")\n# plt.title(\"Words frequented in Threatening Comments\", fontsize=20)\n# plt.imshow(wc.recolor(colormap= 'summer' , random_state=2534), alpha=0.98)\n\n#insult\n# plt.subplot(224)\n# insult_mask=np.array(Image.open(\"./input/swords.png\"))\n# insult_mask=insult_mask[:,:,1]\nsubset=train[train.insult==1]\ntext=subset.comment_text.values\n# FIRST-AUTHOR: remove plotting\n# wc= WordCloud(background_color=\"black\",max_words=2000,mask=insult_mask,stopwords=stopword)\n# wc.generate(\" \".join(text))\n# plt.axis(\"off\")\n# plt.title(\"Words frequented in insult Comments\", fontsize=20)\n# plt.imshow(wc.recolor(colormap= 'Paired_r' , random_state=244), alpha=0.98)\n\n# plt.show()\n",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 7215176
    },
    {
      "raw": "merge=pd.concat([train.iloc[:,0:2],test.iloc[:,0:2]])\ndf=merge.reset_index(drop=True)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 16824028
    },
    {
      "raw": "## Indirect features\n\n#Sentense count in each comment:\n    #  '\\n' can be used to count the number of sentences in each comment\ndf['count_sent']=df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n#Word count in each comment:\ndf['count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n#Unique word count\ndf['count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n#Letter count\ndf['count_letters']=df[\"comment_text\"].apply(lambda x: len(str(x)))\n#punctuation count\ndf[\"count_punctuations\"] =df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#upper case words count\ndf[\"count_words_upper\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n#title case words count\ndf[\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n#Number of stopwords\n# FIRST-AUTHOR: remove ML code\n# df[\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#Average length of the words\ndf[\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 17944453170
    },
    {
      "raw": "#derived features\n#Word count percent in each comment:\ndf['word_unique_percent']=df['count_unique_word']*100/df['count_word']\n#derived features\n#Punct percent in each comment:\ndf['punct_percent']=df['count_punctuations']*100/df['count_word']",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 7683857
    },
    {
      "raw": "#serperate train and test features\ntrain_feats=df.iloc[0:len(train),]\ntest_feats=df.iloc[len(train):,]\n#join the tags\ntrain_tags=train.iloc[:,2:]\ntrain_feats=pd.concat([train_feats,train_tags],axis=1)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 18378187
    },
    {
      "raw": "train_feats['count_sent'].loc[train_feats['count_sent']>10] = 10 \n# FIRST-AUTHOR: remove plotting\n# plt.figure(figsize=(12,6))\n# ## sentenses\n# plt.subplot(121)\n# plt.suptitle(\"Are longer comments more toxic?\",fontsize=20)\n# sns.violinplot(y='count_sent',x='clean', data=train_feats,split=True)\n# plt.xlabel('Clean?', fontsize=12)\n# plt.ylabel('# of sentences', fontsize=12)\n# plt.title(\"Number of sentences in each comment\", fontsize=15)\n# words\ntrain_feats['count_word'].loc[train_feats['count_word']>200] = 200\n# FIRST-AUTHOR: remove plotting\n# plt.subplot(122)\n# sns.violinplot(y='count_word',x='clean', data=train_feats,split=True,inner=\"quart\")\n# plt.xlabel('Clean?', fontsize=12)\n# plt.ylabel('# of words', fontsize=12)\n# plt.title(\"Number of words in each comment\", fontsize=15)\n\n# plt.show()",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 2720801
    },
    {
      "raw": "train_feats['count_unique_word'].loc[train_feats['count_unique_word']>200] = 200\n#prep for split violin plots\n#For the desired plots , the data must be in long format\ntemp_df = pd.melt(train_feats, value_vars=['count_word', 'count_unique_word'], id_vars='clean')\n#spammers - comments with less than 40% unique words\nspammers=train_feats[train_feats['word_unique_percent']<30]",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 21172349
    },
    {
      "raw": "# FIRST-AUTHOR: remove plotting\n# plt.figure(figsize=(16,12))\n# plt.suptitle(\"What's so unique ?\",fontsize=20)\n# gridspec.GridSpec(2,2)\n# plt.subplot2grid((2,2),(0,0))\n# sns.violinplot(x='variable', y='value', hue='clean', data=temp_df,split=True,inner='quartile')\n# plt.title(\"Absolute wordcount and unique words count\")\n# plt.xlabel('Feature', fontsize=12)\n# plt.ylabel('Count', fontsize=12)\n\n# plt.subplot2grid((2,2),(0,1))\n# plt.title(\"Percentage of unique words of total words in comment\")\n# #sns.boxplot(x='clean', y='word_unique_percent', data=train_feats)\n# ax=sns.kdeplot(train_feats[train_feats.clean == 0].word_unique_percent, label=\"Bad\",shade=True,color='r')\n# ax=sns.kdeplot(train_feats[train_feats.clean == 1].word_unique_percent, label=\"Clean\")\n# plt.legend()\n# plt.ylabel('Number of occurances', fontsize=12)\n# plt.xlabel('Percent unique words', fontsize=12)\n_ = train_feats[train_feats.clean == 0].word_unique_percent\n_ = train_feats[train_feats.clean == 0].word_unique_percent\n\nx=spammers.iloc[:,-7:].sum()\n# plt.subplot2grid((2,2),(1,0),colspan=2)\n# plt.title(\"Count of comments with low(<30%) unique words\",fontsize=15)\n# ax=sns.barplot(x=x.index, y=x.values,color=color[3])\n_ = x.index\n_ = x.values\n\n#adding the text labels\n# FIRST-AUTHOR: remove plotting\n# rects = ax.patches\nlabels = x.values\n# FIRST-AUTHOR: remove plotting\n# for rect, label in zip(rects, labels):\n#     height = rect.get_height()\n#     ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\n# plt.xlabel('Threat class', fontsize=12)\n# plt.ylabel('# of comments', fontsize=12)\n# plt.show()",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 7016037
    },
    {
      "raw": "print(\"Clean Spam example:\")\nprint(spammers[spammers.clean==1].comment_text.iloc[1])\nprint(\"Toxic Spam example:\")\nprint(spammers[spammers.toxic==1].comment_text.iloc[2])",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 1225110
    },
    {
      "raw": "#Leaky features\ndf['ip']=df[\"comment_text\"].apply(lambda x: re.findall(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",str(x)))\n#count of ip addresses\ndf['count_ip']=df[\"ip\"].apply(lambda x: len(x))\n\n#links\ndf['link']=df[\"comment_text\"].apply(lambda x: re.findall(\"http://.*com\",str(x)))\n#count of links\ndf['count_links']=df[\"link\"].apply(lambda x: len(x))\n\n#article ids\ndf['article_id']=df[\"comment_text\"].apply(lambda x: re.findall(\"\\d:\\d\\d\\s{0,5}$\",str(x)))\ndf['article_id_flag']=df.article_id.apply(lambda x: len(x))\n\n#username\n##              regex for     Match anything with [[User: ---------- ]]\n# regexp = re.compile(\"\\[\\[User:(.*)\\|\")\ndf['username']=df[\"comment_text\"].apply(lambda x: re.findall(\"\\[\\[User(.*)\\|\",str(x)))\n#count of username mentions\ndf['count_usernames']=df[\"username\"].apply(lambda x: len(x))\n#check if features are created\n#df.username[df.count_usernames>0]\n\n# Leaky Ip\n# FIRST-AUTHOR: remove ML code\n# cv = CountVectorizer()\n# count_feats_ip = cv.fit_transform(df[\"ip\"].apply(lambda x : str(x)))\n\n\n# # Leaky usernames\n\n# cv = CountVectorizer()\n# count_feats_user = cv.fit_transform(df[\"username\"].apply(lambda x : str(x)))\n\n",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 4255934499
    },
    {
      "raw": "df[df.count_usernames!=0].comment_text.iloc[0]",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 85503921
    },
    {
      "raw": "# check few names\n# FIRST-AUTHOR: remove ML code\n# cv.get_feature_names()[120:130]",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 186953
    },
    {
      "raw": "leaky_feats=df[[\"ip\",\"link\",\"article_id\",\"username\",\"count_ip\",\"count_links\",\"count_usernames\",\"article_id_flag\"]]\nleaky_feats_train=leaky_feats.iloc[:train.shape[0]]\nleaky_feats_test=leaky_feats.iloc[train.shape[0]:]",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 12378338
    },
    {
      "raw": "#filterout the entries without ips\ntrain_ips=leaky_feats_train.ip[leaky_feats_train.count_ip!=0]\ntest_ips=leaky_feats_test.ip[leaky_feats_test.count_ip!=0]\n#get the unique list of ips in test and train datasets\ntrain_ip_list=list(set([a for b in train_ips.tolist() for a in b]))\ntest_ip_list=list(set([a for b in test_ips.tolist() for a in b]))\n\n# get common elements\n# FIRST-AUTHOR: remove plotting\n# common_ip_list=list(set(train_ip_list).intersection(test_ip_list))\n# plt.title(\"Common IP addresses\")\n# venn.venn2(subsets=(len(train_ip_list),len(test_ip_list),len(common_ip_list)),set_labels=(\"# of unique IP in train\",\"# of unique IP in test\"))\n# plt.show()",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 5966496
    },
    {
      "raw": "#filterout the entries without links\ntrain_links=leaky_feats_train.link[leaky_feats_train.count_links!=0]\ntest_links=leaky_feats_test.link[leaky_feats_test.count_links!=0]\n#get the unique list of ips in test and train datasets\ntrain_links_list=list(set([a for b in train_links.tolist() for a in b]))\ntest_links_list=list(set([a for b in test_links.tolist() for a in b]))\n\n# get common elements\n# FIRST-AUTHOR: remove plotting\n# common_links_list=list(set(train_links_list).intersection(test_links_list))\n# plt.title(\"Common links\")\n# venn.venn2(subsets=(len(train_links_list),len(test_links_list),len(common_links_list)),\n#            set_labels=(\"# of unique links in train\",\"# of unique links in test\"))\n# plt.show()",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 3107266
    },
    {
      "raw": "#filterout the entries without users\ntrain_users=leaky_feats_train.username[leaky_feats_train.count_usernames!=0]\ntest_users=leaky_feats_test.username[leaky_feats_test.count_usernames!=0]\n#get the unique list of ips in test and train datasets\ntrain_users_list=list(set([a for b in train_users.tolist() for a in b]))\ntest_users_list=list(set([a for b in test_users.tolist() for a in b]))\n\n# get common elements\n# FIRST-AUTHOR: remove plotting\n# common_users_list=list(set(train_users_list).intersection(test_users_list))\n# plt.title(\"Common usernames\")\n# venn.venn2(subsets=(len(train_users_list),len(test_users_list),len(common_users_list)),\n#            set_labels=(\"# of unique Usernames in train\",\"# of unique usernames in test\"))\n# plt.show()",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 1537505
    },
    {
      "raw": "#https://en.wikipedia.org/wiki/Wikipedia:Database_reports/Indefinitely_blocked_IPs)\n\nblocked_ips=[\"216.102.6.176\",\n\"216.120.176.2\",\n\"203.25.150.5\",\n\"203.217.8.30\",\n\"66.90.101.58\",\n\"125.178.86.75\",\n\"210.15.217.194\",\n\"69.36.166.207\",\n\"213.25.24.253\",\n\"24.60.181.235\",\n\"71.204.14.32\",\n\"216.91.92.18\",\n\"212.219.2.4\",\n\"194.74.190.162\",\n\"64.15.152.246\",\n\"59.100.76.166\",\n\"146.145.221.129\",\n\"146.145.221.130\",\n\"74.52.44.34\",\n\"68.5.96.201\",\n\"65.184.176.45\",\n\"209.244.43.209\",\n\"82.46.9.168\",\n\"209.200.236.32\",\n\"209.200.229.181\",\n\"202.181.99.22\",\n\"220.233.226.170\",\n\"212.138.64.178\",\n\"220.233.227.249\",\n\"72.14.194.31\",\n\"72.249.45.0/24\",\n\"72.249.44.0/24\",\n\"80.175.39.213\",\n\"81.109.164.45\",\n\"64.157.15.0/24\",\n\"208.101.10.54\",\n\"216.157.200.254\",\n\"72.14.192.14\",\n\"204.122.16.13\",\n\"217.156.39.245\",\n\"210.11.188.16\",\n\"210.11.188.17\",\n\"210.11.188.18\",\n\"210.11.188.19\",\n\"210.11.188.20\",\n\"64.34.27.153\",\n\"209.68.139.150\",\n\"152.163.100.0/24\",\n\"65.175.48.2\",\n\"131.137.245.197\",\n\"131.137.245.199\",\n\"131.137.245.200\",\n\"64.233.172.37\",\n\"66.99.182.25\",\n\"67.43.21.12\",\n\"66.249.85.85\",\n\"65.175.134.11\",\n\"201.218.3.198\",\n\"193.213.85.12\",\n\"131.137.245.198\",\n\"83.138.189.74\",\n\"72.14.193.163\",\n\"66.249.84.69\",\n\"209.204.71.2\",\n\"80.217.153.189\",\n\"83.138.136.92\",\n\"83.138.136.91\",\n\"83.138.189.75\",\n\"83.138.189.76\",\n\"212.100.250.226\",\n\"212.100.250.225\",\n\"212.159.98.189\",\n\"87.242.116.201\",\n\"74.53.243.18\",\n\"213.219.59.96/27\",\n\"212.219.82.37\",\n\"203.38.149.226\",\n\"66.90.104.22\",\n\"125.16.137.130\",\n\"66.98.128.0/17\",\n\"217.33.236.2\",\n\"24.24.200.113\",\n\"152.22.0.254\",\n\"59.145.89.17\",\n\"71.127.224.0/20\",\n\"65.31.98.71\",\n\"67.53.130.69\",\n\"204.130.130.0/24\",\n\"72.14.193.164\",\n\"65.197.143.214\",\n\"202.60.95.235\",\n\"69.39.89.95\",\n\"88.80.215.14\",\n\"216.218.214.2\",\n\"81.105.175.201\",\n\"203.108.239.12\",\n\"74.220.207.168\",\n\"206.253.55.206\",\n\"206.253.55.207\",\n\"206.253.55.208\",\n\"206.253.55.209\",\n\"206.253.55.210\",\n\"66.64.56.194\",\n\"70.91.90.226\",\n\"209.60.205.96\",\n\"202.173.191.210\",\n\"169.241.10.83\",\n\"91.121.195.205\",\n\"216.70.136.88\",\n\"72.228.151.208\",\n\"66.197.167.120\",\n\"212.219.232.81\",\n\"208.86.225.40\",\n\"63.232.20.2\",\n\"206.219.189.8\",\n\"212.219.14.0/24\",\n\"165.228.71.6\",\n\"99.230.151.129\",\n\"72.91.11.99\",\n\"173.162.177.53\",\n\"60.242.166.182\",\n\"212.219.177.34\",\n\"12.104.27.5\",\n\"85.17.92.13\",\n\"91.198.174.192/27\",\n\"155.246.98.61\",\n\"71.244.123.63\",\n\"81.144.152.130\",\n\"198.135.70.1\",\n\"71.255.126.146\",\n\"74.180.82.59\",\n\"206.158.2.80\",\n\"64.251.53.34\",\n\"24.29.92.238\",\n\"76.254.235.105\",\n\"68.96.242.239\",\n\"203.202.234.226\",\n\"173.72.89.88\",\n\"87.82.229.195\",\n\"68.153.245.37\",\n\"216.240.128.0/19\",\n\"72.46.129.44\",\n\"66.91.35.165\",\n\"82.71.49.124\",\n\"69.132.171.231\",\n\"75.145.183.129\",\n\"194.80.20.237\",\n\"98.207.253.170\",\n\"76.16.222.162\",\n\"66.30.100.130\",\n\"96.22.29.23\",\n\"76.168.140.158\",\n\"202.131.166.252\",\n\"89.207.212.99\",\n\"81.169.155.246\",\n\"216.56.8.66\",\n\"206.15.235.10\",\n\"115.113.95.20\",\n\"204.209.59.11\",\n\"27.33.141.67\",\n\"41.4.65.162\",\n\"99.6.65.6\",\n\"60.234.239.169\",\n\"2620:0:862:101:0:0:2:0/124\",\n\"183.192.165.31\",\n\"50.68.6.12\",\n\"37.214.82.134\",\n\"96.50.0.230\",\n\"60.231.28.109\",\n\"64.90.240.50\",\n\"49.176.97.12\",\n\"209.80.150.137\",\n\"24.22.67.116\",\n\"206.180.81.2\",\n\"195.194.39.100\",\n\"87.41.52.6\",\n\"169.204.164.227\",\n\"50.137.55.117\",\n\"50.77.84.161\",\n\"90.202.230.247\",\n\"186.88.129.224\",\n\"2A02:EC80:101:0:0:0:2:0/124\",\n\"142.4.117.177\",\n\"86.40.105.198\",\n\"120.43.20.149\",\n\"198.199.64.0/18\",\n\"192.34.56.0/21\",\n\"192.81.208.0/20\",\n\"2604:A880:0:0:0:0:0:0/32\",\n\"108.72.107.229\",\n\"2602:306:CC2B:7000:41D3:B92D:731C:959D\",\n\"185.15.59.201\",\n\"180.149.1.229\",\n\"207.191.188.66\",\n\"210.22.63.92\",\n\"117.253.196.217\",\n\"119.160.119.172\",\n\"90.217.133.223\",\n\"194.83.8.3\",\n\"194.83.164.22\",\n\"217.23.228.149\",\n\"65.18.58.1\",\n\"168.11.15.2\",\n\"65.182.127.31\",\n\"207.106.153.252\",\n\"64.193.88.2\",\n\"152.26.71.2\",\n\"199.185.67.179\",\n\"117.90.240.73\",\n\"108.176.58.170\",\n\"195.54.40.28\",\n\"185.35.164.109\",\n\"192.185.0.0/16\",\n\"2605:E000:1605:C0C0:3D3D:A148:3039:71F1\",\n\"107.158.0.0/16\",\n\"85.159.232.0/21\",\n\"69.235.4.10\",\n\"86.176.166.206\",\n\"108.65.152.51\",\n\"10.4.1.0/24\",\n\"103.27.227.139\",\n\"188.55.31.191\",\n\"188.53.13.34\",\n\"176.45.58.252\",\n\"176.45.22.37\",\n\"24.251.44.140\",\n\"108.200.140.191\",\n\"117.177.169.4\",\n\"72.22.162.38\",\n\"24.106.242.82\",\n\"79.125.190.93\",\n\"107.178.200.1\",\n\"123.16.244.246\",\n\"83.228.167.87\",\n\"128.178.197.53\",\n\"14.139.172.18\",\n\"207.108.136.254\",\n\"184.152.17.217\",\n\"186.94.29.73\",\n\"217.200.199.2\",\n\"66.58.141.104\",\n\"166.182.81.30\",\n\"89.168.206.116\",\n\"92.98.163.145\",\n\"77.115.31.71\",\n\"178.36.118.74\",\n\"157.159.10.14\",\n\"103.5.212.139\",\n\"203.174.180.226\",\n\"69.123.252.95\",\n\"199.200.123.233\",\n\"121.45.89.82\",\n\"71.228.87.155\",\n\"68.189.67.92\",\n\"216.161.176.152\",\n\"98.17.30.139\",\n\"2600:1006:B124:84BD:0:0:0:103\",\n\"117.161.0.0/16\",\n\"12.166.68.34\",\n\"96.243.149.64\",\n\"74.143.90.218\",\n\"76.10.176.221\",\n\"104.250.128.0/19\",\n\"185.22.183.128/25\",\n\"89.105.194.64/26\",\n\"202.45.119.0/24\",\n\"73.9.140.64\",\n\"164.127.71.72\",\n\"50.160.129.2\",\n\"49.15.213.207\",\n\"83.7.192.0/18\",\n\"201.174.63.79\",\n\"2A02:C7D:4643:8F00:D09D:BE1:D2DE:BB1F\",\n\"125.60.195.230\",\n\"49.145.113.145\",\n\"168.18.160.134\",\n\"72.193.218.222\",\n\"199.216.164.10\",\n\"120.144.130.89\",\n\"104.130.67.208\",\n\"50.160.221.147\",\n\"163.47.141.50\",\n\"91.200.12.136\",\n\"83.222.0.0/19\",\n\"67.231.16.0/20\",\n\"72.231.0.196\",\n\"180.216.68.197\",\n\"183.160.178.135\",\n\"183.160.176.16\",\n\"24.25.221.150\",\n\"92.222.109.43\",\n\"142.134.243.215\",\n\"216.181.221.72\",\n\"113.205.170.110\",\n\"74.142.2.98\",\n\"192.235.8.3\",\n\"2402:4000:BBFC:36FC:E469:F2F0:9351:71A0\",\n\"80.244.81.191\",\n\"2607:FB90:1377:F765:D45D:46BF:81EA:9773\",\n\"2600:1009:B012:7D88:418B:54BA:FCBC:4584\",\n\"104.237.224.0/19\",\n\"2600:1008:B01B:E495:C05A:7DD3:926:E83C\",\n\"168.8.249.234\",\n\"162.211.179.36\",\n\"138.68.0.0/16\",\n\"145.236.37.195\",\n\"67.205.128.0/18\",\n\"2A02:C7D:2832:CE00:B914:19D6:948D:B37D\",\n\"107.77.203.212\",\n\"2607:FB90:65C:A136:D46F:23BA:87C2:3D10\",\n\"2A02:C7F:DE2F:7900:5D64:E991:FFF0:FA93\",\n\"82.23.32.186\",\n\"106.76.243.74\",\n\"82.33.48.223\",\n\"180.216.160.0/19\",\n\"94.102.184.35\",\n\"94.102.184.26\",\n\"109.92.162.54\",\n\"2600:8800:7180:BF00:4C27:4591:347C:736C\",\n\"178.41.186.50\",\n\"184.97.134.128\",\n\"176.221.32.0/22\",\n\"207.99.40.142\",\n\"109.97.241.134\",\n\"82.136.64.19\",\n\"91.236.74.119\",\n\"197.210.0.0/16\",\n\"173.230.128.0/19\",\n\"162.216.16.0/22\",\n\"80.111.222.211\",\n\"191.37.28.21\",\n\"124.124.103.194\",\n\"50.207.7.198\",\n\"220.233.131.98\",\n\"107.77.241.11\",\n\"68.112.39.0/27\",\n\"173.236.128.0/17\",\n\"49.49.240.24\",\n\"96.31.10.178\",\n\"50.251.229.75\"]\n",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 3408240
    },
    {
      "raw": "train_ip_list=list(set([a for b in train_ips.tolist() for a in b]))\ntest_ip_list=list(set([a for b in test_ips.tolist() for a in b]))\n\n# get common elements\nblocked_ip_list_train=list(set(train_ip_list).intersection(blocked_ips))\nblocked_ip_list_test=list(set(test_ip_list).intersection(blocked_ips))\n\nprint(\"There are\",len(blocked_ip_list_train),\"blocked IPs in train dataset\")\nprint(\"There are\",len(blocked_ip_list_test),\"blocked IPs in test dataset\")",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 2659229
    },
    {
      "raw": "# FIRST-AUTHOR: remove extra display code\n# end_time=time.time()\n# print(\"total time till Leaky feats\",end_time-start_time)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 114940
    },
    {
      "raw": "corpus=merge.comment_text",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 266537
    },
    {
      "raw": "#https://drive.google.com/file/d/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM/view\n# Aphost lookup dict\nAPPO = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"\n}",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 1101335
    },
    {
      "raw": "def clean(comment):\n    \"\"\"\n    This function receives comments and returns clean word-list\n    \"\"\"\n    #Convert to lower case , so that Hi and hi are the same\n    comment=comment.lower()\n    #remove \\n\n    comment=re.sub(\"\\\\n\",\"\",comment)\n    # remove leaky elements like ip,user\n    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n    #removing usernames\n    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n    \n    #Split the sentences into words\n# FIRST-AUTHOR: remove ML code\n#     words=tokenizer.tokenize(comment)\n    \n#     # (')aphostophe  replacement (ie)   you're --> you are  \n#     # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n#     words=[APPO[word] if word in APPO else word for word in words]\n#     words=[lem.lemmatize(word, \"v\") for word in words]\n#     words = [w for w in words if not w in eng_stopwords]\n    \n#     clean_sent=\" \".join(words)\n#     # remove any non alphanum,digit character\n#     #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n#     #clean_sent=re.sub(\"  \",\" \",clean_sent)\n#     return(clean_sent)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 504542
    },
    {
      "raw": "corpus.iloc[12235]",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 234994
    },
    {
      "raw": "clean(corpus.iloc[12235])",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 355839
    },
    {
      "raw": "clean_corpus=corpus.apply(lambda x :clean(x))\n\n# FIRST-AUTHOR: remove extra display code\n# end_time=time.time()\n# print(\"total time till Cleaning\",end_time-start_time)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 2943842699
    },
    {
      "raw": "# To do next:\n# Slang lookup dictionary for sentiments\n#http://slangsd.com/data/SlangSD.zip\n#http://arxiv.org/abs/1608.05129\n# dict lookup \n#https://bytes.com/topic/python/answers/694819-regular-expression-dictionary-key-search\n",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 289817
    },
    {
      "raw": "### Unigrams -- TF-IDF \n# using settings recommended here for TF-IDF -- https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n\n#some detailed description of the parameters\n# min_df=10 --- ignore terms that appear lesser than 10 times \n# max_features=None  --- Create as many words as present in the text corpus\n    # changing max_features to 10k for memmory issues\n# analyzer='word'  --- Create features from words (alternatively char can also be used)\n# ngram_range=(1,1)  --- Use only one word at a time (unigrams)\n# strip_accents='unicode' -- removes accents\n# use_idf=1,smooth_idf=1 --- enable IDF\n# sublinear_tf=1   --- Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf)\n\n\n#temp settings to min=200 to facilitate top features section to run in kernals\n#change back to min=10 to get better results\n# FIRST-AUTHOR: remove ML code\n# start_unigrams=time.time()\n# tfv = TfidfVectorizer(min_df=200,  max_features=10000, \n#             strip_accents='unicode', analyzer='word',ngram_range=(1,1),\n#             use_idf=1,smooth_idf=1,sublinear_tf=1,\n#             stop_words = 'english')\n# tfv.fit(clean_corpus)\n# features = np.array(tfv.get_feature_names())\n\n# train_unigrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\n# test_unigrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])\n_ = clean_corpus.iloc[:train.shape[0]]\n_ = clean_corpus.iloc[train.shape[0]:]",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 677840
    },
    {
      "raw": "#https://buhrmann.github.io/tfidf-analysis.html\ndef top_tfidf_feats(row, features, top_n=25):\n    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df\n\ndef top_feats_in_doc(Xtr, features, row_id, top_n=25):\n    ''' Top tfidf features in specific document (matrix row) '''\n    row = np.squeeze(Xtr[row_id].toarray())\n    return top_tfidf_feats(row, features, top_n)\n\ndef top_mean_feats(Xtr, features, grp_ids, min_tfidf=0.1, top_n=25):\n    ''' Return the top n features that on average are most important amongst documents in rows\n        indentified by indices in grp_ids. '''\n    \n    D = Xtr[grp_ids].toarray()\n\n    D[D < min_tfidf] = 0\n    tfidf_means = np.mean(D, axis=0)\n    return top_tfidf_feats(tfidf_means, features, top_n)\n\n# modified for multilabel milticlass\ndef top_feats_by_class(Xtr, features, min_tfidf=0.1, top_n=20):\n    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n        calculated across documents with the same class label. '''\n    dfs = []\n    cols=train_tags.columns\n    for col in cols:\n        ids = train_tags.index[train_tags[col]==1]\n        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n        feats_df.label = label\n        dfs.append(feats_df)\n    return dfs",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 1634811
    },
    {
      "raw": "#get top n for unigrams\n# FIRST-AUTHOR: remove ML code\n# tfidf_top_n_per_lass=top_feats_by_class(train_unigrams,features)\n\n# end_unigrams=time.time()\n\n# print(\"total time in unigrams\",end_unigrams-start_unigrams)\n# print(\"total time till unigrams\",end_unigrams-start_time)\n",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 142542
    },
    {
      "raw": "# FIRST-AUTHOR: remove plotting\n# plt.figure(figsize=(16,22))\n# plt.suptitle(\"TF_IDF Top words per class(unigrams)\",fontsize=20)\n# gridspec.GridSpec(4,2)\n# plt.subplot2grid((4,2),(0,0))\n# sns.barplot(tfidf_top_n_per_lass[0].feature.iloc[0:9],tfidf_top_n_per_lass[0].tfidf.iloc[0:9],color=color[0])\n# plt.title(\"class : Toxic\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n# plt.subplot2grid((4,2),(0,1))\n# sns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:9],tfidf_top_n_per_lass[1].tfidf.iloc[0:9],color=color[1])\n# plt.title(\"class : Severe toxic\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n\n# plt.subplot2grid((4,2),(1,0))\n# sns.barplot(tfidf_top_n_per_lass[2].feature.iloc[0:9],tfidf_top_n_per_lass[2].tfidf.iloc[0:9],color=color[2])\n# plt.title(\"class : Obscene\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n\n# plt.subplot2grid((4,2),(1,1))\n# sns.barplot(tfidf_top_n_per_lass[3].feature.iloc[0:9],tfidf_top_n_per_lass[3].tfidf.iloc[0:9],color=color[3])\n# plt.title(\"class : Threat\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n\n# plt.subplot2grid((4,2),(2,0))\n# sns.barplot(tfidf_top_n_per_lass[4].feature.iloc[0:9],tfidf_top_n_per_lass[4].tfidf.iloc[0:9],color=color[4])\n# plt.title(\"class : Insult\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n\n# plt.subplot2grid((4,2),(2,1))\n# sns.barplot(tfidf_top_n_per_lass[5].feature.iloc[0:9],tfidf_top_n_per_lass[5].tfidf.iloc[0:9],color=color[5])\n# plt.title(\"class : Identity hate\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n\n# plt.subplot2grid((4,2),(3,0),colspan=2)\n# sns.barplot(tfidf_top_n_per_lass[6].feature.iloc[0:19],tfidf_top_n_per_lass[6].tfidf.iloc[0:19])\n# plt.title(\"class : Clean\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n# plt.show()",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 278701
    },
    {
      "raw": "\n#temp settings to min=150 to facilitate top features section to run in kernals\n#change back to min=10 to get better results\n# FIRST-AUTHOR: remove ML code\n# tfv = TfidfVectorizer(min_df=150,  max_features=30000, \n#             strip_accents='unicode', analyzer='word',ngram_range=(2,2),\n#             use_idf=1,smooth_idf=1,sublinear_tf=1,\n#             stop_words = 'english')\n\n# tfv.fit(clean_corpus)\n# features = np.array(tfv.get_feature_names())\n# train_bigrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\n# test_bigrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])\n# #get top n for bigrams\n# tfidf_top_n_per_lass=top_feats_by_class(train_bigrams,features)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 152460
    },
    {
      "raw": "# FIRST-AUTHOR: remove ML code, plotting\n# plt.figure(figsize=(16,22))\n# plt.suptitle(\"TF_IDF Top words per class(Bigrams)\",fontsize=20)\n# gridspec.GridSpec(4,2)\n# plt.subplot2grid((4,2),(0,0))\n# sns.barplot(tfidf_top_n_per_lass[0].feature.iloc[0:5],tfidf_top_n_per_lass[0].tfidf.iloc[0:5],color=color[0])\n# plt.title(\"class : Toxic\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n# plt.subplot2grid((4,2),(0,1))\n# sns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:5],tfidf_top_n_per_lass[1].tfidf.iloc[0:5],color=color[1])\n# plt.title(\"class : Severe toxic\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n\n# plt.subplot2grid((4,2),(1,0))\n# sns.barplot(tfidf_top_n_per_lass[2].feature.iloc[0:5],tfidf_top_n_per_lass[2].tfidf.iloc[0:5],color=color[2])\n# plt.title(\"class : Obscene\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n\n# plt.subplot2grid((4,2),(1,1))\n# sns.barplot(tfidf_top_n_per_lass[3].feature.iloc[0:5],tfidf_top_n_per_lass[3].tfidf.iloc[0:5],color=color[3])\n# plt.title(\"class : Threat\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n\n# plt.subplot2grid((4,2),(2,0))\n# sns.barplot(tfidf_top_n_per_lass[4].feature.iloc[0:5],tfidf_top_n_per_lass[4].tfidf.iloc[0:5],color=color[4])\n# plt.title(\"class : Insult\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n\n# plt.subplot2grid((4,2),(2,1))\n# sns.barplot(tfidf_top_n_per_lass[5].feature.iloc[0:5],tfidf_top_n_per_lass[5].tfidf.iloc[0:5],color=color[5])\n# plt.title(\"class : Identity hate\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n\n# plt.subplot2grid((4,2),(3,0),colspan=2)\n# sns.barplot(tfidf_top_n_per_lass[6].feature.iloc[0:9],tfidf_top_n_per_lass[6].tfidf.iloc[0:9])\n# plt.title(\"class : Clean\",fontsize=15)\n# plt.xlabel('Word', fontsize=12)\n# plt.ylabel('TF-IDF score', fontsize=12)\n\n# plt.show()",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 267193
    },
    {
      "raw": "# FIRST-AUTHOR: remove extra display code\n# end_time=time.time()\n# print(\"total time till bigrams\",end_time-start_time)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 106117
    },
    {
      "raw": "# FIRST-AUTHOR: remove ML code\n# tfv = TfidfVectorizer(min_df=100,  max_features=30000, \n#             strip_accents='unicode', analyzer='char',ngram_range=(1,4),\n#             use_idf=1,smooth_idf=1,sublinear_tf=1,\n#             stop_words = 'english')\n\n# tfv.fit(clean_corpus)\n# features = np.array(tfv.get_feature_names())\n# train_charngrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\n# test_charngrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])\n# end_time=time.time()\n# print(\"total time till charngrams\",end_time-start_time)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 139425
    },
    {
      "raw": "#Credis to AlexSanchez https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline-eda-0-052-lb#261316\n#custom NB model\n\n# FIRST-AUTHOR: remove ML code\n# class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n#     def __init__(self, C=1.0, dual=False, n_jobs=1):\n#         self.C = C\n#         self.dual = dual\n#         self.n_jobs = n_jobs\n\n#     def predict(self, x):\n#         # Verify that model has been fit\n#         check_is_fitted(self, ['_r', '_clf'])\n#         return self._clf.predict(x.multiply(self._r))\n\n#     def predict_proba(self, x):\n#         # Verify that model has been fit\n#         check_is_fitted(self, ['_r', '_clf'])\n#         return self._clf.predict_proba(x.multiply(self._r))\n\n#     def fit(self, x, y):\n#         # Check that X and y have correct shape\n#         y = y.values\n#         x, y = check_X_y(x, y, accept_sparse=True)\n\n#         def pr(x, y_i, y):\n#             p = x[y==y_i].sum(0)\n#             return (p+1) / ((y==y_i).sum()+1)\n\n#         self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n#         x_nb = x.multiply(self._r)\n#         self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n#         return self\n    \n# model = NbSvmClassifier(C=4, dual=True, n_jobs=-1)\n",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 219916
    },
    {
      "raw": "# FIRST-AUTHOR: remove ML code\n# SELECTED_COLS=['count_sent', 'count_word', 'count_unique_word',\n#        'count_letters', 'count_punctuations', 'count_words_upper',\n#        'count_words_title', 'count_stopwords', 'mean_word_len',\n#        'word_unique_percent', 'punct_percent']\nSELECTED_COLS=['count_sent', 'count_word', 'count_unique_word',\n       'count_letters', 'count_punctuations', 'count_words_upper',\n       'count_words_title', 'mean_word_len',\n       'word_unique_percent', 'punct_percent']\ntarget_x=train_feats[SELECTED_COLS]\n# target_x\n\nTARGET_COLS=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntarget_y=train_tags[TARGET_COLS]\n\n# Strat k fold due to imbalanced classes\n# split = StratifiedKFold(n_splits=2, random_state=1)\n\n#https://www.kaggle.com/yekenot/toxic-regression",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 7722860
    },
    {
      "raw": "#Just the indirect features -- meta features\n# FIRST-AUTHOR: remove ML code\n# print(\"Using only Indirect features\")\n# model = LogisticRegression(C=3)\n# X_train, X_valid, y_train, y_valid = train_test_split(target_x, target_y, test_size=0.33, random_state=2018)\n# train_loss = []\n# valid_loss = []\n# importance=[]\n# preds_train = np.zeros((X_train.shape[0], len(y_train)))\n# preds_valid = np.zeros((X_valid.shape[0], len(y_valid)))\n# for i, j in enumerate(TARGET_COLS):\n#     print('Class:= '+j)\n#     model.fit(X_train,y_train[j])\n#     preds_valid[:,i] = model.predict_proba(X_valid)[:,1]\n#     preds_train[:,i] = model.predict_proba(X_train)[:,1]\n#     train_loss_class=log_loss(y_train[j],preds_train[:,i])\n#     valid_loss_class=log_loss(y_valid[j],preds_valid[:,i])\n#     print('Trainloss=log loss:', train_loss_class)\n#     print('Validloss=log loss:', valid_loss_class)\n#     importance.append(model.coef_)\n#     train_loss.append(train_loss_class)\n#     valid_loss.append(valid_loss_class)\n# print('mean column-wise log loss:Train dataset', np.mean(train_loss))\n# print('mean column-wise log loss:Validation dataset', np.mean(valid_loss))\n\n# end_time=time.time()\n# print(\"total time till Indirect feat model\",end_time-start_time)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 228193
    },
    {
      "raw": "# FIRST-AUTHOR: remove ML code\n# importance[0][0]",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 118596
    },
    {
      "raw": "# FIRST-AUTHOR: remove plotting\n# plt.figure(figsize=(16,22))\n# plt.suptitle(\"Feature importance for indirect features\",fontsize=20)\n# gridspec.GridSpec(3,2)\n# plt.subplots_adjust(hspace=0.4)\n# plt.subplot2grid((3,2),(0,0))\n# sns.barplot(SELECTED_COLS,importance[0][0],color=color[0])\n# plt.title(\"class : Toxic\",fontsize=15)\n# locs, labels = plt.xticks()\n# plt.setp(labels, rotation=45)\n# plt.xlabel('Feature', fontsize=12)\n# plt.ylabel('Importance', fontsize=12)\n\n# plt.subplot2grid((3,2),(0,1))\n# sns.barplot(SELECTED_COLS,importance[1][0],color=color[1])\n# plt.title(\"class : Severe toxic\",fontsize=15)\n# locs, labels = plt.xticks()\n# plt.setp(labels, rotation=45)\n# plt.xlabel('Feature', fontsize=12)\n# plt.ylabel('Importance', fontsize=12)\n\n# plt.subplot2grid((3,2),(1,0))\n# sns.barplot(SELECTED_COLS,importance[2][0],color=color[2])\n# plt.title(\"class : Obscene\",fontsize=15)\n# locs, labels = plt.xticks()\n# plt.setp(labels, rotation=45)\n# plt.xlabel('Feature', fontsize=12)\n# plt.ylabel('Importance', fontsize=12)\n\n\n\n# plt.subplot2grid((3,2),(1,1))\n# sns.barplot(SELECTED_COLS,importance[3][0],color=color[3])\n# plt.title(\"class : Threat\",fontsize=15)\n# locs, labels = plt.xticks()\n# plt.setp(labels, rotation=45)\n# plt.xlabel('Feature', fontsize=12)\n# plt.ylabel('Importance', fontsize=12)\n\n\n# plt.subplot2grid((3,2),(2,0))\n# sns.barplot(SELECTED_COLS,importance[4][0],color=color[4])\n# plt.title(\"class : Insult\",fontsize=15)\n# locs, labels = plt.xticks()\n# plt.setp(labels, rotation=45)\n# plt.xlabel('Feature', fontsize=12)\n# plt.ylabel('Importance', fontsize=12)\n\n\n# plt.subplot2grid((3,2),(2,1))\n# sns.barplot(SELECTED_COLS,importance[5][0],color=color[5])\n# plt.title(\"class : Identity hate\",fontsize=15)\n# locs, labels = plt.xticks()\n# plt.setp(labels, rotation=45)\n# plt.xlabel('Feature', fontsize=12)\n# plt.ylabel('Importance', fontsize=12)\n\n\n# # plt.subplot2grid((4,2),(3,0),colspan=2)\n# # sns.barplot(SELECTED_COLS,importance[6][0],color=color[0])\n# # plt.title(\"class : Clean\",fontsize=15)\n# # locs, labels = plt.xticks()\n# # plt.setp(labels, rotation=90)\n# # plt.xlabel('Feature', fontsize=12)\n# # plt.ylabel('Importance', fontsize=12)\n\n# plt.show()",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 324961
    },
    {
      "raw": "# FIRST-AUTHOR: remove ML code\n# from scipy.sparse import csr_matrix, hstack\n\n# #Using all direct features\n# print(\"Using all features except leaky ones\")\n# target_x = hstack((train_bigrams,train_charngrams,train_unigrams,train_feats[SELECTED_COLS])).tocsr()\n\n\n# end_time=time.time()\n# print(\"total time till Sparse mat creation\",end_time-start_time)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 129064
    },
    {
      "raw": "# FIRST-AUTHOR: remove ML code\n# model = NbSvmClassifier(C=4, dual=True, n_jobs=-1)\n# X_train, X_valid, y_train, y_valid = train_test_split(target_x, target_y, test_size=0.33, random_state=2018)\n# train_loss = []\n# valid_loss = []\n# preds_train = np.zeros((X_train.shape[0], len(y_train)))\n# preds_valid = np.zeros((X_valid.shape[0], len(y_valid)))\n# for i, j in enumerate(TARGET_COLS):\n#     print('Class:= '+j)\n#     model.fit(X_train,y_train[j])\n#     preds_valid[:,i] = model.predict_proba(X_valid)[:,1]\n#     preds_train[:,i] = model.predict_proba(X_train)[:,1]\n#     train_loss_class=log_loss(y_train[j],preds_train[:,i])\n#     valid_loss_class=log_loss(y_valid[j],preds_valid[:,i])\n#     print('Trainloss=log loss:', train_loss_class)\n#     print('Validloss=log loss:', valid_loss_class)\n#     train_loss.append(train_loss_class)\n#     valid_loss.append(valid_loss_class)\n# print('mean column-wise log loss:Train dataset', np.mean(train_loss))\n# print('mean column-wise log loss:Validation dataset', np.mean(valid_loss))\n\n\n# end_time=time.time()\n# print(\"total time till NB base model creation\",end_time-start_time)",
      "external_calls": {},
      "all_calls": {},
      "total-ns": 183799
    }
  ],
  "total-time-in-sec": 27.130072349,
  "max-disk-in-mb": 0
}