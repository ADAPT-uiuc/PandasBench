{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scale_input_data(scale_factor):\n",
    "  file_bases = ['./input/items', './input/transactions', './input/stores', './input/oil']\n",
    "  for file_base in file_bases:\n",
    "    import pandas as pd\n",
    "    import shutil\n",
    "    if scale_factor == 1.0:\n",
    "      shutil.copyfile(file_base + '.csv', file_base + '.scaled.csv')\n",
    "      continue\n",
    "    df_to_scale = pd.read_csv(file_base + '.csv')\n",
    "    new_num_rows = int(scale_factor * len(df_to_scale))\n",
    "    if scale_factor <= 1.0:\n",
    "      df_to_scale = df_to_scale.iloc[:new_num_rows]\n",
    "    else:\n",
    "      while len(df_to_scale) < new_num_rows:\n",
    "        df_to_scale = pd.concat([df_to_scale, df_to_scale[:min(new_num_rows - len(df_to_scale), len(df_to_scale))]])\n",
    "    df_to_scale.to_csv(file_base + '.scaled.csv', index=False)\n",
    "\n",
    "default_nrows = 25000000\n",
    "if 'INPUT_SCALE_FACTOR' in os.environ:\n",
    "  scale_factor = float(os.environ['INPUT_SCALE_FACTOR'])\n",
    "  scale_input_data(scale_factor)\n",
    "  nrows = int(scale_factor * default_nrows)\n",
    "  with open('./input/data.txt', 'w') as file:\n",
    "    file.write(str(nrows))\n",
    "elif os.path.exists('./input/data.txt'):\n",
    "  with open('./input/data.txt', 'r') as file:\n",
    "    try:\n",
    "      nrows = int(file.read().strip())\n",
    "    except:\n",
    "      nrows = default_nrows\n",
    "else:\n",
    "  nrows = default_nrows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4fd51975-815b-43c3-982f-0bcfecacd540",
    "_uuid": "66a0ff948578bc28a98ad41fb8316bc51aa7c46d"
   },
   "source": [
    "**Introduction**\n",
    "As part of this challenge we are trying to predict sales of various items sold by Favorita retailer. Following are the datasets provided to us:\n",
    "*  train.csv\n",
    "* stores.csv\n",
    "* transactions.csv\n",
    "* items.csv\n",
    "* holidays_events.csv\n",
    "* oils.csv\n",
    "\n",
    "**Description of Dataset**\n",
    "Please find link to dataset explaination here[https://www.kaggle.com/c/favorita-grocery-sales-forecasting/data](http://www.kaggle.com/c/favorita-grocery-sales-forecasting/data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0b9f9b53-8f1d-4b6c-aa17-40e6ee35a2c9",
    "_uuid": "15731920b8a79d10bbc7a823694508f6a567e891"
   },
   "source": [
    "* The first step is to read the data from various datasets and carry out some basic analysis. We use pandas read_csv to read the data set.\n",
    "* The second step is to analyse the dataset by means of various graphs and try to understand /co-relate with different datasets we are given. \n",
    "* The third step is to carry out feature engineering whereby we identify some of the key features from the dataset.\n",
    "* The fourth and final step is to train the model and test it. May be we could do K Means??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aa7925f1-9a3e-43b9-a833-976bfd08e73c",
    "_uuid": "2fc1508d27b64ac3090a089f60b6f273f5a6bf66"
   },
   "source": [
    "**I. Reading various data from input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b93dfbdf-47e3-4f3a-a53a-f53ea4df3fc2",
    "_uuid": "e572722b2230283893a3a2684ea1783bff1a5cbe"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "exec(os.environ['IREWR_IMPORTS'])\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FIRST-AUTHOR: remove ML code\n",
    "# import xgboost as xgb\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# FIRST-AUTHOR: remove path printing\n",
    "# from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"./input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "a63fcd2c-8ce6-4fa8-9533-e1bcfadb9281",
    "_uuid": "adc98ac74e04744b9f43f0762c0b03dd8f3be4ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70463/2752535866.py:2: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv(\"./input/train.csv\", nrows=25000000, parse_dates=['date'],index_col='id')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24999995</th>\n",
       "      <td>2014-06-07</td>\n",
       "      <td>7</td>\n",
       "      <td>413987</td>\n",
       "      <td>12.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999996</th>\n",
       "      <td>2014-06-07</td>\n",
       "      <td>7</td>\n",
       "      <td>414305</td>\n",
       "      <td>13.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999997</th>\n",
       "      <td>2014-06-07</td>\n",
       "      <td>7</td>\n",
       "      <td>414353</td>\n",
       "      <td>21.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999998</th>\n",
       "      <td>2014-06-07</td>\n",
       "      <td>7</td>\n",
       "      <td>414354</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999999</th>\n",
       "      <td>2014-06-07</td>\n",
       "      <td>7</td>\n",
       "      <td>414421</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  store_nbr  item_nbr  unit_sales onpromotion\n",
       "id                                                              \n",
       "24999995 2014-06-07          7    413987        12.0       False\n",
       "24999996 2014-06-07          7    414305        13.0       False\n",
       "24999997 2014-06-07          7    414353        21.0       False\n",
       "24999998 2014-06-07          7    414354         4.0       False\n",
       "24999999 2014-06-07          7    414421         6.0       False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since training data is huge, so I am planning to read few millions of rows from the csv file.\n",
    "train = pd.read_csv(\"./input/train.csv\", nrows=nrows, parse_dates=['date'],index_col='id')\n",
    "\n",
    "#print the last 10 rows of the data, this will help us to think what we can dow with the data.\n",
    "train.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "69c0dedd-aac3-4225-89ef-bd9def6b6600",
    "_uuid": "b1f5a3e1bb5ea7b8edcb12cbb96a385101c71405"
   },
   "source": [
    "Training data and items csv are some way or other way related to each other. So I think lets read the items csv and try to merge with training data which will help us in getting more insight from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "f0283c69-c506-424d-912e-ca50d89d80e9",
    "_uuid": "5a11bf486ac988f92f3f6672e40adf9f6e587c5f"
   },
   "outputs": [],
   "source": [
    "items = pd.read_csv(\"./input/items.scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "e6436fb5-8813-4328-98af-90be11aad07b",
    "_uuid": "131078c577c89abcbf16b1f7fd281e56c560c680"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>family</th>\n",
       "      <th>class</th>\n",
       "      <th>perishable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24999995</th>\n",
       "      <td>2014-06-06</td>\n",
       "      <td>45</td>\n",
       "      <td>1303141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>BREAD/BAKERY</td>\n",
       "      <td>2714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999996</th>\n",
       "      <td>2014-06-06</td>\n",
       "      <td>48</td>\n",
       "      <td>1303141</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>BREAD/BAKERY</td>\n",
       "      <td>2714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999997</th>\n",
       "      <td>2014-06-06</td>\n",
       "      <td>51</td>\n",
       "      <td>1303141</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>BREAD/BAKERY</td>\n",
       "      <td>2714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999998</th>\n",
       "      <td>2014-06-07</td>\n",
       "      <td>3</td>\n",
       "      <td>1303141</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>BREAD/BAKERY</td>\n",
       "      <td>2714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999999</th>\n",
       "      <td>2014-06-07</td>\n",
       "      <td>5</td>\n",
       "      <td>1303141</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>BREAD/BAKERY</td>\n",
       "      <td>2714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  store_nbr  item_nbr  unit_sales onpromotion  \\\n",
       "24999995 2014-06-06         45   1303141         1.0       False   \n",
       "24999996 2014-06-06         48   1303141         3.0       False   \n",
       "24999997 2014-06-06         51   1303141         5.0       False   \n",
       "24999998 2014-06-07          3   1303141         9.0       False   \n",
       "24999999 2014-06-07          5   1303141         8.0       False   \n",
       "\n",
       "                family  class  perishable  \n",
       "24999995  BREAD/BAKERY   2714           1  \n",
       "24999996  BREAD/BAKERY   2714           1  \n",
       "24999997  BREAD/BAKERY   2714           1  \n",
       "24999998  BREAD/BAKERY   2714           1  \n",
       "24999999  BREAD/BAKERY   2714           1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_items = pd.merge(train, items, how='inner')\n",
    "train_items.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a1f521b-a81e-45fd-8d2c-6d12b0f60f22",
    "_uuid": "e64e9bf61e8782ac5acdb4aaa60ba183606a3816"
   },
   "source": [
    "After merging two sets of data (training and items) we can now carry out analysis of items sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "7815df92-3029-4dbb-ab05-806873762f53",
    "_uuid": "ec8186f86fff91914ff2510a499351e3029b902a"
   },
   "outputs": [],
   "source": [
    "#Lets find out most popular item ordered by people across the 6 millions rows we have read.\n",
    "#We will group by item_nbr and add the unit sales.\n",
    "df = train_items['unit_sales'].groupby(train_items['item_nbr']).sum()\n",
    "#In order to find top 10 popular items we will sort the numpy array and pick the top 10 from\n",
    "#the list.\n",
    "df = df.sort_values()\n",
    "df_highest = df.nlargest(n=10)\n",
    "\n",
    "#Plot the highest list of items.\n",
    "# FIRST-AUTHOR: remove plotting\n",
    "# df_highest.plot(kind='bar',figsize = (10,10),  title = \"Top 10 items sold across all stores\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "30dcd027-c350-4118-808d-8f8406464520",
    "_uuid": "23718879358bb37b03af79551458e471db014398"
   },
   "outputs": [],
   "source": [
    "#Next we find lowest/less demand product. We use nsmallest to find the bottom 10 items,\n",
    "#probably it doesn;t matter even if we stock them.\n",
    "df_lowest = df.nsmallest(n=10)\n",
    "# FIRST-AUTHOR: remove plotting\n",
    "# df_lowest.plot(kind='bar',figsize = (10,10),  title = \"Bottom 10 items sold\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "e4404dd6-4fa8-4fa0-a128-9abf40acc007",
    "_uuid": "64d2eec2607e5fd51b3f67f3b46108ab353956d6"
   },
   "outputs": [],
   "source": [
    "#Next we could find out popular items in a given year. This will be useful to find out \n",
    "#if there were any new items introduced in the recent times.\n",
    "#In order to do that we need to covert the date field into python date format and then\n",
    "# extract various fields from it.\n",
    "\n",
    "train_items['date'] = pd.to_datetime(train_items['date'], format='%Y-%m-%d')\n",
    "train_items['day_item_purchased'] = train_items['date'].dt.day\n",
    "train_items['month_item_purchased'] =train_items['date'].dt.month\n",
    "train_items['quarter_item_purchased'] = train_items['date'].dt.quarter\n",
    "train_items['year_item_purchased'] = train_items['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "ff082173-3718-4d1d-9f75-1c4d31439f36",
    "_uuid": "d71cec243dc24849a9b7c875ba82c8a227d85258"
   },
   "outputs": [],
   "source": [
    "train_items.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "ff70e183-49e0-4287-9faa-dc31686263ea",
    "_uuid": "2abee79c4e4aa287b3e4f47f1af717a65be47034",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          store_nbr  item_nbr  unit_sales onpromotion        family  class  \\\n",
      "24999998          3   1303141         9.0       False  BREAD/BAKERY   2714   \n",
      "24999999          5   1303141         8.0       False  BREAD/BAKERY   2714   \n",
      "\n",
      "          perishable  day_item_purchased  month_item_purchased  \\\n",
      "24999998           1                   7                     6   \n",
      "24999999           1                   7                     6   \n",
      "\n",
      "          quarter_item_purchased  year_item_purchased  \n",
      "24999998                       2                 2014  \n",
      "24999999                       2                 2014  \n"
     ]
    }
   ],
   "source": [
    "#Lets print out new training dataset\n",
    "print (train_items.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "2795b121-41c8-47fe-8181-a0948aeabec7",
    "_uuid": "913e4ed9934c099106b4ef77b10bcb1bfe2eb9c6"
   },
   "outputs": [],
   "source": [
    "df_year = train_items.groupby(['quarter_item_purchased', 'item_nbr'])['unit_sales'].sum()\n",
    "df_year = df_year.sort_values()\n",
    "df_year_highest = df_year.nlargest(n=10)\n",
    "#Plot the highest list of items.\n",
    "# FIRST-AUTHOR: remove plotting\n",
    "# df_year_highest.plot(kind='bar',figsize = (10,10),  title = \"Top items sold Quarterly\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "8fa0be6b-60a0-4bbf-b20e-bbe925f01cf2",
    "_uuid": "740afb47640bc944c5f0986f8785748c9301db9c"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove plotting\n",
    "# plt.figure(figsize=(9,10))\n",
    "df_items = train_items.groupby(['family'])['unit_sales'].sum()\n",
    "df_items = df_items.sort_values()\n",
    "df_items_highest = df_items.nlargest(n=10)\n",
    "# FIRST-AUTHOR: remove plotting\n",
    "# plt.pie(df_items_highest, labels=df_items_highest.index,shadow=False,autopct='%1.1f%%')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "_ = df_items_highest.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "f557c239-c61d-410d-bbee-20a136f45151",
    "_uuid": "0eb6fe05f8d16870009a245e2e3a37a420628f0e"
   },
   "outputs": [],
   "source": [
    "grocery_info = train_items.loc[train_items['family'] == 'GROCERY I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "26162c2f-3f61-4a3e-9ea9-7f61da389bce",
    "_uuid": "2adb837eb952a0ad58694281666c5295e4ebc724"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove plotting\n",
    "# plt.figure(figsize=(12,12))\n",
    "# #print (grocery_info.tail(2))\n",
    "# plt.plot(grocery_info['day_item_purchased'],grocery_info['unit_sales'])\n",
    "# plt.show()\n",
    "_ = grocery_info['day_item_purchased']\n",
    "_ = grocery_info['unit_sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "49c2ad10-5350-43e0-b4f2-56cc846bfe61",
    "_uuid": "2972e02459c1da3376ef203ca7eba2fa8f420076"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove plotting\n",
    "# plt.figure(figsize=(9,10))\n",
    "df_items = train_items.groupby(['family','perishable'])['unit_sales'].sum()\n",
    "df_items = df_items.sort_values()\n",
    "df_items_perish_highest = df_items.nlargest(n=10)\n",
    "# FIRST-AUTHOR: remove plotting\n",
    "# plt.pie(df_items_perish_highest, labels=df_items_perish_highest.index,shadow=False,autopct='%1.1f%%')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "_ = df_items_perish_highest.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6424c842-613d-4377-a7b9-bfaecfcb4ff9",
    "_uuid": "79566255ce8a884da047b03e2b167f4726dc9d05",
    "collapsed": true
   },
   "source": [
    "Lets read the transactions data and carry out analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "38b62890-4f15-471e-92b8-3abe3f32fdd9",
    "_uuid": "cc2249cc298cd2537acad24ff771f0ff9dec013e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transaction = pd.read_csv(\"./input/transactions.scaled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5cc9baf1-3904-4cf8-bdff-30971325624c",
    "_uuid": "768786877039b318ce25206618c1417882ad817c"
   },
   "source": [
    "Convert date to pandas data time format, so that  we can group items for a given time frame (monthly, yearly, quarterly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "9e528fec-723b-4180-a90d-eb9d7221a278",
    "_uuid": "51bd0a395b6ebbf861980dd32abec4e06059b320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date  store_nbr  transactions  day_item_purchased  \\\n",
      "83486 2017-08-15         53           932                  15   \n",
      "83487 2017-08-15         54           802                  15   \n",
      "\n",
      "       month_item_purchased  quarter_item_purchased  year_item_purchased  \n",
      "83486                     8                       3                 2017  \n",
      "83487                     8                       3                 2017  \n"
     ]
    }
   ],
   "source": [
    "transaction['date'] = pd.to_datetime(transaction['date'], format='%Y-%m-%d')\n",
    "transaction['day_item_purchased'] = transaction['date'].dt.day\n",
    "transaction['month_item_purchased'] =transaction['date'].dt.month\n",
    "transaction['quarter_item_purchased'] = transaction['date'].dt.quarter\n",
    "transaction['year_item_purchased'] = transaction['date'].dt.year\n",
    "print (transaction.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "c1755234-9b24-4b9b-b29c-18d477fc99cc",
    "_uuid": "9880257a31e3dfb0f58de21d9a9eed0b7f3819d1"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove plotting\n",
    "# plt.figure(figsize=(25,25))\n",
    "# plt.plot(transaction['date'],transaction['transactions'])\n",
    "# plt.show()\n",
    "_ = transaction['date']\n",
    "_ = transaction['transactions']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "48e5b899-b915-43cc-86a2-63a824acdec0",
    "_uuid": "7c148f43dbd760c3c78091edd15c7807f079c498"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove plotting\n",
    "# plt.figure(figsize=(8,12))\n",
    "trans_day = transaction['transactions'].groupby(transaction['year_item_purchased']).sum()\n",
    "# FIRST-AUTHOR: remove plotting\n",
    "# trans_day.plot(kind='bar')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "3d187ac3-6032-4f67-8ca5-176339f69cb8",
    "_uuid": "e6d65f3b5515b21330d30e84fdb2f4d704d40951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store_nbr           city                           state type  cluster\n",
      "0          1          Quito                       Pichincha    D       13\n",
      "1          2          Quito                       Pichincha    D       13\n",
      "2          3          Quito                       Pichincha    D        8\n",
      "3          4          Quito                       Pichincha    D        9\n",
      "4          5  Santo Domingo  Santo Domingo de los Tsachilas    D        4\n"
     ]
    }
   ],
   "source": [
    "stores = pd.read_csv(\"./input/stores.scaled.csv\")\n",
    "print (stores.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "0bc71778-e8df-49a0-8492-0f7ff3533423",
    "_uuid": "4d8c6f579f86136c7381fb0669792840473aa9ad"
   },
   "outputs": [],
   "source": [
    "#Lets find out number of cities in each state, which in nothing but finding out number of stores in each\n",
    "#in each state.\n",
    "df = stores['city'].groupby(stores['state']).count()\n",
    "# FIRST-AUTHOR: remove plotting\n",
    "# df.plot(kind='bar', figsize = (12,8), yticks=np.arange(min(df), max(df)+1, 1.0), title = \"Number of cities in each state\")\n",
    "# plt.show()\n",
    "_ = min(df)\n",
    "_ = max(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "c3bf1e4d-dcd2-4c51-9ab1-bd5b148f818d",
    "_uuid": "b93d34ad491bc322f8e8ba31da6ab75cf0ceb85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "               date  store_nbr  item_nbr  unit_sales\n",
      "id                                                  \n",
      "24999995 2014-06-07          7    413987        12.0\n",
      "24999996 2014-06-07          7    414305        13.0\n",
      "24999997 2014-06-07          7    414353        21.0\n",
      "24999998 2014-06-07          7    414354         4.0\n",
      "24999999 2014-06-07          7    414421         6.0\n"
     ]
    }
   ],
   "source": [
    "#Looks like onpromotion field is always NaN, if so we will get rid of that columns \n",
    "#from the training data\n",
    "print(train['onpromotion'].notnull().any())\n",
    "train_new=train.drop('onpromotion',axis=1)\n",
    "print(train_new.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "34dd322c-83a0-46c0-9ea1-103fa859d9e9",
    "_uuid": "7b47f186b73e83bbd2eac0645788d36b4ebce89a"
   },
   "outputs": [],
   "source": [
    "oils = pd.read_csv(\"./input/oil.scaled.csv\")\n",
    "oils['date'] = pd.to_datetime(oils['date'], format='%Y-%m-%d')\n",
    "oils['day_item_purchased'] = oils['date'].dt.day\n",
    "oils['month_item_purchased'] =oils['date'].dt.month\n",
    "oils['quarter_item_purchased'] = oils['date'].dt.quarter\n",
    "oils['year_item_purchased'] = oils['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "6ee70c4f-2001-4bad-935a-3681797fa2d7",
    "_uuid": "4a2eadfc0d711a618ceb8a49e88589817a1ae892"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove plotting\n",
    "# plt.figure(figsize=(25,25))\n",
    "# #trans_day = transaction['transactions'].groupby(transaction['year_item_purchased']).sum()\n",
    "# plt.plot(oils['date'],oils['dcoilwtico'])\n",
    "# #trans_day.plot(kind='bar')\n",
    "# plt.show()\n",
    "_ = oils['date']\n",
    "_ = oils['dcoilwtico']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}