{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scale_input_data(scale_factor):\n",
    "  file_bases = ['./input/train', './input/test']\n",
    "  for file_base in file_bases:\n",
    "    import pandas as pd\n",
    "    import shutil\n",
    "    if scale_factor == 1.0:\n",
    "      shutil.copyfile(file_base + '.csv', file_base + '.scaled.csv')\n",
    "      continue\n",
    "    df_to_scale = pd.read_csv(file_base + '.csv')\n",
    "    new_num_rows = int(scale_factor * len(df_to_scale))\n",
    "    if scale_factor <= 1.0:\n",
    "      df_to_scale = df_to_scale.iloc[:new_num_rows]\n",
    "    else:\n",
    "      while len(df_to_scale) < new_num_rows:\n",
    "        df_to_scale = pd.concat([df_to_scale, df_to_scale[:min(new_num_rows - len(df_to_scale), len(df_to_scale))]])\n",
    "    df_to_scale.to_csv(file_base + '.scaled.csv', index=False)\n",
    "\n",
    "if 'INPUT_SCALE_FACTOR' in os.environ:\n",
    "  scale_input_data(float(os.environ['INPUT_SCALE_FACTOR']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "25b1e1db-8bc5-7029-f719-91da523bd121",
    "_uuid": "bde6dc7acb47f87a08eefc4a7f7a88ec2195a4fb"
   },
   "source": [
    "## Introduction ##\n",
    "\n",
    "This kernel is forked from Sina's elgant work: [Titanic best working Classifier][1] with family/group survival feature extracted from the data. The family/group information is extracted from name, fare and ticket number through close examing of the data and insparation I got from reading the discussions on the competition. I believe it is the first time this feature has been used as a prediction feature (or at least I have not browsed all the kernels to see it being used :-)). This new feature improved the score by ~1.5% and put the score to be 0.81818. I believe this feature can be used in other models to improve the prediction accuracy as well.\n",
    "\n",
    "  [1]: https://www.kaggle.com/sinakhorami/titanic-best-working-classifier?scriptVersionId=560373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2ce68358-02ec-556d-ba88-e773a50bc18b",
    "_uuid": "e4183dfadf752d5a87c362e6d1251b2dcb6c2b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# FIRST-AUTHOR: remove plotting\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "exec(os.environ['IREWR_IMPORTS'])\n",
    "import re as re\n",
    "\n",
    "train = pd.read_csv('./input/train.scaled.csv', header = 0, dtype={'Age': np.float64})\n",
    "test  = pd.read_csv('./input/test.scaled.csv' , header = 0, dtype={'Age': np.float64})\n",
    "full_data = [train, test]\n",
    "\n",
    "print (train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "9e25e7d50b28d19c5578dd2a8c3b181095703300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f9595646-65c9-6fc4-395f-0befc4d122ce",
    "_uuid": "5bdcb4ae1ad461a60ce80b51849b363e1ad18019"
   },
   "source": [
    "# Feature Engineering #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9b4c278b-aaca-e92c-ba77-b9b48379d1f1",
    "_uuid": "2b76e3eb81519f0c52081e8eab0fd223b56baac7"
   },
   "source": [
    "## 1. Pclass ##\n",
    "there is no missing value on this feature and already a numerical value. so let's check it's impact on our train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "4680d950-cf7d-a6ae-e813-535e2247d88e",
    "_uuid": "e9b74bedc8dc91b38ce43408fe60e2dff3920faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass  Survived\n",
      "0       1  0.629630\n",
      "1       2  0.472826\n",
      "2       3  0.242363\n"
     ]
    }
   ],
   "source": [
    "print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5e70f81c-d4e2-1823-f0ba-a7c9b46984ff",
    "_uuid": "51a1209dc0f803834e951646c837a38217a74f87"
   },
   "source": [
    "## 2. Sex ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "6729681d-7915-1631-78d2-ddf3c35a424c",
    "_uuid": "ff90c89fdf8351331d431163d8de7c0861b297d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sex  Survived\n",
      "0  female  0.742038\n",
      "1    male  0.188908\n"
     ]
    }
   ],
   "source": [
    "print (train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c58b7ee-d6a1-0cc9-2346-81c47846a54a",
    "_uuid": "fb807ec520d999227e8d287fe52486cb407e2780"
   },
   "source": [
    "## 3. SibSp and Parch ##\n",
    "With the number of siblings/spouse and the number of children/parents we can create new feature called Family Size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "1a537f10-7cec-d0b7-8a34-fa9975655190",
    "_uuid": "9e85e6f03f4e296e329fb044b6a2671cbf55c85d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FamilySize  Survived\n",
      "0           1  0.303538\n",
      "1           2  0.552795\n",
      "2           3  0.578431\n",
      "3           4  0.724138\n",
      "4           5  0.200000\n",
      "5           6  0.136364\n",
      "6           7  0.333333\n",
      "7           8  0.000000\n",
      "8          11  0.000000\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "print (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e4861d3e-10db-1a23-8728-44e4d5251844",
    "_uuid": "c0fffbaa1abd65de31ed2f8601e5c3db28d815f2"
   },
   "source": [
    "it seems has a good effect on our prediction but let's go further and categorize people to check whether they are alone in this ship or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "8c35e945-c928-e3bc-bd9c-d6ddb287e4c9",
    "_uuid": "02a35757da1f59c896dc4cbadf4e76f60a224fcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IsAlone  Survived\n",
      "0        0  0.505650\n",
      "1        1  0.303538\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "print (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2780ca4e-7923-b845-0b6b-5f68a45f6b93",
    "_uuid": "59e50ee5a0b175ffa5963406c22023949b7fcf54"
   },
   "source": [
    "good! the impact is considerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8aa419c0-6614-7efc-7797-97f4a5158b19",
    "_uuid": "26e051873df13aa1d61ed4debbce8cfabfeae41a"
   },
   "source": [
    "## 4. Embarked ##\n",
    "the embarked feature has some missing value. and we try to fill those with the most occurred value ( 'S' )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "0e70e9af-d7cc-8c40-b7d4-2643889c376d",
    "_uuid": "1ab3775b7b25cc3c49102e492770b8544327ded6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embarked  Survived\n",
      "0        C  0.553571\n",
      "1        Q  0.389610\n",
      "2        S  0.339009\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "print (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e08c9ee8-d6d1-99b7-38bd-f0042c18a5d9",
    "_uuid": "35ed9241587442e065198a8d57e5afed5e1ab06c"
   },
   "source": [
    "## 5. Fare ##\n",
    "Fare also has some missing value and we will replace it with the median. then we categorize it into 4 ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "a21335bd-4e8d-66e8-e6a5-5d2173b72d3b",
    "_uuid": "88ccfa38c0186e4f8664f306af0bf26fa3fef377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CategoricalFare  Survived\n",
      "0   (-0.001, 7.91]  0.197309\n",
      "1   (7.91, 14.454]  0.303571\n",
      "2   (14.454, 31.0]  0.454955\n",
      "3  (31.0, 512.329]  0.581081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1017638/3679174374.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  print (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "print (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec8d1b22-a95f-9f16-77ab-7b60d2103852",
    "_uuid": "ad890584a4ddc57abf4855e21e3e99b7c56cf1c1"
   },
   "source": [
    "## 6. Age ##\n",
    "we have plenty of missing values in this feature. # generate random numbers between (mean - std) and (mean + std).\n",
    "then we categorize age into 5 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b90c2870-ce5d-ae0e-a33d-59e35445500e",
    "_uuid": "47f85ca83a5f478afbf17d839380bbb39ec0abc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CategoricalAge  Survived\n",
      "0  (-0.08, 16.0]  0.518182\n",
      "1   (16.0, 32.0]  0.346578\n",
      "2   (32.0, 48.0]  0.391129\n",
      "3   (48.0, 64.0]  0.434783\n",
      "4   (64.0, 80.0]  0.090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1017638/3509540359.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
      "/tmp/ipykernel_1017638/3509540359.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
      "/tmp/ipykernel_1017638/3509540359.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
      "/tmp/ipykernel_1017638/3509540359.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
      "/tmp/ipykernel_1017638/3509540359.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  print (train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    age_avg \t   = dataset['Age'].mean()\n",
    "    age_std \t   = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    \n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "    \n",
    "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n",
    "\n",
    "print (train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bd25ec3f-b601-c1cc-d701-991fac1621f9",
    "_uuid": "335df3bdf094997131245f0bec9fbfa4b326e97e"
   },
   "source": [
    "## 7. Name ##\n",
    "Another way of getting the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "ad042f43-bfe0-ded0-4171-379d8caaa749",
    "_uuid": "4886916827c3240685277f88c7d2e2897f07b23a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex           female  male\n",
      "Title                     \n",
      "Capt               0     1\n",
      "Col                0     2\n",
      "Don                0     1\n",
      "Dr                 1     6\n",
      "Jonkheer           0     1\n",
      "Lady               1     0\n",
      "Major              0     2\n",
      "Master             0    40\n",
      "Miss             182     0\n",
      "Mlle               2     0\n",
      "Mme                1     0\n",
      "Mr                 0   517\n",
      "Mrs              125     0\n",
      "Ms                 1     0\n",
      "Rev                0     6\n",
      "Sir                0     1\n",
      "the Countess       1     0\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Title'] = [x[1].split(\".\")[0].strip(\" \") for x in dataset['Name'].str.split(\",\")]\n",
    "\n",
    "print(pd.crosstab(train['Title'], train['Sex']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ca5fff8c-7a0d-6c18-2173-b8df6293c50a",
    "_uuid": "b817e282a391fcefc3311ec6365a46b86a1aa6cb"
   },
   "source": [
    " so we have titles. let's categorize it and check the title impact on survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "8357238b-98fe-632a-acd5-33674a6132ce",
    "_uuid": "27e6677968dc730cff2cbef79decad9bf7050fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Title  Survived\n",
      "0        Master  0.575000\n",
      "1          Miss  0.702703\n",
      "2            Mr  0.156673\n",
      "3           Mrs  0.793651\n",
      "4          Rare  0.318182\n",
      "5  the Countess  1.000000\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "print (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8a5b0876-6cd9-43de-8e9d-372a949b165d",
    "_uuid": "84e9e30fbc7c9fe5356e2f4afa50a0b60ebb8795"
   },
   "source": [
    "# Extracting family information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "18ea6b78-4158-4d23-9b7f-229c71885d0f",
    "_uuid": "36e359795c41f96a8de3a1cabb9125430ea444a5"
   },
   "source": [
    "First we can use last name to divide the passengers into families. And if you closely examin the data, same family are paying the same fare for the tickets. This suggests the fare is for the family. We can use both last name and fare to grout passengers into families in case different families with the same last name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "eaa36709-397b-4b07-811c-ff6ed0b267f8",
    "_uuid": "d0f7b8f713daa292f6aaa9b57422e3742381bff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of families with different fares is: 1.7%\n",
      "Whole family survived: 44.7%\n",
      "Whole family perished: 35.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1017638/2399735875.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df['Fare'].fillna(all_df['Fare'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passenger with family survival information: 420\n",
      "partial age group: 0\n",
      "partial cabin group: 4\n",
      "                0\n",
      "count  165.000000\n",
      "mean     9.448485\n",
      "std     10.742783\n",
      "min      0.000000\n",
      "25%      2.000000\n",
      "50%      6.000000\n",
      "75%     13.000000\n",
      "max     48.000000\n"
     ]
    }
   ],
   "source": [
    "train_size = len(train)\n",
    "test_size = len(test)\n",
    "\n",
    "# FIRST-AUTHOR: make notebook run\n",
    "# all_df = train.append(test)\n",
    "all_df = pd.concat([train, test])\n",
    "all_df = all_df[list(train.columns)]\n",
    "\n",
    "all_df.set_index(['PassengerId'], inplace=True) ## This is to make sure of a unique index for both train & test\n",
    "\n",
    "## Processing family information\n",
    "all_df['Last name'] = all_df['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "all_df['Fare'].fillna(all_df['Fare'].mean(), inplace=True)\n",
    "\n",
    "# The Fare is actually for the whole family\n",
    "fare_df = all_df.loc[all_df['FamilySize']>1, [\"Last name\", \"Fare\", \"FamilySize\"]].iloc[:train_size]\n",
    "fare_diff = (((fare_df.groupby(['Last name', 'FamilySize']).max() \n",
    " - fare_df.groupby(['Last name', 'FamilySize']).min())!=0).sum()/train_size * 100)\n",
    "print((\"Percentage of families with different fares is: %.1f\" %(fare_diff.values[0])) + '%')\n",
    "# The data shows only 1.7% has a different fare value between family memebers. It's some type of anomaly\n",
    "# Will use last name and fare to group passengers into families\n",
    "# First would like to show there is value in doing this\n",
    "train_temp_df = all_df.iloc[:train_size]\n",
    "family_df_grpby = train_temp_df[train_temp_df['FamilySize']>1][\n",
    "    ['Last name', 'Fare', 'FamilySize', 'Survived']].groupby(['Last name', 'Fare'])\n",
    "family_df = pd.DataFrame(data=family_df_grpby.size(), columns=['Size in train'])\n",
    "family_df['Survived total'] = family_df_grpby['Survived'].sum().astype(int)\n",
    "family_df['FamilySize'] = family_df_grpby['FamilySize'].mean().astype(int)\n",
    "#family_df = family_df[family_df['FamilySize']==8]\n",
    "print(\"Whole family survived: %.1f\" \n",
    "      %(100*len(family_df[family_df['Size in train']==family_df['Survived total'] ])/len(family_df))+'%') \n",
    "print(\"Whole family perished: %.1f\" \n",
    "      %(100*len(family_df[family_df['Survived total'] == 0])/len(family_df))+'%') \n",
    "## Majority family either all perished or all survived, this means we can use this as one feature to \n",
    "## predict survival\n",
    "\n",
    "# Now let's do the feature extraction\n",
    "# Intialize all 'Family survival', meaning there is no information on if any family members survived. \n",
    "# This number can be tuned I guess but I will use it to start with.\n",
    "grp_partial_age = 0\n",
    "grp_partial_cabin = 0\n",
    "grp_age_diff_df = pd.DataFrame()\n",
    "all_df['Family survival'] = 0.5\n",
    "for grp, grp_df in all_df[['Survived','Name', 'Last name', 'Fare', \n",
    "                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last name', 'Fare']):\n",
    "    if (len(grp_df) != 1):\n",
    "        grp_missing_age = len(grp_df[grp_df['Age'].isnull()])\n",
    "        is_partial_age = (grp_missing_age != 0) & (grp_missing_age != len(grp_df))\n",
    "        grp_partial_age += is_partial_age\n",
    "        \n",
    "        sibsp_df = grp_df.loc[grp_df['SibSp']!=0, ['Age']]\n",
    "        #print(sibsp_df.info())\n",
    "        sibsp_age_diff = sibsp_df.max() - sibsp_df.min()\n",
    "# FIRST-AUTHOR: make notebook run\n",
    "#         grp_age_diff_df = grp_age_diff_df.append(sibsp_age_diff, ignore_index=True)\n",
    "        grp_age_diff_df = pd.concat([grp_age_diff_df, sibsp_age_diff])\n",
    "\n",
    "        grp_missing_cabin = len(grp_df[grp_df['Cabin'].isnull()])\n",
    "        grp_partial_cabin += (grp_missing_cabin != 0) & (grp_missing_cabin != len(grp_df))\n",
    "\n",
    "\n",
    "        for PassID, row in grp_df.iterrows():\n",
    "            ## Find out if any family memebers survived or not\n",
    "            smax = grp_df.drop(PassID)['Survived'].max()\n",
    "            smin = grp_df.drop(PassID)['Survived'].min()\n",
    "\n",
    "            ## If any family memebers survived, put this feature as 1\n",
    "            if (smax==1.0): all_df.loc[PassID, 'Family survival'] = 1\n",
    "            ## Otherwise if any family memebers perished, put this feature as 0\n",
    "            elif (smin==0.0): all_df.loc[PassID, 'Family survival'] = 0\n",
    "\n",
    "print(\"Number of passenger with family survival information: \" \n",
    "      +str(all_df[all_df['Family survival']!=0.5].shape[0]))\n",
    "\n",
    "print('partial age group: ' + str(grp_partial_age))\n",
    "print('partial cabin group: ' + str(grp_partial_cabin))\n",
    "print(grp_age_diff_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "be6a8e96-9a4d-4e75-a15b-baefd4ae4a3d",
    "_uuid": "299eefe2b9b99fc04fae8c6ea8b896df5ed3bcff"
   },
   "source": [
    "# Extracting group information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cf0739d8-c45b-4c25-b0bb-ad001cb25837",
    "_uuid": "df6388bbb9bd9645fd2b8e0609ea094ce3fec8b8"
   },
   "source": [
    "In addtional to family, if you examin the data closely, you will see there are groups of people with same ticket number, and they pay the same fare. This suggests group of friends are travelling together. One will think these friends will help each other and will survive or perish at the same time. We will explore this informtion here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "bd75f4b8-975d-4e92-8adc-fc6b067de99f",
    "_uuid": "ebee067543095fcef1e5f252ace944171a5fa1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups in training set that is not family: 44\n",
      "Whole group perished: 29.5%\n",
      "Whole group survived: 36.4%\n",
      "partial age group: 0\n",
      "partial cabin group: 12\n",
      "Number of passenger with family/group survival information: 546\n",
      "        Age diff\n",
      "count        216\n",
      "unique        47\n",
      "top            1\n",
      "freq          11\n"
     ]
    }
   ],
   "source": [
    "# First find out how many such groups exists that are not families and what is the chance of \n",
    "# passengers within the same group survive or perish together\n",
    "train_temp_df = all_df.iloc[:train_size]\n",
    "ticket_grpby = train_temp_df.groupby('Ticket')\n",
    "ticket_df = pd.DataFrame(data=ticket_grpby.size(), columns=['Size in train'])\n",
    "ticket_df['Survived total'] = ticket_grpby['Survived'].sum().astype(int)\n",
    "ticket_df['Not family'] = ticket_grpby['Last name'].unique().apply(len)\n",
    "#ticket_df['Pclass'] = ticket_grpby['Pclass'].median()\n",
    "ticket_df = ticket_df[(ticket_df['Size in train'] > 1) & (ticket_df['Not family']>1)]\n",
    "print('Number of groups in training set that is not family: '+ str(len(ticket_df)))\n",
    "#print(\"Groups in Pclass 2/3: \" + str(len(ticket_df[ticket_df['Pclass']!=1])))\n",
    "print((\"Whole group perished: %.1f\" %(100/len(ticket_df)*len(ticket_df[ticket_df['Survived total']==0]))) + '%')\n",
    "print((\"Whole group survived: %.1f\" \n",
    "       %(100/len(ticket_df)*len(ticket_df[ticket_df['Survived total']==ticket_df['Size in train']]))) + '%')\n",
    "\n",
    "## Looking at the output, one can see ~76% of group members stay together. So let's extract this feature.\n",
    "## We will overload the 'Family survival' column instead of creating a seperate feature.\n",
    "grp_partial_age = 0\n",
    "grp_partial_cabin = 0\n",
    "grp_age_diff_df = pd.DataFrame(columns=['Age diff'])\n",
    "ticket_grpby = all_df.groupby('Ticket')\n",
    "for _, grp_df in ticket_grpby:\n",
    "    if (len(grp_df) > 1):\n",
    "        grp_missing_age = len(grp_df[grp_df['Age'].isnull()])\n",
    "        grp_partial_age += (grp_missing_age != 0) & (grp_missing_age != len(grp_df))\n",
    "\n",
    "# FIRST-AUTHOR: make notebook run\n",
    "#         grp_age_diff_df = grp_age_diff_df.append(pd.DataFrame(data=[grp_df['Age'].max() \n",
    "#                                                                     - grp_df['Age'].min()]\n",
    "#                                                               , columns=['Age diff']))\n",
    "        grp_age_diff_df = pd.concat([grp_age_diff_df, pd.DataFrame(data=[grp_df['Age'].max() \n",
    "                                                                    - grp_df['Age'].min()]\n",
    "                                                              , columns=['Age diff'])])\n",
    "\n",
    "\n",
    "        grp_missing_cabin = len(grp_df[grp_df['Cabin'].isnull()])\n",
    "        grp_partial_cabin += (grp_missing_cabin != 0) & (grp_missing_cabin != len(grp_df))\n",
    "        for PassID, row in grp_df.iterrows():\n",
    "            if (row['Family survival']==0)|(row['Family survival']==0.5):\n",
    "                smax = grp_df.drop(PassID)['Survived'].max()\n",
    "                smin = grp_df.drop(PassID)['Survived'].min()\n",
    "                if (smax==1.0): all_df.loc[PassID, 'Family survival'] = 1\n",
    "                elif (smin==0.0): all_df.loc[PassID, 'Family survival'] = 0\n",
    "print('partial age group: ' + str(grp_partial_age))\n",
    "print('partial cabin group: ' + str(grp_partial_cabin))\n",
    "print(\"Number of passenger with family/group survival information: \" \n",
    "      +str(all_df[all_df['Family survival']!=0.5].shape[0]))\n",
    "train['Family survival'] = (all_df.iloc[:train_size]['Family survival'].values).astype(float)\n",
    "test['Family survival'] = (all_df.iloc[train_size:]['Family survival'].values).astype(float)\n",
    "print(grp_age_diff_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ac64652c-b566-44de-a398-3c83f702bb18",
    "_uuid": "80148e37ea64f36eacbff10de7c4dbecc4fd1986"
   },
   "source": [
    "Good, we can see 546 passengers have a family/group survival information. That's a sizable chunk out of the total numbers of passengers. Hopefully it will improve our prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "68fa2057-e27a-e252-0d1b-869c00a303ba",
    "_uuid": "e1b330a1855baead6af2c14ebc40cd54b146f3e1"
   },
   "source": [
    "# Data Cleaning #\n",
    "great! now let's clean our data and map our features into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "2502bb70-ce6f-2497-7331-7d1f80521470",
    "_uuid": "7dab8cc063a209f843a947d5476a9c257e530897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass  Sex  Age  Fare  Embarked  IsAlone  Title  Family survival\n",
      "0         0       3    1    1     0         0        0    1.0              0.5\n",
      "1         1       1    0    2     3         1        0    3.0              0.5\n",
      "2         1       3    0    1     1         0        1    2.0              0.5\n",
      "3         1       1    0    2     3         0        0    3.0              0.0\n",
      "4         0       3    1    2     1         0        1    1.0              0.5\n",
      "5         0       3    1    1     1         2        1    1.0              0.5\n",
      "6         0       1    1    3     3         0        1    1.0              0.5\n",
      "7         0       3    1    0     2         0        0    4.0              0.0\n",
      "8         1       3    0    1     1         0        0    3.0              1.0\n",
      "9         1       2    0    0     2         1        0    3.0              0.0\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "    \n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4\n",
    "\n",
    "# Feature Selection\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp',\\\n",
    "                 'Parch', 'FamilySize']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "\n",
    "test  = test.drop(drop_elements, axis = 1)\n",
    "\n",
    "print (train.head(10))\n",
    "\n",
    "train = train.values\n",
    "test  = test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8aaaf2bc-e282-79cc-008a-e2e801b51b07",
    "_uuid": "69e8f0c6230d270355bdd1d504e35103b64d1f26"
   },
   "source": [
    "good! now we have a clean dataset and ready to predict. let's find which classifier works better on this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "23b55b45-572b-7276-32e7-8f7a0dcfd25e",
    "_uuid": "49c5e7101a6eac085d73f7b8b525ec72025a5e54"
   },
   "source": [
    "# Classifier Comparison #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "31ded30a-8de4-6507-e7f7-5805a0f1eaf1",
    "_uuid": "a1be9e966c3e841e7c28784593bdb1c0478e1bdb"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove plotting, ML code\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# from sklearn.metrics import accuracy_score, log_loss\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# FIRST-AUTHOR: remove ML code\n",
    "# classifiers = [\n",
    "#     KNeighborsClassifier(3),\n",
    "#     SVC(probability=True),\n",
    "#     DecisionTreeClassifier(),\n",
    "#     RandomForestClassifier(),\n",
    "# \tAdaBoostClassifier(),\n",
    "#     GradientBoostingClassifier(),\n",
    "#     GaussianNB(),\n",
    "#     LinearDiscriminantAnalysis(),\n",
    "#     QuadraticDiscriminantAnalysis(),\n",
    "#     LogisticRegression()]\n",
    "\n",
    "log_cols = [\"Classifier\", \"Accuracy\"]\n",
    "log \t = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "# FIRST-AUTHOR: remove ML code\n",
    "# sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "\n",
    "X = train[0::, 1::]\n",
    "y = train[0::, 0]\n",
    "\n",
    "# FIRST-AUTHOR: remove ML code, plotting\n",
    "# acc_dict = {}\n",
    "\n",
    "# for train_index, test_index in sss.split(X, y):\n",
    "# \tX_train, X_test = X[train_index], X[test_index]\n",
    "# \ty_train, y_test = y[train_index], y[test_index]\n",
    "\t\n",
    "# \tfor clf in classifiers:\n",
    "# \t\tname = clf.__class__.__name__\n",
    "# \t\tclf.fit(X_train, y_train)\n",
    "# \t\ttrain_predictions = clf.predict(X_test)\n",
    "# \t\tacc = accuracy_score(y_test, train_predictions)\n",
    "# \t\tif name in acc_dict:\n",
    "# \t\t\tacc_dict[name] += acc\n",
    "# \t\telse:\n",
    "# \t\t\tacc_dict[name] = acc\n",
    "\n",
    "# for clf in acc_dict:\n",
    "# \tacc_dict[clf] = acc_dict[clf] / 10.0\n",
    "# \tlog_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n",
    "# # FIRST-AUTHOR: make notebook run\n",
    "# # \tlog = log.append(log_entry)\n",
    "# \tlog = pd.concat([log, log_entry])\n",
    "\n",
    "# plt.xlabel('Accuracy')\n",
    "# plt.title('Classifier Accuracy')\n",
    "\n",
    "# sns.set_color_codes(\"muted\")\n",
    "# sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "438585cf-b7ad-73ba-49aa-87688ff21233",
    "_uuid": "6cd52741b92c26366a144d811f64d70c76a5cb0b"
   },
   "source": [
    "# Prediction #\n",
    "After adding this new feature, it looks like GradientBoostingClassifier or LogisticRegression are better. Nonetheless, we will keep using SVC to see the impact of this new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "24967b57-732b-7180-bfd5-005beff75974",
    "_uuid": "4da9974a1b3b82cbcf29830d90c2b07f1fe4e536"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove ML code\n",
    "# candidate_classifier = SVC()\n",
    "# candidate_classifier.fit(train[0::, 1::], train[0::, 0])\n",
    "# result = candidate_classifier.predict(test)\n",
    "_ = train[0::, 1::]\n",
    "_ = train[0::, 0]\n",
    "result = train[:,0]\n",
    "# FIRST-AUTHOR: make notebook run with input scaling\n",
    "# result_df = pd.DataFrame(columns=['PassengerId', 'Survived'], \n",
    "#                          data=np.array([range(892, 1310), result]).T.astype(int))\n",
    "result_df = pd.DataFrame(columns=['PassengerId', 'Survived'], \n",
    "                         data=np.array([range(892, result.shape[0] + 892), result]).T.astype(int))\n",
    "result_df.to_csv(\"prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "81c3fcd3-5a4c-4668-a299-72748448c3e6",
    "_uuid": "dae8259c046a42286a06217aa1653043e68443b3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}