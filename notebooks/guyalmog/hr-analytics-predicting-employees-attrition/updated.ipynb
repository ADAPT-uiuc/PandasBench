{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HR Analytics - Classifying Employees Attrition\n",
    "## goals:\n",
    "The main goal of this project is to get some useful information and insights about what contributes to employees feeling burnout, fatigue and attrition.\n",
    "Employees attrition has a great amount of consequences to the company, and having the ability to know beforehand which employee is more proned to leave, can help mitigating some of those negative effects.\n",
    "The area of Human Resource Analysis has a big weight in the field of labour market, thus expanding the knowledge about it is important.\n",
    "\n",
    "\n",
    "## data sources:\n",
    "the data we're about to analyze is taken from Kaggle: https://www.kaggle.com/vjchoudhary7/hr-analytics-case-study and contains a number of indicators about each Employee(N=4410), as well as his attrition status('Yes' or No).\n",
    "The data contained 30 features after cleaning was done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# import pandas as pd\n",
    "import os\n",
    "exec(os.environ['IREWR_IMPORTS'])\n",
    "import numpy as np\n",
    "# ALEX: remove plotting, ML code\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split, RandomizedSearchCV\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.metrics import r2_score, accuracy_score, roc_auc_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working stages:\n",
    "\n",
    "#### 1) load the data\n",
    "\n",
    "#### 2) clean the data\n",
    "\n",
    "#### 3) explore the data\n",
    "\n",
    "#### 4) handle null values\n",
    "\n",
    "#### 5) feature engeneering\n",
    "\n",
    "#### 6) model comparison\n",
    "\n",
    "#### 7) model selection\n",
    "\n",
    "#### 8) tuning the model\n",
    "\n",
    "#### note: part 2-3-4 may often come in a different order, depends on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "emp_data = pd.read_csv(\"./input/employee_survey_data.scaled.csv\", index_col='EmployeeID')\n",
    "gen_data = pd.read_csv(\"./input/general_data.scaled.csv\",index_col='EmployeeID')\n",
    "manager_data = pd.read_csv(\"./input/manager_survey_data.scaled.csv\",index_col='EmployeeID')\n",
    "in_time_data = pd.read_csv(\"./input/in_time.scaled.csv\")\n",
    "out_time_data = pd.read_csv(\"./input/out_time.scaled.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_time_data.rename(columns={'Unnamed: 0':'EmployeeID'}, inplace=True)\n",
    "in_time_data.set_index('EmployeeID', inplace=True)\n",
    "in_time_data\n",
    "out_time_data.rename(columns={'Unnamed: 0':'EmployeeID'}, inplace=True)\n",
    "out_time_data.set_index('EmployeeID', inplace=True)\n",
    "out_time_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_time_data = in_time_data.apply(pd.DatetimeIndex)\n",
    "out_time_data = out_time_data.apply(pd.DatetimeIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = pd.concat([in_time_data, out_time_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = times.applymap(lambda x: x.hour+0.01*x.minute)\n",
    "times['avg_in'] = round(times.iloc[:, :261].mean(axis=1),2)\n",
    "times['avg_out'] = round(times.iloc[:, 261:].mean(axis=1),2)\n",
    "times['med_in'] = round(times.iloc[:, :261].median(axis=1),2)\n",
    "times['med_out'] = round(times.iloc[:, 261:].median(axis=1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# fig, axs = plt.subplots(1,3, figsize = (12,4))\n",
    "# sns.distplot(times.iloc[4, :261], ax=axs[0]).set(xlabel = 'In time', ylabel = 'Frequency',xlim=(7,12))\n",
    "# sns.distplot(times.iloc[72, :261], ax=axs[1]).set(xlabel = 'In time', ylabel = 'Frequency',xlim=(7,12))\n",
    "# sns.distplot(times.iloc[102, :261], ax=axs[2]).set(xlabel = 'In time', ylabel = 'Frequency',xlim=(7,12))\n",
    "# plt.tight_layout()\n",
    "_ = times.iloc[4, :261]\n",
    "_ = times.iloc[72, :261]\n",
    "_ = times.iloc[102, :261]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# plt.figure(figsize=(10,4))\n",
    "# g = plt.plot(times.iloc[4, :261])\n",
    "_ = times.iloc[4, :261]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# plt.figure(figsize=(10,4))\n",
    "# g = plt.plot(times.iloc[25, :261])\n",
    "_ = times.iloc[25, :261]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times['total'] = times['med_out'] - times['med_in']\n",
    "time_feats = times[['avg_in', 'avg_out', 'med_in','med_out','total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_time_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data.loc[emp_data['EnvironmentSatisfaction'].isnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there's some null values, let's explore the data a little bit to see how can can handle it properly</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# fig, axs = plt.subplots(1,3, figsize=(10,4))\n",
    "# sns.barplot(emp_data['EnvironmentSatisfaction'], emp_data['JobSatisfaction'],ax=axs[0])\n",
    "# sns.barplot(emp_data['WorkLifeBalance'], emp_data['JobSatisfaction'],ax=axs[1])\n",
    "# sns.barplot(emp_data['WorkLifeBalance'], emp_data['EnvironmentSatisfaction'],ax=axs[2])\n",
    "# plt.tight_layout(pad=3)\n",
    "_ = emp_data['EnvironmentSatisfaction']\n",
    "_ = emp_data['JobSatisfaction']\n",
    "_ = emp_data['WorkLifeBalance']\n",
    "_ = emp_data['JobSatisfaction']\n",
    "_ = emp_data['WorkLifeBalance']\n",
    "_ = emp_data['EnvironmentSatisfaction']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(emp_data, col='WorkLifeBalance',size=2.4, aspect=2, col_wrap=2 )\n",
    "# g = g.map(sns.distplot, 'JobSatisfaction', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(emp_data, col='JobSatisfaction',row ='WorkLifeBalance', size=2.4, aspect=2 )\n",
    "# g = g.map(sns.distplot, 'EnvironmentSatisfaction' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No clear connection between the variables, but in order to be on the safe side, we'll handle the nulls by the conditional mode based on the other two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mode(data, col, col2, col3):\n",
    "    index_nan = list(data[col][data[col].isnull()].index)\n",
    "    for i in index_nan:\n",
    "        cols_mode = data[col].mode()[0]\n",
    "        mode_fill = data[col][((data[col2] == data.loc[i][col2]) & (data[col3] == data.loc[i][col3]))].mode()[0]\n",
    "        data[col].loc[i] = mode_fill\n",
    "\n",
    "\n",
    "    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_mode(emp_data, 'EnvironmentSatisfaction','JobSatisfaction','WorkLifeBalance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_mode(emp_data, 'JobSatisfaction','EnvironmentSatisfaction','WorkLifeBalance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_mode(emp_data, 'WorkLifeBalance','JobSatisfaction','EnvironmentSatisfaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright, we can now merge the dataframes and explore the data as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([gen_data,manager_data,emp_data,time_feats], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns.values:\n",
    "    if df[col].nunique() == 1:\n",
    "        df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Attrition'] = np.where(df['Attrition']=='Yes',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.NumCompaniesWorked.fillna(0, inplace=True)\n",
    "df.TotalWorkingYears.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.distplot(df['total']).set(xlabel = 'Total Hours Of Work', ylabel = 'Frequency')\n",
    "_ = df['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the exact number of people by hours of work\n",
    "print(df['total'][df['total'] <=7].value_counts().sum())\n",
    "print(df['total'][(df['total'] > 7) & (df['total'] <=8)].value_counts().sum())\n",
    "print(df['total'][df['total'] > 8].value_counts().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.subplot(1,2,1)\n",
    "# sns.distplot(df['avg_in'], bins=20)\n",
    "# plt.subplot(1,2,2)\n",
    "# s = sns.distplot(df['avg_out'], bins=20)\n",
    "# plt.xticks((range(16,22)))\n",
    "# plt.tight_layout(pad=5)\n",
    "_ = df['avg_in']\n",
    "_ = df['avg_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.subplot(1,2,1)\n",
    "# sns.distplot(df['med_in'], bins=10)\n",
    "# plt.subplot(1,2,2)\n",
    "# s = sns.distplot(df['med_out'], bins=20)\n",
    "# plt.xticks((range(16,22)))\n",
    "# plt.tight_layout(pad=5)\n",
    "_ = df['med_in']\n",
    "_ = df['med_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot, 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot, 'med_in')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'med_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'avg_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition' )\n",
    "# g = g.map(sns.distplot , 'avg_out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### both the median and the mean in/out time, as well as the total time, show us that people who work more, but pushing the working hours till late, are more likely to attrit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# plt.figure(figsize=(12,4))\n",
    "# plt.subplot(1,3,1)\n",
    "# g = sns.barplot(df['Attrition'], df['EnvironmentSatisfaction'], hue=df['JobSatisfaction'])\n",
    "# plt.legend(loc ='upper right')\n",
    "# plt.subplot(1,3,2)\n",
    "# s = sns.barplot(df['Attrition'], df['JobInvolvement'], hue=df['JobSatisfaction'])\n",
    "# plt.legend(loc ='upper right')\n",
    "# plt.subplot(1,3,3)\n",
    "# f = sns.barplot(df['Attrition'], df['WorkLifeBalance'], hue=df['JobSatisfaction'])\n",
    "# plt.legend(loc ='upper right')\n",
    "# plt.tight_layout()\n",
    "_ = df['Attrition']\n",
    "_ = df['EnvironmentSatisfaction']\n",
    "_ = df['JobSatisfaction']\n",
    "_ = df['Attrition']\n",
    "_ = df['JobInvolvement']\n",
    "_ = df['JobSatisfaction']\n",
    "_ = df['Attrition']\n",
    "_ = df['WorkLifeBalance']\n",
    "_ = df['JobSatisfaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['BusinessTravel', 'Department','EducationField', 'Gender',\n",
    "       'JobRole', 'MaritalStatus','JobInvolvement', 'PerformanceRating',\n",
    "       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'JobLevel']\n",
    "# ALEX: remove plotting\n",
    "# plt.figure(figsize=(15,18))\n",
    "for i in range(len(cat_cols)):\n",
    "# ALEX: remove plotting\n",
    "#     plt.subplot(4,3,i+1)\n",
    "#     sns.countplot(df[cat_cols[i]], hue=df['Attrition'])\n",
    "    _ = df[cat_cols[i]]\n",
    "    _ = df['Attrition']\n",
    "    if len(df[cat_cols[i]].unique()) >= 3:\n",
    "        pass\n",
    "# ALEX: remove plotting\n",
    "#         plt.xticks(rotation=75)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Att_ratio(data, col):\n",
    "    col_values = data[col].unique()\n",
    "    print('For',col, ':')\n",
    "    for index, item in enumerate(col_values):\n",
    "# ALEX: make notebook run with input scaling\n",
    "        if type(item) == float or (type(item) == np.float64 and np.isnan(item)):\n",
    "            continue\n",
    "        ratio = len(df.loc[(df[col] == col_values[index]) & (df['Attrition'] == 1)])/len(df.loc[(df[col] == col_values[index]) & (df['Attrition'] == 0)])\n",
    "        print('The Attrition ratio(Yes/No) under the category %s is %f' %(item, ratio))\n",
    "    print('-----------------------------------------------------------------------------------------------')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['BusinessTravel', 'Department','EducationField', 'Gender',\n",
    "       'JobRole', 'MaritalStatus','JobInvolvement', 'PerformanceRating',\n",
    "       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'JobLevel']\n",
    "for col in cat_cols:\n",
    "    Att_ratio(df, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'].replace({'Female': 1, 'Male': 0}, inplace=True)\n",
    "df['BusinessTravel'].replace({'Travel_Rarely': 1,'Travel_Frequently':2,'Non-Travel':0 }, inplace=True)\n",
    "cat_cols = ['Gender','BusinessTravel',\n",
    "       'JobInvolvement', 'PerformanceRating',\n",
    "       'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'JobLevel']\n",
    "for col in cat_cols:\n",
    "    print('The Attrition ration(Yes/All) For',col+':')\n",
    "    print(df.groupby([col]).Attrition.agg(['mean']))\n",
    "    print('----------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary of the categorical variables:\n",
    "* BusinessTravel seems to have a bit of an influence on attrition.those who travel frequently have an attrition ratio of 0.331731 - Probably those who travel a lot for business purposes are having more stress and thus willing to leave\n",
    "\n",
    "\n",
    "* Department doesn't seem to be correlated strongly but Human Resources attrition ratio is quite high at 0.431818 - maybe that's because of the low salary, we'll check this one later \n",
    "\n",
    "\n",
    "* EducationField seems to be a related to Department in some way, for Human Resources the ratio is 0.687500\n",
    "\n",
    "\n",
    "* Gender doesn't seem correlated - Male attrition ratio is 0.200000 - not significantly more than females\n",
    "\n",
    "\n",
    "* JobRole - the attrition ratio for Research Director is 0.311475 - that's not surprising considering the fact that the largest department is Research and Development. Research director is a role with a lot of responsibility,and that definetely can contribute to the the overall fatigue and burnout.\n",
    "\n",
    "\n",
    "* MaritalStatus - Single status has a slightly bigger ratio with 0.342857 - maybe mediated by low salary/short term job\n",
    "\n",
    "\n",
    "* JobInvolvement - category 1 ratio is 0.276923 - not surprising but isn't much higher than the other categories\n",
    "\n",
    "\n",
    "* PerformanceRating - category 4 ratio 0.221622 - probably those who work the hardest, but again no significantly higher\n",
    "\n",
    "\n",
    "* EnvironmentSatisfaction - here the difference is quite big, the category 1.0 attrition ratio is 0.339117 - it's pretty normal in my opinion. those who aren't satisfied with their work are not going to stay for long\n",
    "\n",
    "\n",
    "* JobSatisfaction - category 1.0 ratio is 0.297134 - same explanation but here it's not significantly higher in my opinion\n",
    "\n",
    "\n",
    "* WorkLifeBalance - here, simillarly to enviroment satisfaction, the gap is quite big, for category 1.0 the ratio is 0.457317 - the reasonable explanation is that people with low work/life balance are strongly prone to burnout, have more stress, etc..\n",
    "\n",
    "\n",
    "* JobLevel - category 2 ratio 0.216401 - very simillar to other categories therefore doesn;t have a strong impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's move on to the numerical variables:\n",
    "### We'll start with income, then the Age and the seniority\n",
    "### We'll check if there's any correlation between those variables and the prone to leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# sns.distplot(df.MonthlyIncome)\n",
    "_ = df.MonthlyIncome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### got some outliers in terms of income. \n",
    "### lets check the income correlations with other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'MaritalStatus', row = 'Department')\n",
    "# g = g.map(sns.distplot , 'MonthlyIncome')\n",
    "# g.fig.subplots_adjust(top=1,right=1.4, wspace=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * seems like, as i hypothesised earlier, that single workers are paid the least.\n",
    "\n",
    "#### * we can also notice that divorced people are paid the most, regardless of department.\n",
    "\n",
    "#### * doesn't seem like there is a clear connection between department and salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can see it here as well, it's more useful to look at the median, as the distribution of income is skewed to the right\n",
    "print(df.groupby(['MaritalStatus']).MonthlyIncome.agg(['mean','median']))\n",
    "print(df.groupby(['Department']).MonthlyIncome.agg(['mean','median']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEF-UPDATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# plt.figure(figsize=(14,8))\n",
    "# top_corr = df.corr().nlargest(15, 'MonthlyIncome').index\n",
    "top_corr = (\n",
    "    df.corr(numeric_only=True)\n",
    "      .nlargest(15, 'MonthlyIncome')\n",
    "      .index\n",
    ")\n",
    "cm = np.corrcoef(df[top_corr].values.T)\n",
    "# ALEX: remove plotting\n",
    "# g = sns.heatmap(cm, cbar=True, annot=True, cmap='BrBG',yticklabels = top_corr.values, xticklabels=top_corr.values)\n",
    "_ = top_corr.values\n",
    "_ = top_corr.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No real connection between salary and any other numerical value\n",
    "### let's have a look at the connection between attrition and salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.distplot(df['MonthlyIncome'][df['Attrition'] == 0],color='blue')\n",
    "# f = sns.distplot(df['MonthlyIncome'][df['Attrition'] == 1],color='orange')\n",
    "_ = df['MonthlyIncome'][df['Attrition'] == 0]\n",
    "_ = df['MonthlyIncome'][df['Attrition'] == 1]\n",
    "df.groupby('Attrition').MonthlyIncome.agg(['mean','median'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from looking at the graph and the table there's seem to be no correlation whatsoever between salary and attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's take a look at the age, the years from last promotion and years at the company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEF-UPDATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# plt.figure(figsize=(14,8))\n",
    "# top_corr = df.corr().nlargest(15, 'Age').index\n",
    "top_corr = (\n",
    "    df.corr(numeric_only=True)\n",
    "      .nlargest(15, 'Age')\n",
    "      .index\n",
    ")\n",
    "cm = np.corrcoef(df[top_corr].values.T)\n",
    "# ALEX: remove plotting\n",
    "# g = sns.heatmap(cm, cbar=True, annot=True, cmap='BrBG',yticklabels = top_corr.values, xticklabels=top_corr.values)\n",
    "_ = top_corr.values\n",
    "_ = top_corr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# fig, axs = plt.subplots(1,3, figsize = (12,4))\n",
    "# sns.distplot(df['Age'], ax=axs[0])\n",
    "# sns.distplot(df['YearsSinceLastPromotion'], ax=axs[1])\n",
    "# sns.distplot(df['YearsAtCompany'], ax=axs[2])\n",
    "# plt.tight_layout()\n",
    "_ = df['Age']\n",
    "_ = df['YearsSinceLastPromotion']\n",
    "_ = df['YearsAtCompany']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quite normal distributed ages.\n",
    "\n",
    "### skewd to the right distribution of total years at the company\n",
    "\n",
    "### skewd to the right distribution of years since last promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# sns.distplot(df['Age'][df['Attrition'] == 0],color='blue')\n",
    "# sns.distplot(df['Age'][df['Attrition'] == 1],color='orange')\n",
    "# plt.legend(['No','Yes'])\n",
    "_ = df['Age'][df['Attrition'] == 0]\n",
    "_ = df['Age'][df['Attrition'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'Age')\n",
    "# g.fig.subplots_adjust(top=1,right=1.2)\n",
    "#plt.tight_layout()\n",
    "df.groupby('Attrition').Age.agg(['median','mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like a tendency to attrit among younger people,that's interesting. mayber thats because they work longer hours. let's check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# sns.scatterplot(df['Age'], df['total'])\n",
    "_ = df['Age']\n",
    "_ = df['total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doesn't seem like that's the case. probably related to other things like more stress, need to prove yourself etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'YearsSinceLastPromotion')\n",
    "# g.fig.subplots_adjust(top=1,right=1.2)\n",
    "df.groupby('Attrition').YearsSinceLastPromotion.agg(['median','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'YearsWithCurrManager')\n",
    "# g.fig.subplots_adjust(top=1,right=1.2)\n",
    "df.groupby('Attrition').YearsWithCurrManager.agg(['median','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'YearsAtCompany')\n",
    "# g.fig.subplots_adjust(top=1,right=1.2)\n",
    "df.groupby('Attrition').YearsAtCompany.agg(['median','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'DistanceFromHome')\n",
    "# g.fig.subplots_adjust(top=1,right=1.2)\n",
    "df.groupby('Attrition').DistanceFromHome.agg(['median','mean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'TotalWorkingYears')\n",
    "# g.fig.subplots_adjust(top=1,right=1.2)\n",
    "df.groupby('Attrition').TotalWorkingYears.agg(['median','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'Education')\n",
    "# g.fig.subplots_adjust(top=1,right=1.2)\n",
    "df.groupby('Attrition').Education.agg(['median','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'PercentSalaryHike')\n",
    "# g.fig.subplots_adjust(top=1,right=1.2)\n",
    "df.groupby('Attrition').PercentSalaryHike.agg(['median','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'NumCompaniesWorked')\n",
    "# g.fig.subplots_adjust(top=1,right=1.2)\n",
    "df.groupby('Attrition').NumCompaniesWorked.agg(['median','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'YearsSinceLastPromotion')\n",
    "# g.fig.subplots_adjust(top=1,right=1.2)\n",
    "df.groupby('Attrition').YearsSinceLastPromotion.agg(['median','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove plotting\n",
    "# g = sns.FacetGrid(df,col = 'Attrition')\n",
    "# g = g.map(sns.distplot , 'StockOptionLevel')\n",
    "# g.fig.subplots_adjust(top=1,right=1.2)\n",
    "df.groupby('Attrition').StockOptionLevel.agg(['median','mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the numerical variables:\n",
    "\n",
    "* Looks like seniority plays somewhat of a role in the attrition attribute, along with total working years and years at the company, though there is a strong correlation beteen age and total working years, as well as total working years and years at the company, therefore there's a risk for multicolinearity. we'll handle it when we get to the model building.\n",
    "\n",
    "\n",
    "* No observed impact among all the other variables on attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling:\n",
    "### 0) A little more feature engineering like creating age groups\n",
    "### 1) seperating x and y\n",
    "### 2) convert strings to dummy variables\n",
    "### 3) scale x features\n",
    "### 4) compare scores of different models and select the two best\n",
    "### 5) perform search grid on the two best models and see if there's an improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first drop some unimportant features and variables with multicolinearity such as med_in avg_in and total etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['Attrition'], axis=1).reset_index(drop=True)\n",
    "y = df['Attrition'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_todrop = ['JobLevel','Department','JobRole','NumCompaniesWorked','PercentSalaryHike','StockOptionLevel','YearsWithCurrManager','med_in', 'avg_in','avg_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.drop(cols_todrop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating age groups\n",
    "x.Age = pd.cut(x.Age, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.Age.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting categorial variables to dummies\n",
    "x = pd.get_dummies(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy = x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaling the features\n",
    "\n",
    "# ALEX: remove ML code\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "# x = scaler.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting the sets into train and test\n",
    "\n",
    "# ALEX: remove ML code\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alright we are ready to start with the predictions!\n",
    "\n",
    "### As I've mentionted above, our working process is :\n",
    "* Compare\n",
    "\n",
    "\n",
    "* Select\n",
    "\n",
    "\n",
    "* Improve\n",
    "\n",
    "\n",
    "* Check the contribution of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# Defining a function which examines each model based on the score, then show each one's score and STD, as well as graphic comparison\n",
    "# evaluate each model in turn\n",
    "# def get_scores(score1, score2):\n",
    "#     models = []\n",
    "#     models.append(('LR', LogisticRegression()))\n",
    "#     models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "#     models.append(('KNN', KNeighborsClassifier()))\n",
    "#     models.append(('CART', DecisionTreeClassifier()))\n",
    "#     models.append(('NB', GaussianNB()))\n",
    "#     models.append(('SVM', SVC()))\n",
    "#     models.append(('ADA', AdaBoostClassifier()))\n",
    "#     models.append(('GradientBooster', GradientBoostingClassifier()))\n",
    "#     models.append(('ExtraTrees', ExtraTreesClassifier()))\n",
    "#     models.append(('RandomForest', RandomForestClassifier()))\n",
    "#     cv_scores = []\n",
    "#     test_scores = []\n",
    "#     names = []\n",
    "#     stds = []\n",
    "#     differences = []\n",
    "#     #res = pd.DataFrame(columns = {'Model',score+('(train)'), 'Std', score+('(test_score)'), 'difference'})\n",
    "#     #res = res[['Model',score+('(train)'), 'Std', score+('(test_score)'), 'difference']]\n",
    "#     res = pd.DataFrame()\n",
    "#     for index, model in enumerate(models):\n",
    "#         kfold = StratifiedKFold(n_splits=7)\n",
    "#         cv_results = cross_val_score(model[1], x_train, y_train, cv=kfold, scoring=score1)\n",
    "#         cv_scores.append(cv_results)\n",
    "#         names.append(model[0])\n",
    "#         model[1].fit(x_train,y_train)\n",
    "#         predictions = model[1].predict(x_test)\n",
    "#         test_score = score2(predictions, y_test)\n",
    "#         test_scores.append(test_score)\n",
    "#         stds.append(cv_results.std())\n",
    "#         differences.append((cv_results.mean() - test_score))\n",
    "#         res.loc[index,'Model'] = model[0]\n",
    "#         res.loc[index,score1+('(train)')] = cv_results.mean()\n",
    "#         res.loc[index,score1+('(test_score)')] = test_score\n",
    "#         res.loc[index,'Std'] = cv_results.std()\n",
    "#         res.loc[index,'difference'] = cv_results.mean() - test_score\n",
    "#     # boxplot algorithm comparison\n",
    "#     fig = plt.figure(figsize = (12,5))\n",
    "#     fig.suptitle('Model Comparison')\n",
    "#     ax = fig.add_subplot(121)\n",
    "#     plt.boxplot(cv_scores)\n",
    "#     ax.set_xticklabels(names, rotation=70)\n",
    "#     axs = fig.add_subplot(122)\n",
    "#     sns.barplot(names,test_scores)\n",
    "#     axs.set_xticklabels(names, rotation=70)\n",
    "#     plt.tight_layout(pad=5)\n",
    "#     return res\n",
    "#     plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# get_scores('accuracy', accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection And Tuning\n",
    "### seems like our models has a strong predicting power, especially the random forest and extra tree booster. let's check if theres any way  to improve them with random search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# params = {'bootstrap': [True, False],\n",
    "#  'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "#  'max_features': ['auto', 'sqrt'],\n",
    "#  'min_samples_leaf': [1, 2, 4],\n",
    "#  'min_samples_split': [2, 5, 10],\n",
    "#  'n_estimators': [200, 400, 600, 800, 1000]}\n",
    "# RandomForest = RandomForestClassifier()\n",
    "# randomgrid_forest = RandomizedSearchCV(estimator=RandomForest, param_distributions = params, \n",
    "#                                cv=5, n_iter=25, scoring = 'accuracy',\n",
    "#                                n_jobs = 4, verbose = 3, random_state = 42,\n",
    "#                                return_train_score = True)\n",
    "# randomgrid_forest.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# randomgrid_forest.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# forest_preds = randomgrid_forest.predict(x_test)\n",
    "# roc_auc_score(forest_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# randomgrid_forest.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little bit of improvement indeed! let's try tuning it a little bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I created a function which take a model and scoring method, then shows the cross validation score for each estimator\n",
    "### and plot it next to the test score.\n",
    "# ALEX: remove ML code\n",
    "# def estimators_compare(model, cv_score, metrics_score):\n",
    "#     train_scores = []\n",
    "#     test_scores= []\n",
    "#     estimators = [80,100,200,400,600,800,1200]\n",
    "#     res = pd.DataFrame(columns = {'Number Of Estimators', 'train_score', 'test_score'})\n",
    "#     for ind, i in enumerate(estimators):\n",
    "#         mode = model(n_estimators=i)\n",
    "#         kfold = StratifiedKFold(n_splits=7)\n",
    "#         cv_results = cross_val_score(mode, x_train, y_train, cv=kfold, scoring=cv_score)\n",
    "#         mode.fit(x_train, y_train)\n",
    "#         predictions = mode.predict(x_test)\n",
    "#         train_score = cv_results.mean()\n",
    "#         train_scores.append(train_score)\n",
    "#         test_score = metrics_score(predictions, y_test)\n",
    "#         test_scores.append(test_score)\n",
    "#         res.loc[ind,'Number Of Estimators'] = i\n",
    "#         res.loc[ind,'train_score'] = train_score\n",
    "#         res.loc[ind,'test_score'] = test_score\n",
    "\n",
    "#     plt.plot(estimators, train_scores, color='red')\n",
    "#     plt.plot(estimators, test_scores, color='blue')\n",
    "#     legs = ['train', 'test']\n",
    "#     plt.legend(legs)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# estimators_compare(RandomForestClassifier, 'accuracy', accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's compare 100 estimators with 600 like the grid search provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# final_random_forest = RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
    "#                        criterion='gini', max_depth=40, max_features='sqrt',\n",
    "#                        max_leaf_nodes=None, max_samples=None,\n",
    "#                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                        min_samples_leaf=1, min_samples_split=2,\n",
    "#                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "#                        n_jobs=None, oob_score=False, random_state=None,\n",
    "#                        verbose=0, warm_start=False)\n",
    "# final_random_forest.fit(x_train, y_train)\n",
    "# final_random_forest.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well, Looks like our randomized grid search produced simmilar results. before we move on to the extra trees, let's have a look at the contribution of each feature to our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# featuers_coefficients = final_random_forest.feature_importances_.tolist()\n",
    "feature_names = x_copy.columns\n",
    "# ALEX: remove ML code\n",
    "# feats = pd.DataFrame(pd.Series(featuers_coefficients, feature_names).sort_values(ascending=False),columns=['Coefficient'])\n",
    "# feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same process for the Extra Trees model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# params2 = {'bootstrap': [True, False],\n",
    "#  'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "#  'max_features': ['auto', 'sqrt'],\n",
    "#  'min_samples_leaf': [1, 2, 4],\n",
    "#  'min_samples_split': [2, 5, 10],\n",
    "#  'n_estimators': [200, 400, 600, 800, 1000]}\n",
    "# ExtraTress = ExtraTreesClassifier()\n",
    "# randomgrid_extrees = RandomizedSearchCV(estimator=ExtraTress, param_distributions = params2, \n",
    "#                                cv=5, n_iter=25, scoring = 'accuracy',\n",
    "#                                n_jobs = 4, verbose = 3, random_state = 42,\n",
    "#                                return_train_score = True)\n",
    "# randomgrid_extrees.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# randomgrid_extrees.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# randomgrid_extrees.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# estimators_compare(ExtraTreesClassifier, 'accuracy', accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go with 100 estimators!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# final_extra_trees_A = ExtraTreesClassifier(max_depth=40, max_features='sqrt', n_estimators=600)\n",
    "# final_extra_trees_A.fit(x_train, y_train)\n",
    "# print(final_extra_trees_A.score(x_test, y_test))\n",
    "# final_extra_trees_B = ExtraTreesClassifier(max_depth=40, max_features='sqrt', n_estimators=100)\n",
    "# final_extra_trees_B.fit(x_train, y_train)\n",
    "# print(final_extra_trees_B.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### again, our random grid search had it on point when combining the parameters together. let's check the coefficients than wrap it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX: remove ML code\n",
    "# featuers_coefficients = final_extra_trees.feature_importances_.tolist()\n",
    "feature_names = x_copy.columns\n",
    "# ALEX: remove ML code\n",
    "# feats = pd.DataFrame(pd.Series(featuers_coefficients, feature_names).sort_values(ascending=False),columns=['Coefficient'])\n",
    "# feats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
