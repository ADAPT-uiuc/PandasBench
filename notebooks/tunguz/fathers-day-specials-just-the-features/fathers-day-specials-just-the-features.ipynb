{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scale_input_data(scale_factor):\n",
    "  file_bases = ['./input/test', './input/train', './input/historical_transactions',\n",
    "                './input/new_merchant_transactions']\n",
    "  for file_base in file_bases:\n",
    "    import pandas as pd\n",
    "    import shutil\n",
    "    if scale_factor == 1.0:\n",
    "      shutil.copyfile(file_base + '.csv', file_base + '.scaled.csv')\n",
    "      continue\n",
    "    df_to_scale = pd.read_csv(file_base + '.csv')\n",
    "    new_num_rows = int(scale_factor * len(df_to_scale))\n",
    "    if scale_factor <= 1.0:\n",
    "      df_to_scale = df_to_scale.iloc[:new_num_rows]\n",
    "    else:\n",
    "      while len(df_to_scale) < new_num_rows:\n",
    "        df_to_scale = pd.concat([df_to_scale, df_to_scale[:min(new_num_rows - len(df_to_scale), len(df_to_scale))]])\n",
    "    df_to_scale.to_csv(file_base + '.scaled.csv', index=False)\n",
    "\n",
    "if 'INPUT_SCALE_FACTOR' in os.environ:\n",
    "  scale_input_data(float(os.environ['INPUT_SCALE_FACTOR']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1dee50e02ba20b8aabe9775e0f8b19c40e1bcff8"
   },
   "source": [
    "This is a version of Ashish Gupta's notebook, which in turn was based on Chau Ngoc Huynh's kernel (3.699) and Panchajanya Banerjee's idea to include the holidays. This purpose of this notebook is just to save teh feature-engineered train and test sets, so that they can be further used in other kenels. \n",
    "\n",
    "\n",
    "Special Brazil holidays:\n",
    "\n",
    "For 2017-18\n",
    "\n",
    "1. Mother's Day : Second Sunday of May : May 13 2018\n",
    "2. Father's Day : Second Sunday of August :  August 13 2017\n",
    "3. Valentine's Day : 12th June, 2017\n",
    "4. Children's Day : 12th October 2017\n",
    "5. Black Friday : 24th November 2017\n",
    "6. Christmas day : 25th December 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "a594abe9f66c5a090177dff966d6bedf63718a9c"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "exec(os.environ['IREWR_IMPORTS'])\n",
    "# FIRST-AUTHOR: remove plotting, GC code\n",
    "# import os\n",
    "# import time\n",
    "# import warnings\n",
    "# import gc\n",
    "# import os\n",
    "# from six.moves import urllib\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import datetime\n",
    "# FIRST-AUTHOR: remove plotting, ML code\n",
    "# warnings.filterwarnings('ignore')\n",
    "# %matplotlib inline\n",
    "# plt.style.use('seaborn')\n",
    "# from scipy.stats import norm, skew\n",
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "bfbbf30dcb4c354827aae6fff1aeaa632a9e874e"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove ML code\n",
    "# #Add All the Models Libraries\n",
    "\n",
    "# # Scalers\n",
    "# from sklearn.utils import shuffle\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# # Models\n",
    "\n",
    "# from sklearn.linear_model import Lasso\n",
    "# from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error\n",
    "\n",
    "# from sklearn.model_selection import train_test_split #training and testing data split\n",
    "# from sklearn import metrics #accuracy measure\n",
    "# from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "# from scipy.stats import reciprocal, uniform\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # Cross-validation\n",
    "# from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "# from sklearn.model_selection import cross_val_score #score evaluation\n",
    "# from sklearn.model_selection import cross_val_predict #prediction\n",
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "# # GridSearchCV\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# #Common data processors\n",
    "# from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "# from sklearn import feature_selection\n",
    "# from sklearn import model_selection\n",
    "# from sklearn import metrics\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# from sklearn.utils import check_array\n",
    "# from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "798008f46ddb84ca5ec248990d56b64e5ec84e5a"
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(123)\n",
    "# FIRST-AUTHOR: remove GC code, plotting\n",
    "# gc.collect()\n",
    "# # To plot pretty figures\n",
    "# %matplotlib inline\n",
    "# plt.rcParams['axes.labelsize'] = 14\n",
    "# plt.rcParams['xtick.labelsize'] = 12\n",
    "# plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "47d5d544fa7e4bf9305814409c3cabc148b0169a"
   },
   "outputs": [],
   "source": [
    "#Reduce the memory usage - Inspired by Panchajanya Banerjee\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "450f05f1bb8bd470972ec8a8f80a96f4b56402a7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(pd.read_csv('./input/train.scaled.csv',parse_dates=[\"first_active_month\"]))\n",
    "test = reduce_mem_usage(pd.read_csv('./input/test.scaled.csv', parse_dates=[\"first_active_month\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "2352513f1d53487e789e41fbfa050199154f8916"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201917, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "78407176c3c40b9d15227165028d6b61f94f3ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 201917 entries, 0 to 201916\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   first_active_month  201917 non-null  datetime64[ns]\n",
      " 1   card_id             201917 non-null  object        \n",
      " 2   feature_1           201917 non-null  int8          \n",
      " 3   feature_2           201917 non-null  int8          \n",
      " 4   feature_3           201917 non-null  int8          \n",
      " 5   target              201917 non-null  float16       \n",
      "dtypes: datetime64[ns](1), float16(1), int8(3), object(1)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "ce244a5a05abaa8c91ff3c73d8c5f86ba592c9a1"
   },
   "outputs": [],
   "source": [
    "# Now extract the month, year, day, weekday\n",
    "train[\"month\"] = train[\"first_active_month\"].dt.month\n",
    "train[\"year\"] = train[\"first_active_month\"].dt.year\n",
    "# FIRST-AUTHOR: make notebook run\n",
    "# train['week'] = train[\"first_active_month\"].dt.weekofyear\n",
    "train['week'] = train[\"first_active_month\"].dt.isocalendar().week\n",
    "train['dayofweek'] = train['first_active_month'].dt.dayofweek\n",
    "# FIRST-AUTHOR: make notebook run\n",
    "# train['days'] = (datetime.date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\n",
    "train['days'] = pd.to_timedelta((datetime.date(2018, 2, 1) - train['first_active_month'].dt.date)).dt.days\n",
    "\n",
    "test[\"month\"] = test[\"first_active_month\"].dt.month\n",
    "test[\"year\"] = test[\"first_active_month\"].dt.year\n",
    "# FIRST-AUTHOR: make notebook run\n",
    "# test['week'] = test[\"first_active_month\"].dt.weekofyear\n",
    "test['week'] = test[\"first_active_month\"].dt.isocalendar().week\n",
    "test['dayofweek'] = test['first_active_month'].dt.dayofweek\n",
    "# FIRST-AUTHOR: make notebook run\n",
    "# test['days'] = (datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days\n",
    "test['days'] = pd.to_timedelta(datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b54460353af274062f649afe26f034c4126a6a84"
   },
   "source": [
    "Now we will try to extract more features from Transactions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "f9835dd6582d3e9c36954b498810929c8438d2d2"
   },
   "outputs": [],
   "source": [
    "# Taking Reference from Other Kernels\n",
    "def aggregate_transaction_hist(trans, prefix):  \n",
    "        \n",
    "    agg_func = {\n",
    "        'purchase_date' : ['max','min'],\n",
    "        'month_diff' : ['mean'],\n",
    "        'weekend' : ['sum', 'mean'],\n",
    "        'authorized_flag': ['sum', 'mean'],\n",
    "        'category_1': ['sum','mean'],\n",
    "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'installments': ['sum', 'mean', 'max', 'min', 'std'],  \n",
    "        #'merchant_id': ['nunique'],\n",
    "        'month_lag': ['max','min','mean','var'],\n",
    "        'month_diff' : ['mean'],\n",
    "        'card_id' : ['size'],\n",
    "        'month': ['nunique'],\n",
    "        'hour': ['nunique'],\n",
    "        'weekofyear': ['nunique'],\n",
    "        'dayofweek': ['nunique'],\n",
    "        'year': ['nunique'],\n",
    "        'subsector_id': ['nunique'],\n",
    "        'merchant_category_id' : ['nunique'],\n",
    "        'Christmas_Day_2017':['mean'],\n",
    "        #'Mothers_Day_2017':['mean'],\n",
    "        'fathers_day_2017':['mean'],\n",
    "        'Children_day_2017':['mean'],\n",
    "        'Black_Friday_2017':['mean'],\n",
    "        'Valentine_day_2017':['mean'],\n",
    "        'Mothers_Day_2018':['mean']\n",
    "    }\n",
    "    \n",
    "    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n",
    "    agg_trans.columns = [prefix + '_'.join(col).strip() \n",
    "                           for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "    \n",
    "    df = (trans.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='{}transactions_count'.format(prefix)))\n",
    "    \n",
    "    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n",
    "    \n",
    "    return agg_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "06a04f3393a83cb2687b76deba417012e18ef606",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "transactions = reduce_mem_usage(pd.read_csv('./input/historical_transactions.scaled.csv'))\n",
    "transactions['authorized_flag'] = transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "transactions['category_1'] = transactions['category_1'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "1decf075c9422fe20823691972f055b8a1c84f50",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_905329/2798247112.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  transactions['category_2'] = transactions['category_2'].fillna(1.0,inplace=True)\n",
      "/tmp/ipykernel_905329/2798247112.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  transactions['category_3'] = transactions['category_3'].fillna('A',inplace=True)\n",
      "/tmp/ipykernel_905329/2798247112.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  transactions['merchant_id'] = transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "transactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])\n",
    "transactions['year'] = transactions['purchase_date'].dt.year\n",
    "# FIRST-AUTHOR: make notebook run\n",
    "# transactions['weekofyear'] = transactions['purchase_date'].dt.weekofyear\n",
    "transactions['weekofyear'] = transactions['purchase_date'].dt.isocalendar().week\n",
    "transactions['month'] = transactions['purchase_date'].dt.month\n",
    "transactions['dayofweek'] = transactions['purchase_date'].dt.dayofweek\n",
    "transactions['weekend'] = (transactions.purchase_date.dt.weekday >=5).astype(int)\n",
    "transactions['hour'] = transactions['purchase_date'].dt.hour \n",
    "transactions['month_diff'] = ((datetime.datetime.today() - transactions['purchase_date']).dt.days)//30\n",
    "transactions['month_diff'] += transactions['month_lag']\n",
    "\n",
    "#impute missing values - This is now excluded.\n",
    "transactions['category_2'] = transactions['category_2'].fillna(1.0,inplace=True)\n",
    "transactions['category_3'] = transactions['category_3'].fillna('A',inplace=True)\n",
    "transactions['merchant_id'] = transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "# FIRST-AUTHOR: remove GC code\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "93a165f209a8853e4c8e1f624becd0506e6fb0f1"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: make notebook run\n",
    "# agg_func = {\n",
    "#         'mean': ['mean'],\n",
    "#     }\n",
    "# for col in ['category_2','category_3']:\n",
    "#     transactions[col+'_mean'] = transactions['purchase_amount'].groupby(transactions[col]).agg(agg_func)\n",
    "for col in ['category_2','category_3']:\n",
    "    transactions[col+'_mean'] = transactions['purchase_amount'].groupby(transactions[col]).agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "dee055f4b8bbb8be045d9a109aae2299d3b2b242"
   },
   "outputs": [],
   "source": [
    "# New Features with Key Shopping times considered in the dataset. if the purchase has been made within 60 days, it is considered as an influence\n",
    "#Christmas : December 25 2017\n",
    "transactions['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "#Mothers Day: May 14 2017\n",
    "#transactions['Mothers_Day_2017'] = (pd.to_datetime('2017-05-04') - transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "#fathers day: August 13 2017\n",
    "transactions['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "#Childrens day: October 12 2017\n",
    "transactions['Children_day_2017'] = (pd.to_datetime('2017-10-12') - transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "#Black Friday : 24th November 2017\n",
    "transactions['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "#Valentines Day\n",
    "transactions['Valentine_day_2017'] = (pd.to_datetime('2017-06-12') - transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "#2018\n",
    "#Mothers Day: May 13 2018\n",
    "transactions['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "# FIRST-AUTHOR: remove GC code\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "95128e635d5c0ad26b170652971aca4fc9c41ad1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merge_trans = aggregate_transaction_hist(transactions, prefix='hist_')\n",
    "del transactions\n",
    "# FIRST-AUTHOR: remove GC code\n",
    "# gc.collect()\n",
    "train = pd.merge(train, merge_trans, on='card_id',how='left')\n",
    "test = pd.merge(test, merge_trans, on='card_id',how='left')\n",
    "del merge_trans\n",
    "# FIRST-AUTHOR: remove GC code\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "c2ecbbf35630592774f720ccf88849cb8dfe9987"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>...</th>\n",
       "      <th>hist_dayofweek_nunique</th>\n",
       "      <th>hist_year_nunique</th>\n",
       "      <th>hist_subsector_id_nunique</th>\n",
       "      <th>hist_merchant_category_id_nunique</th>\n",
       "      <th>hist_Christmas_Day_2017_mean</th>\n",
       "      <th>hist_fathers_day_2017_mean</th>\n",
       "      <th>hist_Children_day_2017_mean</th>\n",
       "      <th>hist_Black_Friday_2017_mean</th>\n",
       "      <th>hist_Valentine_day_2017_mean</th>\n",
       "      <th>hist_Mothers_Day_2018_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820312</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>41</td>\n",
       "      <td>13.123077</td>\n",
       "      <td>6.265385</td>\n",
       "      <td>26.765385</td>\n",
       "      <td>16.465385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.753846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392822</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>57</td>\n",
       "      <td>10.648571</td>\n",
       "      <td>12.551429</td>\n",
       "      <td>13.788571</td>\n",
       "      <td>9.834286</td>\n",
       "      <td>6.568571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows \u00d7 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0         2017-06-01  C_ID_92a2005557          5          2          1   \n",
       "1         2017-01-01  C_ID_3d0044924f          4          1          0   \n",
       "\n",
       "     target  month  year  week  dayofweek  ...  hist_dayofweek_nunique  \\\n",
       "0 -0.820312      6  2017    22          3  ...                       7   \n",
       "1  0.392822      1  2017    52          6  ...                       7   \n",
       "\n",
       "   hist_year_nunique hist_subsector_id_nunique  \\\n",
       "0                  2                        21   \n",
       "1                  2                        24   \n",
       "\n",
       "  hist_merchant_category_id_nunique  hist_Christmas_Day_2017_mean  \\\n",
       "0                                41                     13.123077   \n",
       "1                                57                     10.648571   \n",
       "\n",
       "   hist_fathers_day_2017_mean  hist_Children_day_2017_mean  \\\n",
       "0                    6.265385                    26.765385   \n",
       "1                   12.551429                    13.788571   \n",
       "\n",
       "   hist_Black_Friday_2017_mean  hist_Valentine_day_2017_mean  \\\n",
       "0                    16.465385                      0.000000   \n",
       "1                     9.834286                      6.568571   \n",
       "\n",
       "   hist_Mothers_Day_2018_mean  \n",
       "0                    7.753846  \n",
       "1                    0.000000  \n",
       "\n",
       "[2 rows x 49 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "a3e6c6e3dec6f2153d02a684fb7ddbbace4c5550"
   },
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "train['hist_purchase_date_max'] = pd.to_datetime(train['hist_purchase_date_max'])\n",
    "train['hist_purchase_date_min'] = pd.to_datetime(train['hist_purchase_date_min'])\n",
    "train['hist_purchase_date_diff'] = (train['hist_purchase_date_max'] - train['hist_purchase_date_min']).dt.days\n",
    "train['hist_purchase_date_average'] = train['hist_purchase_date_diff']/train['hist_card_id_size']\n",
    "train['hist_purchase_date_uptonow'] = (datetime.datetime.today() - train['hist_purchase_date_max']).dt.days\n",
    "train['hist_first_buy'] = (train['hist_purchase_date_min'] - train['first_active_month']).dt.days\n",
    "for feature in ['hist_purchase_date_max','hist_purchase_date_min']:\n",
    "    train[feature] = train[feature].astype(np.int64) * 1e-9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "370757bfba37b643f463f385d9e878ff27e320bc"
   },
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "test['hist_purchase_date_max'] = pd.to_datetime(test['hist_purchase_date_max'])\n",
    "test['hist_purchase_date_min'] = pd.to_datetime(test['hist_purchase_date_min'])\n",
    "test['hist_purchase_date_diff'] = (test['hist_purchase_date_max'] - test['hist_purchase_date_min']).dt.days\n",
    "test['hist_purchase_date_average'] = test['hist_purchase_date_diff']/test['hist_card_id_size']\n",
    "test['hist_purchase_date_uptonow'] = (datetime.datetime.today() - test['hist_purchase_date_max']).dt.days\n",
    "test['hist_first_buy'] = (test['hist_purchase_date_min'] - test['first_active_month']).dt.days\n",
    "for feature in ['hist_purchase_date_max','hist_purchase_date_min']:\n",
    "    test[feature] = test[feature].astype(np.int64) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "88381a0c6e50924860a133e7d61838c86ee1eab1"
   },
   "outputs": [],
   "source": [
    "# Taking Reference from Other Kernels\n",
    "def aggregate_transaction_new(trans, prefix):  \n",
    "        \n",
    "    agg_func = {\n",
    "        'purchase_date' : ['max','min'],\n",
    "        'month_diff' : ['mean'],\n",
    "        'weekend' : ['sum', 'mean'],\n",
    "        'authorized_flag': ['sum'],\n",
    "        'category_1': ['sum','mean'],\n",
    "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'installments': ['sum', 'mean', 'max', 'min', 'std'],  \n",
    "        #'merchant_id': ['nunique'],\n",
    "        'month_lag': ['max','min','mean','var'],\n",
    "        'month_diff' : ['mean'],\n",
    "        'card_id' : ['size'],\n",
    "        'month': ['nunique'],\n",
    "        'hour': ['nunique'],\n",
    "        'weekofyear': ['nunique'],\n",
    "        'dayofweek': ['nunique'],\n",
    "        'year': ['nunique'],\n",
    "        'subsector_id': ['nunique'],\n",
    "        'merchant_category_id' : ['nunique'],\n",
    "        'Christmas_Day_2017':['mean'],\n",
    "        #'Mothers_Day_2017':['mean'],\n",
    "        'fathers_day_2017':['mean'],\n",
    "        'Children_day_2017':['mean'],\n",
    "        'Black_Friday_2017':['mean'],\n",
    "        'Valentine_Day_2017' : ['mean'],\n",
    "        'Mothers_Day_2018':['mean']\n",
    "    }\n",
    "    \n",
    "    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n",
    "    agg_trans.columns = [prefix + '_'.join(col).strip() \n",
    "                           for col in agg_trans.columns.values]\n",
    "    agg_trans.reset_index(inplace=True)\n",
    "    \n",
    "    df = (trans.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='{}transactions_count'.format(prefix)))\n",
    "    \n",
    "    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n",
    "    \n",
    "    return agg_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "9c653335214ef763a6d9e13de46db5800a75c5a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 114.20 Mb (45.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Now extract the data from the new transactions\n",
    "new_transactions = reduce_mem_usage(pd.read_csv('./input/new_merchant_transactions.scaled.csv'))\n",
    "new_transactions['authorized_flag'] = new_transactions['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "new_transactions['category_1'] = new_transactions['category_1'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "fd7fc65f351e45fdd1e8a43b730b8ed50b2e4456",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_905329/3168607165.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  new_transactions['category_2'] = new_transactions['category_2'].fillna(1.0,inplace=True)\n",
      "/tmp/ipykernel_905329/3168607165.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  new_transactions['category_3'] = new_transactions['category_3'].fillna('A',inplace=True)\n",
      "/tmp/ipykernel_905329/3168607165.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  new_transactions['merchant_id'] = new_transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "new_transactions['purchase_date'] = pd.to_datetime(new_transactions['purchase_date'])\n",
    "new_transactions['year'] = new_transactions['purchase_date'].dt.year\n",
    "# FIRST-AUTHOR: make notebook run\n",
    "# new_transactions['weekofyear'] = new_transactions['purchase_date'].dt.weekofyear\n",
    "new_transactions['weekofyear'] = new_transactions['purchase_date'].dt.isocalendar().week\n",
    "new_transactions['month'] = new_transactions['purchase_date'].dt.month\n",
    "new_transactions['dayofweek'] = new_transactions['purchase_date'].dt.dayofweek\n",
    "new_transactions['weekend'] = (new_transactions.purchase_date.dt.weekday >=5).astype(int)\n",
    "new_transactions['hour'] = new_transactions['purchase_date'].dt.hour \n",
    "new_transactions['month_diff'] = ((datetime.datetime.today() - new_transactions['purchase_date']).dt.days)//30\n",
    "new_transactions['month_diff'] += new_transactions['month_lag']\n",
    "\n",
    "#impute missing values\n",
    "new_transactions['category_2'] = new_transactions['category_2'].fillna(1.0,inplace=True)\n",
    "new_transactions['category_3'] = new_transactions['category_3'].fillna('A',inplace=True)\n",
    "new_transactions['merchant_id'] = new_transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "\n",
    "# New Features with Key Shopping times considered in the dataset. if the purchase has been made within 60 days, \n",
    "# it is considered as an influence\n",
    "\n",
    "#Christmas : December 25 2017\n",
    "new_transactions['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - new_transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "#Mothers Day: May 14 2017 - Was not significant in Feature Importance\n",
    "#new_transactions['Mothers_Day_2017'] = (pd.to_datetime('2017-06-04') - new_transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "#fathers day: August 13 2017\n",
    "new_transactions['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - new_transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "#Childrens day: October 12 2017\n",
    "new_transactions['Children_day_2017'] = (pd.to_datetime('2017-10-12') - new_transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "#Valentine's Day : 12th June, 2017\n",
    "new_transactions['Valentine_Day_2017'] = (pd.to_datetime('2017-06-12') - new_transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "#Black Friday : 24th November 2017\n",
    "new_transactions['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - new_transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "#2018\n",
    "#Mothers Day: May 13 2018\n",
    "new_transactions['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - new_transactions['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "# FIRST-AUTHOR: make notebook run\n",
    "# agg_func = {\n",
    "#         'mean': ['mean'],\n",
    "#     }\n",
    "# for col in ['category_2','category_3']:\n",
    "#     new_transactions[col+'_mean'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg(agg_func)\n",
    "for col in ['category_2','category_3']:\n",
    "    new_transactions[col+'_mean'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('mean')\n",
    "\n",
    "# FIRST-AUTHOR: remove GC code\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "5ebf2d2ac4064c8e9a008f555c7f902ca0fdf853",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merge_new = aggregate_transaction_new(new_transactions, prefix='new_')\n",
    "del new_transactions\n",
    "# FIRST-AUTHOR: remove GC code\n",
    "# gc.collect()\n",
    "\n",
    "train = pd.merge(train, merge_new, on='card_id',how='left')\n",
    "test = pd.merge(test, merge_new, on='card_id',how='left')\n",
    "del merge_new\n",
    "\n",
    "# FIRST-AUTHOR: remove GC code\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "4741d24d2c897ebd286807d0d3bd658800e316ca"
   },
   "outputs": [],
   "source": [
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "train['new_purchase_date_max'] = pd.to_datetime(train['new_purchase_date_max'])\n",
    "train['new_purchase_date_min'] = pd.to_datetime(train['new_purchase_date_min'])\n",
    "train['new_purchase_date_diff'] = (train['new_purchase_date_max'] - train['new_purchase_date_min']).dt.days\n",
    "train['new_purchase_date_average'] = train['new_purchase_date_diff']/train['new_card_id_size']\n",
    "train['new_purchase_date_uptonow'] = (datetime.datetime.today() - train['new_purchase_date_max']).dt.days\n",
    "train['new_first_buy'] = (train['new_purchase_date_min'] - train['first_active_month']).dt.days\n",
    "for feature in ['new_purchase_date_max','new_purchase_date_min']:\n",
    "    train[feature] = train[feature].astype(np.int64) * 1e-9\n",
    "\n",
    "#Feature Engineering - Adding new features inspired by Chau's first kernel\n",
    "test['new_purchase_date_max'] = pd.to_datetime(test['new_purchase_date_max'])\n",
    "test['new_purchase_date_min'] = pd.to_datetime(test['new_purchase_date_min'])\n",
    "test['new_purchase_date_diff'] = (test['new_purchase_date_max'] - test['new_purchase_date_min']).dt.days\n",
    "test['new_purchase_date_average'] = test['new_purchase_date_diff']/test['new_card_id_size']\n",
    "test['new_purchase_date_uptonow'] = (datetime.datetime.today() - test['new_purchase_date_max']).dt.days\n",
    "test['new_first_buy'] = (test['new_purchase_date_min'] - test['first_active_month']).dt.days\n",
    "for feature in ['new_purchase_date_max','new_purchase_date_min']:\n",
    "    test[feature] = test[feature].astype(np.int64) * 1e-9\n",
    "    \n",
    "#added new feature - Interactive\n",
    "train['card_id_total'] = train['new_card_id_size'] + train['hist_card_id_size']\n",
    "train['purchase_amount_total'] = train['new_purchase_amount_sum'] + train['hist_purchase_amount_sum']\n",
    "\n",
    "test['card_id_total'] = test['new_card_id_size'] + test['hist_card_id_size']\n",
    "test['purchase_amount_total'] = test['new_purchase_amount_sum'] + test['hist_purchase_amount_sum']\n",
    "\n",
    "# FIRST-AUTHOR: remove GC code\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "de5dbc4519c349552d66f192c47d73872ddf0c71",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201917, 96)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now check the shape of Train and Test Data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "d19048974a748af94a1db2a98f754dad9934f8be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123623, 95)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "e98856ac9c9f94acc517b8008f9dcb273228bbf2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>...</th>\n",
       "      <th>new_Children_day_2017_mean</th>\n",
       "      <th>new_Black_Friday_2017_mean</th>\n",
       "      <th>new_Valentine_Day_2017_mean</th>\n",
       "      <th>new_Mothers_Day_2018_mean</th>\n",
       "      <th>new_purchase_date_diff</th>\n",
       "      <th>new_purchase_date_average</th>\n",
       "      <th>new_purchase_date_uptonow</th>\n",
       "      <th>new_first_buy</th>\n",
       "      <th>card_id_total</th>\n",
       "      <th>purchase_amount_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820312</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.739130</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.347826</td>\n",
       "      <td>2382.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>-179.212631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392822</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.833333</td>\n",
       "      <td>56.0</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>2412.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>-214.361801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>8</td>\n",
       "      <td>2016</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2383.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-29.867586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142456</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.714286</td>\n",
       "      <td>41.0</td>\n",
       "      <td>5.857143</td>\n",
       "      <td>2393.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-54.146149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159790</td>\n",
       "      <td>11</td>\n",
       "      <td>2017</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.194444</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>2382.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>-68.613800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows \u00d7 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0         2017-06-01  C_ID_92a2005557          5          2          1   \n",
       "1         2017-01-01  C_ID_3d0044924f          4          1          0   \n",
       "2         2016-08-01  C_ID_d639edf6cd          2          2          0   \n",
       "3         2017-09-01  C_ID_186d6a6901          4          3          0   \n",
       "4         2017-11-01  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  month  year  week  dayofweek  ...  new_Children_day_2017_mean  \\\n",
       "0 -0.820312      6  2017    22          3  ...                         0.0   \n",
       "1  0.392822      1  2017    52          6  ...                         0.0   \n",
       "2  0.687988      8  2016    31          0  ...                         0.0   \n",
       "3  0.142456      9  2017    35          4  ...                         0.0   \n",
       "4 -0.159790     11  2017    44          2  ...                         0.0   \n",
       "\n",
       "   new_Black_Friday_2017_mean  new_Valentine_Day_2017_mean  \\\n",
       "0                         0.0                          0.0   \n",
       "1                         0.0                          0.0   \n",
       "2                         0.0                          0.0   \n",
       "3                         0.0                          0.0   \n",
       "4                         0.0                          0.0   \n",
       "\n",
       "   new_Mothers_Day_2018_mean  new_purchase_date_diff  \\\n",
       "0                  41.739130                    54.0   \n",
       "1                  56.833333                    56.0   \n",
       "2                  14.000000                     0.0   \n",
       "3                  37.714286                    41.0   \n",
       "4                  41.194444                    57.0   \n",
       "\n",
       "   new_purchase_date_average  new_purchase_date_uptonow  new_first_buy  \\\n",
       "0                   2.347826                     2382.0          277.0   \n",
       "1                   9.333333                     2412.0          396.0   \n",
       "2                   0.000000                     2383.0          635.0   \n",
       "3                   5.857143                     2393.0          187.0   \n",
       "4                   1.583333                     2382.0          121.0   \n",
       "\n",
       "   card_id_total  purchase_amount_total  \n",
       "0          283.0            -179.212631  \n",
       "1          356.0            -214.361801  \n",
       "2           44.0             -29.867586  \n",
       "3           84.0             -54.146149  \n",
       "4          169.0             -68.613800  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "26324d62739c824e870c7f97d247c2744899d422"
   },
   "source": [
    "Imputations and Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "b3dd37511e6a80f0ce02ec552a9930bbb3a5b309",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Programs/python/dias/.venv/lib64/python3.10/site-packages/numpy/core/fromnumeric.py:84: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "#Check for missing values in training set\n",
    "nulls = np.sum(train.isnull())\n",
    "nullcols = nulls.loc[(nulls != 0)]\n",
    "dtypes = train.dtypes\n",
    "dtypes2 = dtypes.loc[(nulls != 0)]\n",
    "info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\n",
    "#print(info)\n",
    "#print(\"There are\", len(nullcols), \"columns with missing values in data set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "1737d40d9a033b7de10b4e401743ee1f2324fa32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Programs/python/dias/.venv/lib64/python3.10/site-packages/numpy/core/fromnumeric.py:84: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "#Check for missing values in training set\n",
    "nulls = np.sum(test.isnull())\n",
    "nullcols = nulls.loc[(nulls != 0)]\n",
    "dtypes = test.dtypes\n",
    "dtypes2 = dtypes.loc[(nulls != 0)]\n",
    "info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\n",
    "#print(info)\n",
    "#print(\"There are\", len(nullcols), \"columns with missing values in test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1b45687c9755b0672753aa84a0765480ff1d67b"
   },
   "source": [
    "Impute any values will significantly affect the RMSE score for test set. So Imputations have been excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "868b8f492ded29822f14d74fb4350f03ee4b3f2d"
   },
   "outputs": [],
   "source": [
    "numeric_dtypes = ['float64']\n",
    "numerics = []\n",
    "for i in train.columns:\n",
    "    if train[i].dtype in numeric_dtypes: \n",
    "        numerics.append(i)\n",
    "        \n",
    "#train.update(train[numerics].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "fd0491f1ce6813e21979ca54e534bfe5bdd76260"
   },
   "outputs": [],
   "source": [
    "numeric_dtypes = ['float64']\n",
    "numerics = []\n",
    "for i in test.columns:\n",
    "    if test[i].dtype in numeric_dtypes: \n",
    "        numerics.append(i)\n",
    "        \n",
    "#test.update(test[numerics].fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b0f12459d09a2d88ea34a60c85c1aa7553ef1aa"
   },
   "source": [
    "Detect and Correct Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "0dd1c98207ed527cfd85a6b1a71235f3424629ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outliers\n",
       "0    199710\n",
       "1      2207\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the Outliers if any \n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "6b5d796a748bbdf483888c96f37634f07e4c8663"
   },
   "outputs": [],
   "source": [
    "for features in ['feature_1','feature_2','feature_3']:\n",
    "    order_label = train.groupby([features])['outliers'].mean()\n",
    "    train[features] = train[features].map(order_label)\n",
    "    test[features] =  test[features].map(order_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "ff17dbfc2d4087d08c9ee9a719c6f4f867692b48"
   },
   "outputs": [],
   "source": [
    "# Get the X and Y\n",
    "df_train_columns = [c for c in train.columns if c not in ['card_id', 'first_active_month', 'outliers']]\n",
    "df_test_columns = [c for c in train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "e5a2945056e41f7c612821d8d5be552a7d944de1"
   },
   "outputs": [],
   "source": [
    "train['fold'] = 0\n",
    "# FIRST-AUTHOR: remove ML code\n",
    "# folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1974)\n",
    "\n",
    "# for fold_, (trn_idx, val_idx) in enumerate(folds.split(train,train['outliers'].values)):\n",
    "#     train['fold'].iloc[val_idx] = fold_\n",
    "_ = train['outliers'].values\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "75b60f1f4a5de71fdfc081e952b57f54c9658e8a"
   },
   "outputs": [],
   "source": [
    "df_train_columns.append('fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "2aa124e3731a2e6e010f3558b33cecc85c05af1e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>...</th>\n",
       "      <th>new_Valentine_Day_2017_mean</th>\n",
       "      <th>new_Mothers_Day_2018_mean</th>\n",
       "      <th>new_purchase_date_diff</th>\n",
       "      <th>new_purchase_date_average</th>\n",
       "      <th>new_purchase_date_uptonow</th>\n",
       "      <th>new_first_buy</th>\n",
       "      <th>card_id_total</th>\n",
       "      <th>purchase_amount_total</th>\n",
       "      <th>outliers</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>-0.820312</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.739130</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.347826</td>\n",
       "      <td>2382.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>-179.212631</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.392822</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.833333</td>\n",
       "      <td>56.0</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>2412.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>-214.361801</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>8</td>\n",
       "      <td>2016</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2383.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-29.867586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.142456</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.714286</td>\n",
       "      <td>41.0</td>\n",
       "      <td>5.857143</td>\n",
       "      <td>2393.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-54.146149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>-0.159790</td>\n",
       "      <td>11</td>\n",
       "      <td>2017</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.194444</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>2382.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>-68.613800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>C_ID_0894217f2f</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.871582</td>\n",
       "      <td>9</td>\n",
       "      <td>2016</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>2607.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-15.175980</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>C_ID_7e63323c00</td>\n",
       "      <td>0.010479</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.600000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2405.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>-189.024124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>C_ID_dfa21fc124</td>\n",
       "      <td>0.010479</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>2.136719</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.333333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2451.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-15.663989</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>C_ID_fe0fdac8ea</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-9.971558</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>C_ID_bf62c0b49d</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>8</td>\n",
       "      <td>2016</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.666667</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>-79.137260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>C_ID_92853cdb2c</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>-1.029297</td>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-61.945114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016-03-01</td>\n",
       "      <td>C_ID_269d816788</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>-2.433594</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2430.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-13.102302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>C_ID_61d50d7057</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>1.304688</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-24.361458</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>C_ID_4e07413433</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.093628</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>2501.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-14.985926</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>C_ID_b6302b31c6</td>\n",
       "      <td>0.010479</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>1.597656</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2503.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>-40.772755</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>C_ID_3b69154173</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>1.010742</td>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>C_ID_9feec11e78</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>1.131836</td>\n",
       "      <td>7</td>\n",
       "      <td>2016</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2428.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>-74.502594</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>C_ID_f6658dbefe</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.318115</td>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2408.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>-50.854855</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>C_ID_4a7dda0f9e</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.757324</td>\n",
       "      <td>11</td>\n",
       "      <td>2017</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.250000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>2391.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-17.468821</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_6adae2a906</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.334229</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.800000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>2426.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>-325.046173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows \u00d7 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0          2017-06-01  C_ID_92a2005557   0.013145   0.008752   0.011428   \n",
       "1          2017-01-01  C_ID_3d0044924f   0.010712   0.011385   0.010283   \n",
       "2          2016-08-01  C_ID_d639edf6cd   0.010610   0.008752   0.010283   \n",
       "3          2017-09-01  C_ID_186d6a6901   0.010712   0.014166   0.010283   \n",
       "4          2017-11-01  C_ID_cdbd2c0db2   0.008058   0.014166   0.010283   \n",
       "5          2016-09-01  C_ID_0894217f2f   0.010712   0.008752   0.010283   \n",
       "6          2016-12-01  C_ID_7e63323c00   0.010479   0.008752   0.011428   \n",
       "7          2017-09-01  C_ID_dfa21fc124   0.010479   0.008752   0.011428   \n",
       "8          2017-08-01  C_ID_fe0fdac8ea   0.010610   0.011385   0.010283   \n",
       "9          2016-08-01  C_ID_bf62c0b49d   0.010610   0.008752   0.010283   \n",
       "10         2016-10-01  C_ID_92853cdb2c   0.013145   0.008752   0.011428   \n",
       "11         2016-03-01  C_ID_269d816788   0.010610   0.008752   0.010283   \n",
       "12         2017-09-01  C_ID_61d50d7057   0.013145   0.008752   0.011428   \n",
       "13         2017-04-01  C_ID_4e07413433   0.010610   0.011385   0.010283   \n",
       "14         2017-08-01  C_ID_b6302b31c6   0.010479   0.008752   0.011428   \n",
       "15         2017-02-01  C_ID_3b69154173   0.010610   0.011385   0.010283   \n",
       "16         2016-07-01  C_ID_9feec11e78   0.010610   0.008752   0.010283   \n",
       "17         2016-12-01  C_ID_f6658dbefe   0.010712   0.008752   0.010283   \n",
       "18         2017-11-01  C_ID_4a7dda0f9e   0.010610   0.011385   0.010283   \n",
       "19         2017-01-01  C_ID_6adae2a906   0.010712   0.011385   0.010283   \n",
       "\n",
       "      target  month  year  week  dayofweek  ...  new_Valentine_Day_2017_mean  \\\n",
       "0  -0.820312      6  2017    22          3  ...                          0.0   \n",
       "1   0.392822      1  2017    52          6  ...                          0.0   \n",
       "2   0.687988      8  2016    31          0  ...                          0.0   \n",
       "3   0.142456      9  2017    35          4  ...                          0.0   \n",
       "4  -0.159790     11  2017    44          2  ...                          0.0   \n",
       "5   0.871582      9  2016    35          3  ...                          0.0   \n",
       "6   0.230103     12  2016    48          3  ...                          0.0   \n",
       "7   2.136719      9  2017    35          4  ...                          0.0   \n",
       "8  -0.065430      8  2017    31          1  ...                          0.0   \n",
       "9   0.300049      8  2016    31          0  ...                          0.0   \n",
       "10 -1.029297     10  2016    39          5  ...                          0.0   \n",
       "11 -2.433594      3  2016     9          1  ...                          0.0   \n",
       "12  1.304688      9  2017    35          4  ...                          0.0   \n",
       "13  0.093628      4  2017    13          5  ...                          0.0   \n",
       "14  1.597656      8  2017    31          1  ...                          0.0   \n",
       "15  1.010742      2  2017     5          2  ...                          NaN   \n",
       "16  1.131836      7  2016    26          4  ...                          0.0   \n",
       "17  0.318115     12  2016    48          3  ...                          0.0   \n",
       "18  0.757324     11  2017    44          2  ...                          0.0   \n",
       "19  0.334229      1  2017    52          6  ...                          0.0   \n",
       "\n",
       "    new_Mothers_Day_2018_mean  new_purchase_date_diff  \\\n",
       "0                   41.739130                    54.0   \n",
       "1                   56.833333                    56.0   \n",
       "2                   14.000000                     0.0   \n",
       "3                   37.714286                    41.0   \n",
       "4                   41.194444                    57.0   \n",
       "5                    0.000000                    31.0   \n",
       "6                   56.600000                    31.0   \n",
       "7                   87.333333                    12.0   \n",
       "8                   54.500000                     3.0   \n",
       "9                   38.666667                     7.0   \n",
       "10                  36.500000                    14.0   \n",
       "11                  61.000000                     0.0   \n",
       "12                  19.000000                     0.0   \n",
       "13                   0.000000                    21.0   \n",
       "14                   0.000000                    54.0   \n",
       "15                        NaN                     NaN   \n",
       "16                  62.333333                     7.0   \n",
       "17                  39.000000                     0.0   \n",
       "18                  56.250000                    50.0   \n",
       "19                  63.800000                    13.0   \n",
       "\n",
       "    new_purchase_date_average  new_purchase_date_uptonow  new_first_buy  \\\n",
       "0                    2.347826                     2382.0          277.0   \n",
       "1                    9.333333                     2412.0          396.0   \n",
       "2                    0.000000                     2383.0          635.0   \n",
       "3                    5.857143                     2393.0          187.0   \n",
       "4                    1.583333                     2382.0          121.0   \n",
       "5                    7.750000                     2607.0          348.0   \n",
       "6                    6.200000                     2405.0          460.0   \n",
       "7                    4.000000                     2451.0          158.0   \n",
       "8                    1.500000                     2421.0          228.0   \n",
       "9                    2.333333                     2403.0          608.0   \n",
       "10                   7.000000                     2398.0          544.0   \n",
       "11                   0.000000                     2430.0          741.0   \n",
       "12                   0.000000                     2388.0          234.0   \n",
       "13                   5.250000                     2501.0          253.0   \n",
       "14                   9.000000                     2503.0           96.0   \n",
       "15                        NaN                        NaN            NaN   \n",
       "16                   2.333333                     2428.0          614.0   \n",
       "17                   0.000000                     2408.0          488.0   \n",
       "18                  12.500000                     2391.0          120.0   \n",
       "19                   1.300000                     2426.0          426.0   \n",
       "\n",
       "    card_id_total  purchase_amount_total  outliers  fold  \n",
       "0           283.0            -179.212631         0     0  \n",
       "1           356.0            -214.361801         0     0  \n",
       "2            44.0             -29.867586         0     0  \n",
       "3            84.0             -54.146149         0     0  \n",
       "4           169.0             -68.613800         0     0  \n",
       "5            37.0             -15.175980         0     0  \n",
       "6           265.0            -189.024124         0     0  \n",
       "7            25.0             -15.663989         0     0  \n",
       "8            17.0              -9.971558         0     0  \n",
       "9           116.0             -79.137260         0     0  \n",
       "10           99.0             -61.945114         0     0  \n",
       "11           20.0             -13.102302         0     0  \n",
       "12           38.0             -24.361458         0     0  \n",
       "13           24.0             -14.985926         0     0  \n",
       "14           71.0             -40.772755         0     0  \n",
       "15            NaN                    NaN         0     0  \n",
       "16          118.0             -74.502594         0     0  \n",
       "17           83.0             -50.854855         0     0  \n",
       "18           31.0             -17.468821         0     0  \n",
       "19          467.0            -325.046173         0     0  \n",
       "\n",
       "[20 rows x 98 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "5a0ed2a5b0ab317cf8697fbdcd18f780859d6299"
   },
   "outputs": [],
   "source": [
    "train[df_train_columns].to_csv('new_train.csv', index=False)\n",
    "test[df_test_columns].to_csv('new_test.csv', index=False)\n",
    "train[['card_id', 'outliers']].to_csv('outliers', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}