{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scale_input_data(scale_factor):\n",
    "  file_bases = ['./input/train', './input/test', './input/share-of-adults-who-smoke',\n",
    "                './input/WEO', './input/Life expectancy at birth', './input/covid19countryinfo',\n",
    "                './input/submission']\n",
    "  for file_base in file_bases:\n",
    "    import pandas as pd\n",
    "    import shutil\n",
    "    if scale_factor == 1.0:\n",
    "      shutil.copyfile(file_base + '.csv', file_base + '.scaled.csv')\n",
    "      continue\n",
    "    df_to_scale = pd.read_csv(file_base + '.csv')\n",
    "    new_num_rows = int(scale_factor * len(df_to_scale))\n",
    "    if scale_factor <= 1.0:\n",
    "      df_to_scale = df_to_scale.iloc[:new_num_rows]\n",
    "    else:\n",
    "      while len(df_to_scale) < new_num_rows:\n",
    "        df_to_scale = pd.concat([df_to_scale, df_to_scale[:min(new_num_rows - len(df_to_scale), len(df_to_scale))]])\n",
    "    df_to_scale.to_csv(file_base + '.scaled.csv')\n",
    "\n",
    "if 'INPUT_SCALE_FACTOR' in os.environ:\n",
    "  scale_input_data(float(os.environ['INPUT_SCALE_FACTOR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "exec(os.environ['IREWR_IMPORTS'])\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# ALEX: remove path printing\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('./input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os, gc, pickle, copy, datetime, warnings\n",
    "# ALEX: remove plotting, ML code\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import lightgbm as lgb\n",
    "# from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./input/train.scaled.csv\")\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./input/test.scaled.csv\")\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traintest = pd.concat([df_train, df_test])\n",
    "print(df_train.shape, df_test.shape, df_traintest.shape,df_traintest.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    try:\n",
    "        x_new = x['Country_Region'] + \"/\" + x['Province_State']\n",
    "    except:\n",
    "        x_new = x['Country_Region']\n",
    "    return x_new\n",
    "        \n",
    "df_traintest['place_id'] = df_traintest.apply(lambda x: func(x), axis=1)\n",
    "tmp = np.sort(df_traintest['place_id'].unique())\n",
    "print(\"num unique places: {}\".format(len(tmp)))\n",
    "print(tmp[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = np.sort(df_traintest['place_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\n",
    "df_traintest['day'] = df_traintest['Date'].apply(lambda x: x.dayofyear).astype(np.int16)\n",
    "df_traintest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traintest2 = copy.deepcopy(df_traintest)\n",
    "df_traintest2['cases/day'] = 0\n",
    "df_traintest2['fatal/day'] = 0\n",
    "tmp_list = np.zeros(len(df_traintest2))\n",
    "for place in places:\n",
    "    tmp = df_traintest2['ConfirmedCases'][df_traintest2['place_id']==place].values\n",
    "    tmp[1:] -= tmp[:-1]\n",
    "    df_traintest2['cases/day'][df_traintest2['place_id']==place] = tmp\n",
    "    tmp = df_traintest2['Fatalities'][df_traintest2['place_id']==place].values\n",
    "    tmp[1:] -= tmp[:-1]\n",
    "    df_traintest2['fatal/day'][df_traintest2['place_id']==place] = tmp\n",
    "print(df_traintest2.shape)\n",
    "df_traintest2[df_traintest2['place_id']=='China/Hubei'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_aggregation(df, col, mean_range):\n",
    "    df_new = copy.deepcopy(df)\n",
    "    col_new = '{}_({}-{})'.format(col, mean_range[0], mean_range[1])\n",
    "    df_new[col_new] = 0\n",
    "    tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean()\n",
    "    df_new[col_new][mean_range[0]:] = tmp[:-(mean_range[0])]\n",
    "    df_new[col_new][pd.isna(df_new[col_new])] = 0\n",
    "    return df_new[[col_new]].reset_index(drop=True)\n",
    "\n",
    "def do_aggregations(df):\n",
    "    df = pd.concat([df, do_aggregation(df, 'cases/day', [1,1]).reset_index(drop=True)], axis=1)\n",
    "    df = pd.concat([df, do_aggregation(df, 'cases/day', [1,7]).reset_index(drop=True)], axis=1)\n",
    "    df = pd.concat([df, do_aggregation(df, 'cases/day', [8,14]).reset_index(drop=True)], axis=1)\n",
    "    df = pd.concat([df, do_aggregation(df, 'cases/day', [15,21]).reset_index(drop=True)], axis=1)\n",
    "    df = pd.concat([df, do_aggregation(df, 'fatal/day', [1,1]).reset_index(drop=True)], axis=1)\n",
    "    df = pd.concat([df, do_aggregation(df, 'fatal/day', [1,7]).reset_index(drop=True)], axis=1)\n",
    "    df = pd.concat([df, do_aggregation(df, 'fatal/day', [8,14]).reset_index(drop=True)], axis=1)\n",
    "    df = pd.concat([df, do_aggregation(df, 'fatal/day', [15,21]).reset_index(drop=True)], axis=1)\n",
    "    for threshold in [1, 10, 100]:\n",
    "        days_under_threshold = (df['ConfirmedCases']<threshold).sum()\n",
    "        tmp = df['day'].values - 22 - days_under_threshold\n",
    "        tmp[tmp<=0] = 0\n",
    "        df['days_since_{}cases'.format(threshold)] = tmp\n",
    "            \n",
    "    for threshold in [1, 10, 100]:\n",
    "        days_under_threshold = (df['Fatalities']<threshold).sum()\n",
    "        tmp = df['day'].values - 22 - days_under_threshold\n",
    "        tmp[tmp<=0] = 0\n",
    "        df['days_since_{}fatal'.format(threshold)] = tmp\n",
    "    \n",
    "    # process China/Hubei\n",
    "    if df['place_id'][0]=='China/Hubei':\n",
    "        df['days_since_1cases'] += 35 # 2019/12/8\n",
    "        df['days_since_10cases'] += 35-13 # 2019/12/8-2020/1/2 assume 2019/12/8+13\n",
    "        df['days_since_100cases'] += 4 # 2020/1/18\n",
    "        df['days_since_1fatal'] += 13 # 2020/1/9\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traintest3 = []\n",
    "for place in places[:]:\n",
    "    df_tmp = df_traintest2[df_traintest2['place_id']==place].reset_index(drop=True)\n",
    "    df_tmp = do_aggregations(df_tmp)\n",
    "    df_traintest3.append(df_tmp)\n",
    "df_traintest3 = pd.concat(df_traintest3).reset_index(drop=True)\n",
    "df_traintest3[df_traintest3['place_id']=='China/Hubei'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smoking = pd.read_csv(\"./input/share-of-adults-who-smoke.scaled.csv\")\n",
    "print(np.sort(df_smoking['Entity'].unique())[:10])\n",
    "df_smoking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smoking_recent = df_smoking.sort_values('Year', ascending=False).reset_index(drop=True)\n",
    "df_smoking_recent = df_smoking_recent[df_smoking_recent['Entity'].duplicated()==False]\n",
    "df_smoking_recent['Country_Region'] = df_smoking_recent['Entity']\n",
    "df_smoking_recent['SmokingRate'] = df_smoking_recent['Smoking prevalence, total (ages 15+) (% of adults)']\n",
    "df_smoking_recent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traintest4 = pd.merge(df_traintest3, df_smoking_recent[['Country_Region', 'SmokingRate']], on='Country_Region', how='left')\n",
    "print(df_traintest4.shape)\n",
    "df_traintest4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SmokingRate = df_smoking_recent['SmokingRate'][df_smoking_recent['Entity']=='World'].values[0]\n",
    "print(\"Smoking rate of the world: {:.6f}\".format(SmokingRate))\n",
    "df_traintest4['SmokingRate'][pd.isna(df_traintest4['SmokingRate'])] = SmokingRate\n",
    "df_traintest4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weo = pd.read_csv(\"./input/WEO.scaled.csv\")\n",
    "df_weo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_weo['Subject Descriptor'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs  = df_weo['Subject Descriptor'].unique()[:-1]\n",
    "df_weo_agg = df_weo[['Country']][df_weo['Country'].duplicated()==False].reset_index(drop=True)\n",
    "for sub in subs[:]:\n",
    "    df_tmp = df_weo[['Country', '2019']][df_weo['Subject Descriptor']==sub].reset_index(drop=True)\n",
    "    df_tmp = df_tmp[df_tmp['Country'].duplicated()==False].reset_index(drop=True)\n",
    "    df_tmp.columns = ['Country', sub]\n",
    "    df_weo_agg = df_weo_agg.merge(df_tmp, on='Country', how='left')\n",
    "df_weo_agg.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_weo_agg.columns]\n",
    "df_weo_agg.columns\n",
    "df_weo_agg['Country_Region'] = df_weo_agg['Country']\n",
    "df_weo_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traintest5 = pd.merge(df_traintest4, df_weo_agg, on='Country_Region', how='left')\n",
    "print(df_traintest5.shape)\n",
    "df_traintest5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_life = pd.read_csv(\"./input/Life expectancy at birth.scaled.csv\")\n",
    "tmp = df_life.iloc[:,1].values.tolist()\n",
    "df_life = df_life[['Country', '2018']]\n",
    "def func(x):\n",
    "    x_new = 0\n",
    "    try:\n",
    "        x_new = float(x.replace(\",\", \"\"))\n",
    "    except:\n",
    "#         print(x)\n",
    "        x_new = np.nan\n",
    "    return x_new\n",
    "    \n",
    "df_life['2018'] = df_life['2018'].apply(lambda x: func(x))\n",
    "df_life.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_life = df_life[['Country', '2018']]\n",
    "df_life.columns = ['Country_Region', 'LifeExpectancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traintest6 = pd.merge(df_traintest5, df_life, on='Country_Region', how='left')\n",
    "print(len(df_traintest6))\n",
    "df_traintest6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = pd.read_csv(\"./input/covid19countryinfo.scaled.csv\")\n",
    "df_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country['Country_Region'] = df_country['country']\n",
    "df_country = df_country[df_country['country'].duplicated()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_country[df_country['country'].duplicated()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country[df_country['country'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traintest7 = pd.merge(df_traintest6, \n",
    "                         df_country.drop(['tests', 'testpop', 'country'], axis=1), \n",
    "                         on=['Country_Region',], how='left')\n",
    "print(df_traintest7.shape)\n",
    "df_traintest7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(df, col, freq_limit=0):\n",
    "    df[col][pd.isna(df[col])] = 'nan'\n",
    "    tmp = df[col].value_counts()\n",
    "    cols = tmp.index.values\n",
    "    freq = tmp.values\n",
    "    num_cols = (freq>=freq_limit).sum()\n",
    "    print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n",
    "\n",
    "    col_new = '{}_le'.format(col)\n",
    "    df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n",
    "    for i, item in enumerate(cols[:num_cols]):\n",
    "        df_new[col_new][df[col]==item] = i\n",
    "\n",
    "    return df_new\n",
    "\n",
    "def get_df_le(df, col_index, col_cat):\n",
    "    df_new = df[[col_index]]\n",
    "    for col in col_cat:\n",
    "        df_tmp = encode_label(df, col)\n",
    "        df_new = pd.concat([df_new, df_tmp], axis=1)\n",
    "    return df_new\n",
    "\n",
    "df_traintest7['id'] = np.arange(len(df_traintest7))\n",
    "df_le = get_df_le(df_traintest7, 'id', ['Country_Region', 'Province_State'])\n",
    "df_traintest8 = pd.merge(df_traintest7, df_le, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traintest8['cases/day'] = df_traintest8['cases/day'].astype(np.float)\n",
    "df_traintest8['fatal/day'] = df_traintest8['fatal/day'].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traintest8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    x_new = 0\n",
    "    try:\n",
    "        x_new = float(x.replace(\",\", \"\"))\n",
    "    except:\n",
    "#         print(x)\n",
    "        x_new = np.nan\n",
    "    return x_new\n",
    "cols = [\n",
    "    'Gross_domestic_product__constant_prices', \n",
    "    'Gross_domestic_product__current_prices', \n",
    "    'Gross_domestic_product__deflator', \n",
    "    'Gross_domestic_product_per_capita__constant_prices', \n",
    "    'Gross_domestic_product_per_capita__current_prices', \n",
    "    'Output_gap_in_percent_of_potential_GDP', \n",
    "    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP', \n",
    "    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP', \n",
    "    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total', \n",
    "    'Implied_PPP_conversion_rate', 'Total_investment', \n",
    "    'Gross_national_savings', 'Inflation__average_consumer_prices', \n",
    "    'Inflation__end_of_period_consumer_prices', \n",
    "    'Six_month_London_interbank_offered_rate__LIBOR_', \n",
    "    'Volume_of_imports_of_goods_and_services', \n",
    "    'Volume_of_Imports_of_goods', \n",
    "    'Volume_of_exports_of_goods_and_services', \n",
    "    'Volume_of_exports_of_goods', 'Unemployment_rate', 'Employment', 'Population', \n",
    "    'General_government_revenue', 'General_government_total_expenditure', \n",
    "    'General_government_net_lending_borrowing', 'General_government_structural_balance', \n",
    "    'General_government_primary_net_lending_borrowing', 'General_government_net_debt', \n",
    "    'General_government_gross_debt', 'Gross_domestic_product_corresponding_to_fiscal_year__current_prices', \n",
    "    'Current_account_balance', 'pop'\n",
    "]\n",
    "for col in cols:\n",
    "    df_traintest8[col] = df_traintest8[col].apply(lambda x: func(x))  \n",
    "print(df_traintest8['pop'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traintest8[df_traintest8['place_id']=='China/Hubei'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(y_true, y_pred):\n",
    "    y_true[y_true<0] = 0\n",
    "    score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "params = {'num_leaves': 8,\n",
    "          'min_data_in_leaf': 5,  # 42,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 8,\n",
    "          'learning_rate': 0.02,\n",
    "          'boosting': 'gbdt',\n",
    "          'bagging_freq': 5,  # 5\n",
    "          'bagging_fraction': 0.8,  # 0.5,\n",
    "          'feature_fraction': 0.8201,\n",
    "          'bagging_seed': SEED,\n",
    "          'reg_alpha': 1,  # 1.728910519108444,\n",
    "          'reg_lambda': 4.9847051755586085,\n",
    "          'random_state': SEED,\n",
    "          'metric': 'mse',\n",
    "          'verbosity': 100,\n",
    "          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n",
    "          'min_child_weight': 5,  # 19.428902804238373,\n",
    "          'num_threads': 6,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_target = 'fatal/day'\n",
    "col_var = [\n",
    "#    'Lat', 'Long',\n",
    "#     'days_since_1cases', \n",
    "#     'days_since_10cases', \n",
    "#     'days_since_100cases',\n",
    "#     'days_since_1fatal', \n",
    "#     'days_since_10fatal', 'days_since_100fatal',\n",
    "#     'days_since_1recov',\n",
    "#     'days_since_10recov', 'days_since_100recov', \n",
    "    'cases/day_(1-1)', \n",
    "    'cases/day_(1-7)', \n",
    "#     'cases/day_(8-14)',  \n",
    "#     'cases/day_(15-21)', \n",
    "    \n",
    "#     'fatal/day_(1-1)', \n",
    "    'fatal/day_(1-7)', \n",
    "    'fatal/day_(8-14)', \n",
    "    'fatal/day_(15-21)', \n",
    "    'SmokingRate',\n",
    "#     'Gross_domestic_product__constant_prices',\n",
    "#     'Gross_domestic_product__current_prices',\n",
    "#     'Gross_domestic_product__deflator',\n",
    "#     'Gross_domestic_product_per_capita__constant_prices',\n",
    "#     'Gross_domestic_product_per_capita__current_prices',\n",
    "#     'Output_gap_in_percent_of_potential_GDP',\n",
    "#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n",
    "#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n",
    "#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n",
    "#     'Implied_PPP_conversion_rate', 'Total_investment',\n",
    "#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n",
    "#     'Inflation__end_of_period_consumer_prices',\n",
    "#     'Six_month_London_interbank_offered_rate__LIBOR_',\n",
    "#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n",
    "#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n",
    "#     'Unemployment_rate', \n",
    "#     'Employment', 'Population',\n",
    "#     'General_government_revenue', 'General_government_total_expenditure',\n",
    "#     'General_government_net_lending_borrowing',\n",
    "#     'General_government_structural_balance',\n",
    "#     'General_government_primary_net_lending_borrowing',\n",
    "#     'General_government_net_debt', 'General_government_gross_debt',\n",
    "#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n",
    "#     'Current_account_balance', \n",
    "#     'LifeExpectancy',\n",
    "#     'pop',\n",
    "    'density', \n",
    "#     'medianage', \n",
    "#     'urbanpop', \n",
    "#     'hospibed', 'smokers',\n",
    "]\n",
    "col_cat = []\n",
    "df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<61)]\n",
    "df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61) & (df_traintest8['day']<72)]\n",
    "df_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\n",
    "X_train = df_train[col_var]\n",
    "X_valid = df_valid[col_var]\n",
    "y_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\n",
    "y_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n",
    "# ALEX: remove ML code\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n",
    "# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n",
    "# num_round = 15000\n",
    "# model = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n",
    "#                   verbose_eval=100,\n",
    "#                   early_stopping_rounds=150,)\n",
    "\n",
    "# best_itr = model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_valid['fatal/day'].values\n",
    "# ALEX: remove ML code\n",
    "# y_pred = np.exp(model.predict(X_valid))-1\n",
    "# score = calc_score(y_true, y_pred)\n",
    "# print(\"{:.6f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame()\n",
    "tmp[\"feature\"] = col_var\n",
    "# ALEX: remove ML code\n",
    "# tmp[\"importance\"] = model.feature_importance()\n",
    "# tmp = tmp.sort_values('importance', ascending=False)\n",
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\n",
    "df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\n",
    "# df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61)]\n",
    "df_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\n",
    "X_train = df_train[col_var]\n",
    "X_valid = df_valid[col_var]\n",
    "y_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\n",
    "y_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n",
    "# ALEX: remove ML code\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n",
    "# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n",
    "# model = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n",
    "#                   verbose_eval=100,\n",
    "#                   early_stopping_rounds=150,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_target2 = 'cases/day'\n",
    "col_var2 = [\n",
    "#    'Lat', 'Long',\n",
    "#     'days_since_1cases', \n",
    "    'days_since_10cases', #selected\n",
    "#     'days_since_100cases',\n",
    "#     'days_since_1fatal', \n",
    "#     'days_since_10fatal',\n",
    "#     'days_since_100fatal',\n",
    "#     'days_since_1recov',\n",
    "#     'days_since_10recov', 'days_since_100recov', \n",
    "    'cases/day_(1-1)', \n",
    "    'cases/day_(1-7)', \n",
    "    'cases/day_(8-14)',  \n",
    "    'cases/day_(15-21)', \n",
    "    \n",
    "#     'fatal/day_(1-1)', \n",
    "#     'fatal/day_(1-7)', \n",
    "#     'fatal/day_(8-14)', \n",
    "#     'fatal/day_(15-21)', \n",
    "#     'recov/day_(1-1)', 'recov/day_(1-7)', \n",
    "#     'recov/day_(8-14)',  'recov/day_(15-21)',\n",
    "#     'active_(1-1)', \n",
    "#     'active_(1-7)', \n",
    "#     'active_(8-14)',  'active_(15-21)', \n",
    "#     'SmokingRate',\n",
    "#     'Gross_domestic_product__constant_prices',\n",
    "#     'Gross_domestic_product__current_prices',\n",
    "#     'Gross_domestic_product__deflator',\n",
    "#     'Gross_domestic_product_per_capita__constant_prices',\n",
    "#     'Gross_domestic_product_per_capita__current_prices',\n",
    "#     'Output_gap_in_percent_of_potential_GDP',\n",
    "#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n",
    "#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n",
    "#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n",
    "#     'Implied_PPP_conversion_rate', 'Total_investment',\n",
    "#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n",
    "#     'Inflation__end_of_period_consumer_prices',\n",
    "#     'Six_month_London_interbank_offered_rate__LIBOR_',\n",
    "#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n",
    "#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n",
    "#     'Unemployment_rate', \n",
    "#     'Employment', \n",
    "#     'Population',\n",
    "#     'General_government_revenue', 'General_government_total_expenditure',\n",
    "#     'General_government_net_lending_borrowing',\n",
    "#     'General_government_structural_balance',\n",
    "#     'General_government_primary_net_lending_borrowing',\n",
    "#     'General_government_net_debt', 'General_government_gross_debt',\n",
    "#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n",
    "#     'Current_account_balance', \n",
    "#     'LifeExpectancy',\n",
    "#     'pop',\n",
    "#     'density', \n",
    "#     'medianage', \n",
    "#     'urbanpop', \n",
    "#     'hospibed', 'smokers', \n",
    "]\n",
    "col_cat = []\n",
    "df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<61)]\n",
    "df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61) & (df_traintest8['day']<72)]\n",
    "# df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61)]\n",
    "df_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\n",
    "X_train = df_train[col_var2]\n",
    "X_valid = df_valid[col_var2]\n",
    "y_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\n",
    "y_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\n",
    "# ALEX: remove ML code\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n",
    "# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n",
    "# model2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n",
    "#                   verbose_eval=100,\n",
    "#                   early_stopping_rounds=150,)\n",
    "# best_itr = model2.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_valid['cases/day'].values\n",
    "# ALEX: remove ML code\n",
    "# y_pred = np.exp(model2.predict(X_valid))-1\n",
    "# score = calc_score(y_true, y_pred)\n",
    "# print(\"{:.6f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\n",
    "df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\n",
    "# df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61)]\n",
    "df_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\n",
    "X_train = df_train[col_var2]\n",
    "X_valid = df_valid[col_var2]\n",
    "y_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\n",
    "y_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\n",
    "# ALEX: remove ML code\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n",
    "# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n",
    "# model2 = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n",
    "#                   verbose_eval=100,\n",
    "#                   early_stopping_rounds=150,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\n",
    "df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=72)]\n",
    "df_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\n",
    "X_train = df_train[col_var]\n",
    "X_valid = df_valid[col_var]\n",
    "y_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\n",
    "y_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n",
    "# ALEX: remove ML code\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n",
    "# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n",
    "# model_pri = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n",
    "#                   verbose_eval=100,\n",
    "#                   early_stopping_rounds=150,)\n",
    "# best_itr = model_pri.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\n",
    "df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\n",
    "# df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61)]\n",
    "df_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\n",
    "X_train = df_train[col_var]\n",
    "X_valid = df_valid[col_var]\n",
    "y_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\n",
    "y_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n",
    "# ALEX: remove ML code\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n",
    "# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n",
    "# model_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n",
    "#                   verbose_eval=100,\n",
    "#                   early_stopping_rounds=150,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<72)]\n",
    "df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=72)]\n",
    "# df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61)]\n",
    "df_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\n",
    "X_train = df_train[col_var2]\n",
    "X_valid = df_valid[col_var2]\n",
    "y_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\n",
    "y_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\n",
    "# ALEX: remove ML code\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n",
    "# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n",
    "# model2_pri = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n",
    "#                   verbose_eval=100,\n",
    "#                   early_stopping_rounds=150,)\n",
    "# best_itr = model2_pri.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\n",
    "df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\n",
    "# df_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>=61)]\n",
    "df_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\n",
    "X_train = df_train[col_var2]\n",
    "X_valid = df_valid[col_var2]\n",
    "y_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\n",
    "y_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\n",
    "# ALEX: remove ML code\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n",
    "# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n",
    "# model2_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n",
    "#                   verbose_eval=100,\n",
    "#                   early_stopping_rounds=150,)\n",
    "# best_itr = model2_pri.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_traintest8[(df_traintest8['day']<72) | (pd.isna(df_traintest8['ForecastId'])==False)].reset_index(drop=True)\n",
    "df_tmp = df_tmp.drop([\n",
    "    'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n",
    "    'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)',\n",
    "    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n",
    "    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n",
    "                               ],  axis=1)\n",
    "df_traintest9 = []\n",
    "for i, place in enumerate(places[:]):\n",
    "    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n",
    "    df_tmp2 = do_aggregations(df_tmp2)\n",
    "    df_traintest9.append(df_tmp2)\n",
    "df_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)\n",
    "df_traintest9[df_traintest9['day']>68].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = 'Bhutan'\n",
    "# place = places[np.random.randint(len(places))]\n",
    "# place = 'Iran'\n",
    "df_interest_base = df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True)\n",
    "df_interest = copy.deepcopy(df_interest_base)\n",
    "df_interest['ConfirmedCases'] = df_interest['ConfirmedCases'].astype(np.float)\n",
    "df_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\n",
    "df_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\n",
    "df_interest['Fatalities'] = df_interest['Fatalities'].astype(np.float)\n",
    "df_interest['cases/day'][df_interest['day']>=72] = -1\n",
    "df_interest['fatal/day'][df_interest['day']>=72] = -1\n",
    "len_known = (df_interest['cases/day']!=-1).sum()\n",
    "len_unknown = (df_interest['cases/day']==-1).sum()\n",
    "print(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\n",
    "X_valid = df_interest[col_var][df_interest['day']>=72]\n",
    "X_valid2 = df_interest[col_var2][df_interest['day']>=72]\n",
    "# ALEX: remove ML code\n",
    "# pred_f =  np.exp(model.predict(X_valid))-1\n",
    "# pred_c = np.exp(model2.predict(X_valid2))-1\n",
    "# df_interest['fatal/day'][df_interest['day']>=72] = pred_f.clip(0, 1e10)\n",
    "# df_interest['cases/day'][df_interest['day']>=72] = pred_c.clip(0, 1e10)\n",
    "df_interest['fatal/day'][df_interest['day']>=72]\n",
    "df_interest['cases/day'][df_interest['day']>=72]\n",
    "df_interest['Fatalities'] = np.cumsum(df_interest['fatal/day'].values)\n",
    "df_interest['ConfirmedCases'] = np.cumsum(df_interest['cases/day'].values)\n",
    "for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n",
    "    X_valid = df_interest[col_var].iloc[j+len_known]\n",
    "    X_valid2 = df_interest[col_var2].iloc[j+len_known]\n",
    "# ALEX: remove ML code\n",
    "#     pred_f = model.predict(X_valid)\n",
    "#     pred_c = model2.predict(X_valid2)\n",
    "#     pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n",
    "#     pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n",
    "#     df_interest['fatal/day'][j+len_known] = pred_f\n",
    "#     df_interest['cases/day'][j+len_known] = pred_c\n",
    "#     df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n",
    "#     df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n",
    "    df_interest['fatal/day'][j+len_known]\n",
    "    df_interest['cases/day'][j+len_known]\n",
    "    df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1]\n",
    "    df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1]\n",
    "    df_interest = df_interest.drop([\n",
    "        'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n",
    "        'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)', \n",
    "        'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n",
    "        'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n",
    "                                   \n",
    "                                   ],  axis=1)\n",
    "    df_interest = do_aggregations(df_interest.reset_index(drop=True))\n",
    "\n",
    "# visualize\n",
    "tmp = df_interest['cases/day'].values\n",
    "tmp = np.cumsum(tmp)\n",
    "# ALEX: remove plotting\n",
    "# sns.lineplot(x=df_interest_base['day'], y=tmp, label='pred')\n",
    "_ = df_interest_base['day']\n",
    "tmp = df_traintest8['ConfirmedCases'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values\n",
    "print(len(tmp), tmp)\n",
    "# ALEX: remove plotting\n",
    "# sns.lineplot(x=df_traintest8['day'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values,\n",
    "#              y=tmp, label='true')\n",
    "# print(place)\n",
    "# plt.show()\n",
    "_ = df_traintest8['day'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day_train = df_traintest8['day'][pd.isna(df_traintest8['ForecastId'])].max()\n",
    "print(last_day_train)\n",
    "df_tmp = df_traintest8[\n",
    "    (pd.isna(df_traintest8['ForecastId'])) |\n",
    "    ((df_traintest8['day']>last_day_train) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\n",
    "df_tmp = df_tmp.drop([\n",
    "    'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n",
    "    'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)',\n",
    "    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n",
    "    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n",
    "                               ],  axis=1)\n",
    "df_traintest10 = []\n",
    "for i, place in enumerate(places[:]):\n",
    "    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n",
    "    df_tmp2 = do_aggregations(df_tmp2)\n",
    "    df_traintest10.append(df_tmp2)\n",
    "df_traintest10 = pd.concat(df_traintest10).reset_index(drop=True)\n",
    "df_traintest10[df_traintest10['day']>last_day_train-5].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = 'Sweden'\n",
    "place = places[np.random.randint(len(places))]\n",
    "# place = 'Iran'\n",
    "df_interest_base = df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True)\n",
    "df_interest = copy.deepcopy(df_interest_base)\n",
    "df_interest['ConfirmedCases'] = df_interest['ConfirmedCases'].astype(np.float)\n",
    "df_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\n",
    "df_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\n",
    "df_interest['Fatalities'] = df_interest['Fatalities'].astype(np.float)\n",
    "df_interest['cases/day'][df_interest['day']>=72] = -1\n",
    "df_interest['fatal/day'][df_interest['day']>=72] = -1\n",
    "len_known = (df_interest['cases/day']!=-1).sum()\n",
    "len_unknown = (df_interest['cases/day']==-1).sum()\n",
    "print(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\n",
    "X_valid = df_interest[col_var][df_interest['day']>=72]\n",
    "X_valid2 = df_interest[col_var2][df_interest['day']>=72]\n",
    "# ALEX: remove ML code\n",
    "# pred_f =  np.exp(model.predict(X_valid))-1\n",
    "# pred_c = np.exp(model2.predict(X_valid2))-1\n",
    "# df_interest['fatal/day'][df_interest['day']>=72] = pred_f.clip(0, 1e10)\n",
    "# df_interest['cases/day'][df_interest['day']>=72] = pred_c.clip(0, 1e10)\n",
    "df_interest['fatal/day'][df_interest['day']>=72]\n",
    "df_interest['cases/day'][df_interest['day']>=72]\n",
    "df_interest['Fatalities'] = np.cumsum(df_interest['fatal/day'].values)\n",
    "df_interest['ConfirmedCases'] = np.cumsum(df_interest['cases/day'].values)\n",
    "for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n",
    "    X_valid = df_interest[col_var].iloc[j+len_known]\n",
    "    X_valid2 = df_interest[col_var2].iloc[j+len_known]\n",
    "# ALEX: remove ML code\n",
    "#     pred_f = model.predict(X_valid)\n",
    "#     pred_c = model2.predict(X_valid2)\n",
    "#     pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n",
    "#     pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n",
    "#     df_interest['fatal/day'][j+len_known] = pred_f\n",
    "#     df_interest['cases/day'][j+len_known] = pred_c\n",
    "#     df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n",
    "#     df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n",
    "    df_interest['fatal/day'][j+len_known]\n",
    "    df_interest['cases/day'][j+len_known]\n",
    "    df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1]\n",
    "    df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1]\n",
    "    df_interest = df_interest.drop([\n",
    "        'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n",
    "        'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)', \n",
    "        'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n",
    "        'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n",
    "                                   \n",
    "                                   ],  axis=1)\n",
    "    df_interest = do_aggregations(df_interest.reset_index(drop=True))\n",
    "\n",
    "# visualize\n",
    "tmp = df_interest['fatal/day'].values\n",
    "tmp = np.cumsum(tmp)\n",
    "# ALEX: remove plotting\n",
    "# sns.lineplot(x=df_interest_base['day'], y=tmp, label='pred')\n",
    "_ = df_interest_base['day']\n",
    "tmp = df_traintest8['Fatalities'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values\n",
    "print(len(tmp), tmp)\n",
    "# ALEX: remove plotting\n",
    "# sns.lineplot(x=df_traintest8['day'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values,\n",
    "#              y=tmp, label='true')\n",
    "# print(place)\n",
    "# plt.show()\n",
    "_ = df_traintest8['day'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Check the predictions of Cases Private\n",
    "place = 'Bhutan'\n",
    "place = places[np.random.randint(len(places))]\n",
    "# place = 'Iran'\n",
    "df_interest_base = df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True)\n",
    "df_interest = copy.deepcopy(df_interest_base)\n",
    "df_interest['ConfirmedCases'] = df_interest['ConfirmedCases'].astype(np.float)\n",
    "df_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\n",
    "df_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\n",
    "df_interest['Fatalities'] = df_interest['Fatalities'].astype(np.float)\n",
    "df_interest['cases/day'][df_interest['day']>last_day_train] = -1\n",
    "df_interest['fatal/day'][df_interest['day']>last_day_train] = -1\n",
    "len_known = (df_interest['cases/day']!=-1).sum()\n",
    "len_unknown = (df_interest['cases/day']==-1).sum()\n",
    "print(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\n",
    "X_valid = df_interest[col_var][df_interest['day']>84]\n",
    "X_valid2 = df_interest[col_var2][df_interest['day']>84]\n",
    "pred_f =  np.exp(model.predict(X_valid))-1\n",
    "pred_c = np.exp(model2.predict(X_valid2))-1\n",
    "df_interest['fatal/day'][df_interest['day']>last_day_train] = pred_f.clip(0, 1e10)\n",
    "df_interest['cases/day'][df_interest['day']>last_day_train] = pred_c.clip(0, 1e10)\n",
    "df_interest['Fatalities'] = np.cumsum(df_interest['fatal/day'].values)\n",
    "df_interest['ConfirmedCases'] = np.cumsum(df_interest['cases/day'].values)\n",
    "for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n",
    "    X_valid = df_interest[col_var].iloc[j+len_known]\n",
    "    X_valid2 = df_interest[col_var2].iloc[j+len_known]\n",
    "    pred_f = model_pri.predict(X_valid)\n",
    "    pred_c = model2_pri.predict(X_valid2)\n",
    "    pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n",
    "    pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n",
    "    df_interest['fatal/day'][j+len_known] = pred_f\n",
    "    df_interest['cases/day'][j+len_known] = pred_c\n",
    "    df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n",
    "    df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n",
    "    df_interest = df_interest.drop([\n",
    "        'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n",
    "        'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)', \n",
    "        'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n",
    "        'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n",
    "                                   \n",
    "                                   ],  axis=1)\n",
    "    df_interest = do_aggregations(df_interest.reset_index(drop=True))\n",
    "\n",
    "# visualize\n",
    "tmp = df_interest['cases/day'].values\n",
    "tmp = np.cumsum(tmp)\n",
    "sns.lineplot(x=df_interest_base['day'], y=tmp, label='pred')\n",
    "tmp = df_traintest10['ConfirmedCases'][(df_traintest10['place_id']==place)& (pd.isna(df_traintest10['ForecastId']))].values\n",
    "print(len(tmp), tmp)\n",
    "sns.lineplot(x=df_traintest10['day'][(df_traintest10['place_id']==place)& (pd.isna(df_traintest10['ForecastId']))].values,\n",
    "             y=tmp, label='true')\n",
    "print(place)\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_before_public = 71\n",
    "df_preds = []\n",
    "for i, place in enumerate(places[:]):\n",
    "#     if place!='Japan' and place!='Afghanistan' :continue\n",
    "    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n",
    "    df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n",
    "    df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n",
    "    len_known = (df_interest['day']<=day_before_public).sum()\n",
    "    len_unknown = (day_before_public<df_interest['day']).sum()\n",
    "    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n",
    "        X_valid = df_interest[col_var].iloc[j+len_known]\n",
    "        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n",
    "# ALEX: remove ML code\n",
    "#         pred_f = model.predict(X_valid)\n",
    "#         pred_c = model2.predict(X_valid2)\n",
    "#         pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n",
    "#         pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n",
    "#         df_interest['fatal/day'][j+len_known] = pred_f\n",
    "#         df_interest['cases/day'][j+len_known] = pred_c\n",
    "#         df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n",
    "#         df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n",
    "        df_interest['fatal/day'][j+len_known]\n",
    "        df_interest['cases/day'][j+len_known]\n",
    "        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1]\n",
    "        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1]\n",
    "        df_interest = df_interest.drop([\n",
    "            'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n",
    "            'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)',\n",
    "            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n",
    "            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n",
    "\n",
    "                                       ],  axis=1)\n",
    "        df_interest = do_aggregations(df_interest)\n",
    "    if (i+1)%10==0:\n",
    "        print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n",
    "    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n",
    "    df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n",
    "    df_preds.append(df_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds= pd.concat(df_preds)\n",
    "df_preds = df_preds.sort_values('day')\n",
    "col_tmp = ['place_id', 'ForecastId', 'day', 'cases/day', 'cases_pred', 'fatal/day', 'fatal_pred',]\n",
    "df_preds[col_tmp][(df_preds['place_id']=='Afghanistan') & (df_preds['day']>75)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_before_private = 84\n",
    "df_preds_pri = []\n",
    "for i, place in enumerate(places[:]):\n",
    "#     if place!='Japan' and place!='Afghanistan' :continue\n",
    "    df_interest = copy.deepcopy(df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True))\n",
    "    df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n",
    "    df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n",
    "    len_known = (df_interest['day']<=day_before_private).sum()\n",
    "    len_unknown = (day_before_private<df_interest['day']).sum()\n",
    "    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n",
    "        X_valid = df_interest[col_var].iloc[j+len_known]\n",
    "        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n",
    "# ALEX: remove ML code\n",
    "#         pred_f = model_pri.predict(X_valid)\n",
    "#         pred_c = model2_pri.predict(X_valid2)\n",
    "#         pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n",
    "#         pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n",
    "#         df_interest['fatal/day'][j+len_known] = pred_f\n",
    "#         df_interest['cases/day'][j+len_known] = pred_c\n",
    "#         df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n",
    "#         df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n",
    "        df_interest['fatal/day'][j+len_known]\n",
    "        df_interest['cases/day'][j+len_known]\n",
    "        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1]\n",
    "        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1]\n",
    "        df_interest = df_interest.drop([\n",
    "            'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n",
    "            'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)',\n",
    "            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n",
    "            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n",
    "\n",
    "                                       ],  axis=1)\n",
    "        df_interest = do_aggregations(df_interest)\n",
    "    if (i+1)%10==0:\n",
    "        print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n",
    "    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n",
    "    df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n",
    "    df_preds_pri.append(df_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat prediction\n",
    "df_preds_pri= pd.concat(df_preds_pri)\n",
    "df_preds_pri = df_preds_pri.sort_values('day')\n",
    "col_tmp = ['place_id', 'ForecastId', 'Date', 'day', 'cases/day', 'cases_pred', 'fatal/day', 'fatal_pred',]\n",
    "df_preds_pri[col_tmp][(df_preds_pri['place_id']=='Japan') & (df_preds_pri['day']>79)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_preds[df_preds['day']>last_day_train] = df_preds_pri[df_preds['day']>last_day_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.to_csv(\"df_preds.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"./input/submission.scaled.csv\")\n",
    "print(len(df_sub))\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge prediction with sub\n",
    "df_sub = pd.merge(df_sub, df_traintest3[['ForecastId', 'place_id', 'day']])\n",
    "df_sub = pd.merge(df_sub, df_preds[['place_id', 'day', 'cases_pred', 'fatal_pred']], on=['place_id', 'day',], how='left')\n",
    "df_sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "df_sub['ConfirmedCases'] = df_sub['cases_pred']\n",
    "df_sub['Fatalities'] = df_sub['fatal_pred']\n",
    "df_sub = df_sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n",
    "df_sub.to_csv(\"submission.csv\", index=None)\n",
    "df_sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}