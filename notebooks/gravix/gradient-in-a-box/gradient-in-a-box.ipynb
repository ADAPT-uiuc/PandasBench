{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scale_input_data(scale_factor):\n",
    "  file_bases = ['./input/train', './input/test']\n",
    "  for file_base in file_bases:\n",
    "    import pandas as pd\n",
    "    import shutil\n",
    "    if scale_factor == 1.0:\n",
    "      shutil.copyfile(file_base + '.csv', file_base + '.scaled.csv')\n",
    "      continue\n",
    "    df_to_scale = pd.read_csv(file_base + '.csv')\n",
    "    new_num_rows = int(scale_factor * len(df_to_scale))\n",
    "    if scale_factor <= 1.0:\n",
    "      df_to_scale = df_to_scale.iloc[:new_num_rows]\n",
    "    else:\n",
    "      while len(df_to_scale) < new_num_rows:\n",
    "        df_to_scale = pd.concat([df_to_scale, df_to_scale[:min(new_num_rows - len(df_to_scale), len(df_to_scale))]])\n",
    "    df_to_scale.to_csv(file_base + '.scaled.csv', index=False)\n",
    "\n",
    "if 'INPUT_SCALE_FACTOR' in os.environ:\n",
    "  scale_input_data(float(os.environ['INPUT_SCALE_FACTOR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "b799e41169df2336b36da7dca90a2cdc4a14d58a"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "exec(os.environ['IREWR_IMPORTS'])\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# FIRST-AUTHOR: remove plotting\n",
    "# import os\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import ast\n",
    "\n",
    "import re\n",
    "\n",
    "# FIRST-AUTHOR: remove ML code\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "d29ea452b1dc18e9a85d7edae515678cc450d5af"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove path printing\n",
    "# print(os.listdir(\"./input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "5ec21b1eb08e960d4bb780c3ef932084d057c70b"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./input/train.scaled.csv\")\n",
    "test = pd.read_csv(\"./input/test.scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "322d5ace26656c47d98d02259bb49d38da2e84fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          0\n",
       "belongs_to_collection    2396\n",
       "budget                      0\n",
       "genres                      7\n",
       "homepage                 2054\n",
       "imdb_id                     0\n",
       "original_language           0\n",
       "original_title              0\n",
       "overview                    8\n",
       "popularity                  0\n",
       "poster_path                 1\n",
       "production_companies      156\n",
       "production_countries       55\n",
       "release_date                0\n",
       "runtime                     2\n",
       "spoken_languages           20\n",
       "status                      0\n",
       "tagline                   597\n",
       "title                       0\n",
       "Keywords                  276\n",
       "cast                       13\n",
       "crew                       16\n",
       "revenue                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "2dbfdbfb13cb2cf870ba062b8058a5ff121026b4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          0\n",
       "belongs_to_collection    3521\n",
       "budget                      0\n",
       "genres                     16\n",
       "homepage                 2978\n",
       "imdb_id                     0\n",
       "original_language           0\n",
       "original_title              0\n",
       "overview                   14\n",
       "popularity                  0\n",
       "poster_path                 1\n",
       "production_companies      258\n",
       "production_countries      102\n",
       "release_date                1\n",
       "runtime                     4\n",
       "spoken_languages           42\n",
       "status                      2\n",
       "tagline                   863\n",
       "title                       3\n",
       "Keywords                  393\n",
       "cast                       13\n",
       "crew                       22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c84945a76ba8c7cbbc084afb7d8f7521f232f67"
   },
   "source": [
    "Transforming dictionary columns to proper format :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "78580f37f2e7fdde9eebdc5e82901118b6f776d4"
   },
   "outputs": [],
   "source": [
    "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
    "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
    "\n",
    "def text_to_dict(df):\n",
    "    for column in dict_columns:\n",
    "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
    "    return df\n",
    "        \n",
    "train = text_to_dict(train)\n",
    "test = text_to_dict(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "322dcf783f984f5813308caad6e2b31237bc8c19"
   },
   "source": [
    "Extracting categories from selected dictionary columnns : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "3c5a7a63b94d60197133e70a276703351ee1d218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belongs_to_collection\n",
      "genres\n",
      "production_countries\n",
      "production_companies\n",
      "spoken_languages\n",
      "Keywords\n",
      "cast\n",
      "crew\n"
     ]
    }
   ],
   "source": [
    "def build_category_list(x, field, feature):\n",
    "    regex = re.compile('[^0-9a-zA-Z_]')\n",
    "    category_list = \"\"\n",
    "    \n",
    "    for d in x:\n",
    "        new_category = regex.sub('', d[field].lower().replace(\" \",\"_\"))\n",
    "        category_list += \" \" + new_category\n",
    "    return category_list.strip()\n",
    "\n",
    "\n",
    "target_fields = {'belongs_to_collection': 'name', 'genres': 'name',\n",
    "                 'production_countries': 'iso_3166_1', 'production_companies': 'name',\n",
    "                 'spoken_languages': 'iso_639_1', 'Keywords': 'name', 'cast':'name',\n",
    "                 'crew':'name'\n",
    "                }\n",
    "\n",
    "train['crew_copy'] = train['crew']\n",
    "test['crew_copy'] = test['crew']\n",
    "\n",
    "train['cast_copy'] = train['cast']\n",
    "test['cast_copy'] = test['cast']\n",
    "\n",
    "\n",
    "for k,v in target_fields.items():\n",
    "    print(k)\n",
    "    train[k] = train[k].apply(lambda x: build_category_list(x, v, k))\n",
    "    test[k] = test[k].apply(lambda x: build_category_list(x, v, k)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "80322928c71ab0840a7e31b3da3c6577f36fe3c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belongs_to_collection\n",
      "Initial: 423\n",
      "Kept: 423\n",
      "\n",
      "genres\n",
      "Initial: 21\n",
      "Kept: 21\n",
      "\n",
      "production_countries\n",
      "Initial: 75\n",
      "Kept: 24\n",
      "\n",
      "production_companies\n",
      "Initial: 3669\n",
      "Kept: 70\n",
      "\n",
      "spoken_languages\n",
      "Initial: 80\n",
      "Kept: 25\n",
      "\n",
      "Keywords\n",
      "Initial: 7398\n",
      "Kept: 309\n",
      "\n",
      "cast\n",
      "Initial: 38557\n",
      "Kept: 296\n",
      "\n",
      "crew\n",
      "Initial: 38547\n",
      "Kept: 383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds = {'belongs_to_collection': 0, 'genres': 0,\n",
    "                 'production_countries': 10, 'production_companies': 10,\n",
    "                 'spoken_languages': 10, 'Keywords': 10, 'cast': 10, 'crew': 10\n",
    "                }\n",
    "\n",
    "def streamline(x, kept):\n",
    "    streamlined = \"\"\n",
    "    for w in x.split(\" \"):\n",
    "        if w in kept:\n",
    "            streamlined = streamlined + \" \" + w\n",
    "    return streamlined.strip()\n",
    "\n",
    "for k,v in thresholds.items():\n",
    "    print(k)\n",
    "    c = Counter(\" \".join(train[k]).split(\" \"))\n",
    "    print(\"Initial:\", len(c))\n",
    "    kept = [w for w,nb in c.items() if nb > v]\n",
    "    print(\"Kept:\", len(kept))\n",
    "    print(\"\")\n",
    "    train[k] = train[k].apply(lambda x: streamline(x, kept))\n",
    "    test[k] = test[k].apply(lambda x: streamline(x, kept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1cb3d5389db7c88cad5b1b94b50ac04e7d6f71aa"
   },
   "source": [
    "For cast and crew we select only key roles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "6a2eec3a71684da97e159a95d8ea8aa54cccaf14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cast_copy\n",
      "crew_copy\n"
     ]
    }
   ],
   "source": [
    "def build_category_list_with_roles(x, v, rv):\n",
    "    regex = re.compile('[^0-9a-zA-Z_]')\n",
    "    category_list = \"\"\n",
    "    for d in x:\n",
    "        if d[v['role_field']] != rv:\n",
    "            pass\n",
    "        else:\n",
    "            if category_list == \"\":\n",
    "                new_category = regex.sub('', d[v['field']].lower().replace(\" \",\"_\"))\n",
    "                category_list += \" \" + new_category\n",
    "    return category_list.strip()  \n",
    "    \n",
    "target_fields = {'cast_copy':{'field':'name', 'role_field':'order', 'role_values':[0,1,2,3,4,5]}, \n",
    "                 'crew_copy':{'field': 'name', 'role_field': 'job',\n",
    "                         'role_values':['Director', 'Producer',\n",
    "                                        'Executive Producer', 'Writer', 'First Assistant Director',\n",
    "                                        'Associate Producer', 'Director of Photography'\n",
    "                                       ]\n",
    "                        }\n",
    "                }\n",
    "\n",
    "\n",
    "additional_label_encoding_columns = []\n",
    "\n",
    "for k,v in target_fields.items():\n",
    "    print(k)\n",
    "    for rv in v['role_values']:\n",
    "        striped_rv = str(rv).lower().replace(' ','_')\n",
    "        additional_label_encoding_columns.append(k + '_' + striped_rv)\n",
    "        train[k + '_' + striped_rv] = train[k].apply(lambda x: build_category_list_with_roles(x, v, rv))\n",
    "        test[k + '_' + striped_rv] = test[k].apply(lambda x: build_category_list_with_roles(x, v, rv))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "110b4d9a7cbc21964b7305c2cec68c477d2fa0dd"
   },
   "source": [
    "Filling nan values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "52ff4fe23979408c185dcb0becaefd22fad15dd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release_date :  9/10/10\n",
      "status :  Released\n",
      "belongs_to_collection :  none\n",
      "runtime :  90.0\n"
     ]
    }
   ],
   "source": [
    "fillna_columns = {'release_date':'mode',\n",
    "                  'status':'mode',\n",
    "                  'belongs_to_collection': 'none',\n",
    "                  'runtime': 'mode'}\n",
    "\n",
    "for k,v in fillna_columns.items():\n",
    "    if v == 'mode':\n",
    "        fill = train[k].mode()[0]\n",
    "    else:\n",
    "        fill = v\n",
    "    print(k, ': ', fill)\n",
    "    train[k] = train[k].fillna(value = fill)\n",
    "    test[k] = test[k].fillna(value = fill)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb17bdc263a88aed4a775dc154f847bae2402573"
   },
   "source": [
    "Adding a few features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "d6eab3698baab075b5220898378d55b4e5955061"
   },
   "outputs": [],
   "source": [
    "def extract_nb_within_collection(r):\n",
    "    regex = re.compile('[^0-9a-zA-Z_]')\n",
    "    original_title = regex.sub('', r['original_title'].lower().replace(\" \",\"_\"))\n",
    "    \n",
    "    if r['is_part_of_collection'] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        if (r['belongs_to_collection'] == original_title + '_collection') or (r['belongs_to_collection'] == original_title):\n",
    "            return 1\n",
    "        else:\n",
    "            regex = re.compile('[^0-9]')\n",
    "            probable_number = regex.sub('', r['original_title'])\n",
    "            if probable_number == '' or int(probable_number) > 5:\n",
    "                return 0\n",
    "            else:\n",
    "                return probable_number\n",
    "\n",
    "def feature_addition(df):\n",
    "    \n",
    "    df['release_year'] = df.release_date.apply(lambda x: x[-2:]).astype('int')\n",
    "    df['release_month'] = df.release_date.apply(lambda x: x.split('/')[0]).astype('int')\n",
    "    df['release_quarter'] = df.release_month % 4 + 1\n",
    "    \n",
    "    df['budget'] = df.budget / 1000000\n",
    "    \n",
    "    df['nb_spoken_languages'] = df.spoken_languages.apply(lambda r: len(r.split(' ')))\n",
    "    df['nb_words_overview'] = df.overview.apply(lambda x: len(str(x).split(' ')) )\n",
    "    df['nb_production_companies'] = df.production_companies.apply(lambda x: len(x.split(' ')) )\n",
    "    df['nb_production_countries'] = df.production_countries.apply(lambda x: len(x.split(' ')) )\n",
    "    df['nb_cast'] = df.cast.apply(lambda x: len(x.split(' ')) )\n",
    "    df['nb_crew'] = df.crew.apply(lambda x: len(x.split(' ')) )\n",
    "    df['nb_keywords'] = df.Keywords.apply(lambda x: len(x.split(' ')) )\n",
    "    df['nb_words_title'] = df.title.apply(lambda x: len(str(x).split(' ')) )\n",
    "    df['nb_words_tagline'] = df.tagline.apply(lambda x: len(str(x).split(' ')) )\n",
    "    \n",
    "    df['nb_words_original_title'] = df.original_title.apply(lambda x: len(x.split(' ')) )\n",
    "    \n",
    "    df['has_original_title'] = (df.title == df.original_title).astype('int')\n",
    "\n",
    "    df['has_homepage'] = 1 - df.homepage.isna().astype('int')\n",
    "    df['homepage_base'] = df.homepage.apply(lambda x: str(x).split('//')[-1].split('/')[0].split('www.')[-1].split('.')[0])\n",
    "    df['homepage_extension'] = df.homepage.apply(lambda x: str(x).split('//')[-1].split('/')[0].split('www.')[-1].split('.')[-1]).fillna(value = '')\n",
    "\n",
    "    df['is_part_of_collection'] = 1 - (df.belongs_to_collection == '').astype('int')\n",
    "    df['nb_within_collection'] =  df.apply(lambda r: extract_nb_within_collection(r), axis = 1).astype('int')\n",
    "    \n",
    "    return df\n",
    "                                                \n",
    "train = feature_addition(train)\n",
    "test = feature_addition(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab43b36549f8c5558134a127b0d84ebd44b0b56d"
   },
   "source": [
    "Label encoding selected features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "d45dfbbc46a86066eb8e9b2aaaa888f6b3d8d225"
   },
   "outputs": [],
   "source": [
    "columns_to_categorize = ['belongs_to_collection', 'status', 'original_language', 'homepage_base', 'homepage_extension']\n",
    "columns_to_categorize += additional_label_encoding_columns\n",
    "\n",
    "for c in columns_to_categorize:\n",
    "# FIRST-AUTHOR: remove ML code\n",
    "#     print(c)\n",
    "#     le = LabelEncoder()\n",
    "#     le.fit_transform(train[c])\n",
    "#     test[c] = test[c].map(lambda s: 'unknown' if s not in le.classes_ else s)\n",
    "#     le.classes_ = np.append(le.classes_, 'unknown')\n",
    "#     train[c] = le.transform(train[c])\n",
    "#     test[c] = le.transform(test[c])\n",
    "    test[c] = test[c].map(lambda s: 'unknown')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9bdf377edbf2263cb9c71a49cbb36ab505eb95cd"
   },
   "source": [
    "Removing unused columns : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "05bb80cdb51cde3a4819a9634628419a7385ecd6"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(test['id'])\n",
    "\n",
    "removed_columns = ['id', 'homepage', 'imdb_id', 'original_title', 'spoken_languages',\n",
    "                   'overview', 'poster_path', 'tagline', 'title',\n",
    "                  'release_date', 'crew_copy', 'cast_copy']\n",
    "\n",
    "\n",
    "train.drop(removed_columns, axis = 1, inplace = True)\n",
    "test.drop(removed_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "180be87b8bdb2d5796c293838ab4772f76dd5fbf"
   },
   "source": [
    "Vectorizing selected columns : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "d4c70aa996d0c70e772c1667049b4bf0e2d4745f"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove ML code\n",
    "# features_to_vectorize = ['genres', 'production_countries', 'production_companies', 'Keywords', 'cast', 'crew']\n",
    "\n",
    "# for f in features_to_vectorize[1:]:\n",
    "#     print(f)\n",
    "#     vectorizer = TfidfVectorizer(use_idf = False)\n",
    "#     vectorized_features = vectorizer.fit_transform(train[f])\n",
    "#     vectorized_features_names = [f + '_' + v for v in vectorizer.get_feature_names()]\n",
    "\n",
    "#     vectorized_features_sparse = pd.SparseDataFrame([ pd.SparseSeries(vectorized_features[i].toarray().ravel()) \n",
    "#                               for i in np.arange(vectorized_features.shape[0]) ], columns = vectorized_features_names)\n",
    "\n",
    "#     train = pd.concat([train, vectorized_features_sparse], axis = 1)\n",
    "    \n",
    "#     vectorized_features = vectorizer.transform(test[f])\n",
    "#     vectorized_features_sparse = pd.SparseDataFrame([ pd.SparseSeries(vectorized_features[i].toarray().ravel()) \n",
    "#                               for i in np.arange(vectorized_features.shape[0]) ], columns = vectorized_features_names)\n",
    "    \n",
    "#     test = pd.concat([test, vectorized_features_sparse], axis = 1)\n",
    "    \n",
    "#     train.drop(f, inplace = True, axis = 1)\n",
    "#     test.drop(f, inplace = True, axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "636a46b683f31f4cc51e9c5e15a4190809b14b7c"
   },
   "source": [
    "Transforming revenue to log for log rmse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "701df37a74488d266fdc2cafbea03fbf4950ddde"
   },
   "outputs": [],
   "source": [
    "train['revenue'] = np.log1p(train['revenue'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9e64030cfa1de0c19ae235c6b890c2eb1462fc1"
   },
   "source": [
    "Train test split : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ab023183cb0e08f00b38ec73df57f08ddae25cae"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove ML code\n",
    "# target_column = 'revenue'\n",
    "\n",
    "# train_set, validate_set = train_test_split(train, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# x_train = train_set.drop([target_column], axis = 1).copy()\n",
    "# y_train = train_set[target_column].copy()\n",
    "\n",
    "# x_validate = validate_set.drop([target_column], axis = 1).copy()\n",
    "# y_validate = validate_set[target_column].copy()\n",
    "\n",
    "# x_total = train.drop([target_column], axis = 1).copy()\n",
    "# y_total = train[target_column].copy()\n",
    "\n",
    "# x_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89f0eddc4c9904f26410cd9f30f6460ed77b9efe"
   },
   "source": [
    "LGBM with preliminary params optimization (hyperopt) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "f68cf601cf7d4699a7be6ddd3e1328a1b6007408"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove ML code\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# params_lgb = {'drop_rate': [0.09777484320779173], 'feature_fraction': [0.6087324102659581],\n",
    "#               'lambda_l1': [0.03915143495854047], 'lambda_l2': [26.68081917087524],\n",
    "#               'learning_rate': [0.013231541159028165],\n",
    "#               'max_drop': [67.0], 'min_data_in_leaf': [1.0],\n",
    "#               'num_leaves': [32.0], 'num_trees': [1370.0]}\n",
    "\n",
    "# params_lgb = {k:v[0] for k,v in params_lgb.items()}\n",
    "\n",
    "\n",
    "# lg = lgb.LGBMRegressor(\n",
    "#                         objective = 'regression',\n",
    "#                         metric = 'rmse',\n",
    "#                         early_stopping_round = 50,\n",
    "#                         drop_rate = params_lgb['drop_rate'],\n",
    "#                         feature_fraction = params_lgb['feature_fraction'],\n",
    "#                         lambda_l1 = params_lgb['lambda_l1'],\n",
    "#                         lambda_l2 = params_lgb['lambda_l2'],\n",
    "#                         learning_rate = params_lgb['learning_rate'],\n",
    "#                         max_drop = int(params_lgb['max_drop']),\n",
    "#                         min_data_in_leaf = int(params_lgb['min_data_in_leaf']),\n",
    "#                         num_leaves = int(params_lgb['num_leaves']),\n",
    "#                         num_trees = int(params_lgb['num_trees']))\n",
    "\n",
    "# lg.fit(x_train, y_train.values, eval_set=[(x_train, y_train), (x_validate, y_validate)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "cefb06b4c71603f153bfa363896ce5471db7a25a"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove ML code\n",
    "# feature_importance = pd.DataFrame(lg.feature_importances_, columns = ['importance'])\n",
    "# feature_importance['feature'] = train.columns[:-1]\n",
    "# feature_importance.sort_values(by='importance', inplace = True, ascending = False)\n",
    "# feature_importance.reset_index(drop = True, inplace = True)\n",
    "# feature_importance\n",
    "_ = train.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "df18dfa732380fd1e9ab2cac6a0ed85977c64414"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove plotting\n",
    "# fig, ax = plt.subplots(figsize=(10, 15))\n",
    "# sns.barplot(y = 'feature', x = 'importance', data = feature_importance[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "81147cbc45b6cf878708bbb189b313c1fc10e428"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove ML code\n",
    "# lg = lgb.LGBMRegressor(\n",
    "#                         objective = 'regression',\n",
    "#                         metric = 'rmse',\n",
    "#                         early_stopping_round = 50,\n",
    "#                         drop_rate = params_lgb['drop_rate'],\n",
    "#                         feature_fraction = params_lgb['feature_fraction'],\n",
    "#                         lambda_l1 = params_lgb['lambda_l1'],\n",
    "#                         lambda_l2 = params_lgb['lambda_l2'],\n",
    "#                         learning_rate = params_lgb['learning_rate'],\n",
    "#                         max_drop = int(params_lgb['max_drop']),\n",
    "#                         min_data_in_leaf = int(params_lgb['min_data_in_leaf']),\n",
    "#                         num_leaves = int(params_lgb['num_leaves']),\n",
    "#                         num_trees = 592)\n",
    "\n",
    "# lg.fit(x_total, y_total.values, eval_set=[(x_train, y_train), (x_validate, y_validate)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "4dee8033ce36b566570efbbe7bdf5fbef09ce59c"
   },
   "outputs": [],
   "source": [
    "# FIRST-AUTHOR: remove ML code\n",
    "# y_test_p = pd.Series(lg.predict(x_test))\n",
    "# submission['revenue'] = np.expm1(y_test_p)\n",
    "# submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6115089381d6f3124dca3dfaeef759791a74243f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}